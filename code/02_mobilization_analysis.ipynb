{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c695437",
   "metadata": {
    "papermill": {
     "duration": 0.020666,
     "end_time": "2025-05-05T20:35:54.770979",
     "exception": false,
     "start_time": "2025-05-05T20:35:54.750313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Eligibility for mobilization - Analysis\n",
    "\n",
    "Run this script after running the [01_cohort_identification.ipynb](01_cohort_identification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debbe3c9",
   "metadata": {
    "papermill": {
     "duration": 0.008986,
     "end_time": "2025-05-05T20:35:54.792613",
     "exception": false,
     "start_time": "2025-05-05T20:35:54.783627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d90660",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:54.810382Z",
     "iopub.status.busy": "2025-05-05T20:35:54.810138Z",
     "iopub.status.idle": "2025-05-05T20:35:56.197661Z",
     "shell.execute_reply": "2025-05-05T20:35:56.197341Z"
    },
    "papermill": {
     "duration": 1.398106,
     "end_time": "2025-05-05T20:35:56.198508",
     "exception": false,
     "start_time": "2025-05-05T20:35:54.800402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! pip install pandas numpy duckdb seaborn matplotlib tableone\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "from tableone import TableOne\n",
    "import pyCLIF\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "from upsetplot import UpSet, from_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b875e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:56.225641Z",
     "iopub.status.busy": "2025-05-05T20:35:56.225435Z",
     "iopub.status.idle": "2025-05-05T20:35:56.465776Z",
     "shell.execute_reply": "2025-05-05T20:35:56.465409Z"
    },
    "papermill": {
     "duration": 0.248165,
     "end_time": "2025-05-05T20:35:56.466870",
     "exception": false,
     "start_time": "2025-05-05T20:35:56.218705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df = pd.read_parquet('../output/intermediate/final_df_hourly.parquet')\n",
    "all_ids_w_outcome = pd.read_parquet('../output/intermediate/cohort_all_ids_w_outcome.parquet')\n",
    "final_df_blocks = pd.read_parquet('../output/intermediate/final_df_blocks.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3306bc",
   "metadata": {
    "papermill": {
     "duration": 0.005779,
     "end_time": "2025-05-05T20:35:56.478753",
     "exception": false,
     "start_time": "2025-05-05T20:35:56.472974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Forward and Backward fill the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2852cfac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:56.491542Z",
     "iopub.status.busy": "2025-05-05T20:35:56.491380Z",
     "iopub.status.idle": "2025-05-05T20:35:56.633482Z",
     "shell.execute_reply": "2025-05-05T20:35:56.632837Z"
    },
    "papermill": {
     "duration": 0.151546,
     "end_time": "2025-05-05T20:35:56.636176",
     "exception": false,
     "start_time": "2025-05-05T20:35:56.484630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "before_filling = final_df.isnull().sum() / len(final_df) * 100\n",
    "print(\"Shape of final_df: \", final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d9ad8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:56.651160Z",
     "iopub.status.busy": "2025-05-05T20:35:56.650747Z",
     "iopub.status.idle": "2025-05-05T20:36:08.998861Z",
     "shell.execute_reply": "2025-05-05T20:36:08.998491Z"
    },
    "papermill": {
     "duration": 12.356486,
     "end_time": "2025-05-05T20:36:08.999951",
     "exception": false,
     "start_time": "2025-05-05T20:35:56.643465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Forward and Backward fill for hourly dataframe\")\n",
    "# 0 ── safety ordering ───────────────────────────────────────\n",
    "final_df = final_df.sort_values(\n",
    "    by=['encounter_block', 'recorded_date', 'recorded_hour']\n",
    ")\n",
    "\n",
    "# 1 ── identify column groups ────────────────────────────────\n",
    "flag_columns = [\n",
    "    'hourly_trach','hourly_on_vent','nicardipine_flag','nitroprusside_flag',\n",
    "    'clevidipine_flag','red_meds_flag','cisatracurium_flag','vecuronium_flag',\n",
    "    'rocuronium_flag','paralytics_flag'\n",
    "]\n",
    "exclude_columns = [\n",
    "    'patient_id','hospitalization_id','encounter_block',\n",
    "     'recorded_date','recorded_hour',\n",
    "    'time_from_vent','time_from_vent_adjusted', 'lactate',\n",
    "    'last_ne_dose_last_6_hours','ne_calc_last']\n",
    "\n",
    "all_cols           = set(final_df.columns)\n",
    "potential_fill     = all_cols - set(flag_columns) - set(exclude_columns) \n",
    "continuous_columns = [\n",
    "    c for c in potential_fill\n",
    "    if pd.api.types.is_numeric_dtype(final_df[c])\n",
    "]\n",
    "\n",
    "# 2 ── binary flags → 0/1 ints ───────────────────────────────\n",
    "for col in flag_columns:\n",
    "    final_df[col] = final_df[col].fillna(0).astype(int)\n",
    "\n",
    "# 3 ── forward / backward fill for numeric variables ---------\n",
    "final_df[continuous_columns] = (\n",
    "    final_df\n",
    "      .groupby('encounter_block')[continuous_columns]\n",
    "      .transform(lambda s: s.ffill().bfill())\n",
    ")\n",
    "\n",
    "# 4 ── lactate: forward-fill but **only 24 h** (24 rows) -----\n",
    "final_df['lactate'] = (\n",
    "    final_df\n",
    "      .groupby('encounter_block')\n",
    "      .apply(lambda g: g['lactate'].fillna(method='ffill', limit=24))\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# 5 ── tracheostomy flag stays 1 once first seen -------------\n",
    "final_df['hourly_trach'] = (\n",
    "    final_df.groupby('encounter_block')['hourly_trach']\n",
    "            .transform(lambda s: s.cummax())\n",
    "            .astype(int)\n",
    ")\n",
    "\n",
    "final_df['ne_calc_last'] = (\n",
    "    final_df\n",
    "      .groupby('encounter_block')['ne_calc_last']\n",
    "      .transform(lambda s: s.ffill())\n",
    ")\n",
    "\n",
    "\n",
    "# 6 ── norepinephrine: exact-6-hour look-backs, but\n",
    "#     **only replace rows that were NA already**\n",
    "# ------------------------------------------------\n",
    "def add_exact_6h_ne(block: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    • If a row already has last_ne_dose_last_6_hours \n",
    "      it is **left unchanged**.\n",
    "    • Otherwise we insert the value from 6 hours earlier\n",
    "      (0 if no row exists 6 hours before).\n",
    "    Expect *block* to be time-sorted.\n",
    "    \"\"\"\n",
    "    block = block.copy()\n",
    "\n",
    "    # make sure the columns exist\n",
    "    if 'last_ne_dose_last_6_hours' not in block.columns:\n",
    "        block[col] = np.nan\n",
    "\n",
    "    # candidate fill values = value 6 rows earlier\n",
    "    fill_last = block['ne_calc_last'].shift(6)\n",
    "\n",
    "    # only overwrite where current value is NA\n",
    "    block.loc[block['last_ne_dose_last_6_hours'].isna(), 'last_ne_dose_last_6_hours'] = fill_last\n",
    "\n",
    "    # still-missing ⇒ 0  (= “no vasopressor recorded 6 h ago”)\n",
    "    block[['last_ne_dose_last_6_hours']] = block[[\n",
    "               'last_ne_dose_last_6_hours']].fillna(0)\n",
    "\n",
    "    return block\n",
    "\n",
    "\n",
    "final_df = (\n",
    "    final_df\n",
    "      .sort_values(['encounter_block', 'recorded_date', 'recorded_hour']) \n",
    "      .groupby('encounter_block', group_keys=False)\n",
    "      .apply(add_exact_6h_ne)\n",
    "      .reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284a1f0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:09.013704Z",
     "iopub.status.busy": "2025-05-05T20:36:09.013550Z",
     "iopub.status.idle": "2025-05-05T20:36:09.176809Z",
     "shell.execute_reply": "2025-05-05T20:36:09.176164Z"
    },
    "papermill": {
     "duration": 0.171152,
     "end_time": "2025-05-05T20:36:09.177821",
     "exception": false,
     "start_time": "2025-05-05T20:36:09.006669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "after_filling = final_df.isnull().sum() / len(final_df) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_after = pd.DataFrame({\n",
    "    'before_filling': before_filling,\n",
    "    'after_filling': after_filling\n",
    "}).reset_index()\n",
    "before_after = before_after.rename(columns={'index': 'column'})\n",
    "# save before_after to csv\n",
    "before_after.to_csv('../output/final/missingness_final_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf752a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['is_weekday'] = final_df['recorded_date'].apply(\n",
    "    lambda x: pd.to_datetime(x).weekday() < 5  # 0=Monday, 6=Sunday\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac381b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:09.191606Z",
     "iopub.status.busy": "2025-05-05T20:36:09.191454Z",
     "iopub.status.idle": "2025-05-05T20:36:10.106927Z",
     "shell.execute_reply": "2025-05-05T20:36:10.106580Z"
    },
    "papermill": {
     "duration": 0.923458,
     "end_time": "2025-05-05T20:36:10.107871",
     "exception": false,
     "start_time": "2025-05-05T20:36:09.184413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checkpoint- useful to compare to the original df and check filling logic\n",
    "final_df.to_parquet(f'../output/intermediate/final_df_filled.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07786367",
   "metadata": {
    "papermill": {
     "duration": 0.005973,
     "end_time": "2025-05-05T20:36:10.120420",
     "exception": false,
     "start_time": "2025-05-05T20:36:10.114447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create Criteria Flags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc68edf",
   "metadata": {
    "papermill": {
     "duration": 0.00623,
     "end_time": "2025-05-05T20:36:10.132516",
     "exception": false,
     "start_time": "2025-05-05T20:36:10.126286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Patel et al. Criteria:\n",
    "\n",
    "Cardio\n",
    "* Mean arterial blood pressure: 65-110 mm Hg\n",
    "* Systolic blood pressure: ≤ 200 mm Hg\n",
    "* Heart rate: 40-130 beats per minute\n",
    "\n",
    "Respiratory\n",
    "* Respiratory rate: 5-40 breaths per minute\n",
    "* Pulse oximetry: ≥ 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2aca63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:10.145371Z",
     "iopub.status.busy": "2025-05-05T20:36:10.145192Z",
     "iopub.status.idle": "2025-05-05T20:36:10.181168Z",
     "shell.execute_reply": "2025-05-05T20:36:10.180771Z"
    },
    "papermill": {
     "duration": 0.043514,
     "end_time": "2025-05-05T20:36:10.182200",
     "exception": false,
     "start_time": "2025-05-05T20:36:10.138686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply Patel et al. Criteria\n",
    "print(\"Create Patel/Chicago Criteria flags\")\n",
    "# 1. Mean arterial blood pressure: 65-110 mm Hg\n",
    "## 5/5/25 -- UPDATE MAP TO CONSIDER AVERAGE VALUE BASED ON VASOPRESSOR DOSE\n",
    "# final_df['patel_map_flag'] = (\n",
    "#     (final_df['min_map'] >= 65) & (final_df['max_map'] <= 110)\n",
    "# ).astype(int)\n",
    "\n",
    "final_df['patel_map_flag'] = (\n",
    "    (final_df['avg_map'] >= 65) & (final_df['avg_map'] <= 110)\n",
    ").astype(int)\n",
    "\n",
    "# 2. Systolic blood pressure: ≤ 200 mm Hg\n",
    "final_df['patel_sbp_flag'] = (\n",
    "    final_df['max_sbp'].isna() |\n",
    "    (final_df['max_sbp'] <= 200)\n",
    ").astype(int)\n",
    "\n",
    "# 3. Heart rate (Pulse): 40-130 beats per minute\n",
    "final_df['patel_pulse_flag'] = (\n",
    "    (final_df['min_heart_rate'] >= 40) & (final_df['max_heart_rate'] <= 130)\n",
    ").astype(int)\n",
    "\n",
    "# 4. Respiratory rate: 5-40 breaths per minute\n",
    "final_df['patel_resp_rate_flag'] = (\n",
    "    (final_df['min_respiratory_rate'] >= 5) & (final_df['max_respiratory_rate'] <= 40)\n",
    ").astype(int)\n",
    "\n",
    "# 5. Pulse oximetry (SpO2): ≥ 88%\n",
    "final_df['patel_spo2_flag'] = (\n",
    "    final_df['min_spo2'].isna() |\n",
    "    (final_df['min_spo2'] >= 88)\n",
    ").astype(int)\n",
    "\n",
    "# Resp flag: Combines respiratory rate and SpO2 criteria\n",
    "final_df['patel_resp_flag'] = (\n",
    "    final_df['patel_resp_rate_flag'] &\n",
    "    final_df['patel_spo2_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "# Cardio flag: Combines MAP, SBP, and Pulse criteria\n",
    "final_df['patel_cardio_flag'] = (\n",
    "    final_df['patel_map_flag'] &\n",
    "    final_df['patel_sbp_flag'] &\n",
    "    final_df['patel_pulse_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "# Create the overall Patel flag\n",
    "final_df['patel_flag'] = (\n",
    "    final_df['patel_map_flag'] &\n",
    "    final_df['patel_sbp_flag'] &\n",
    "    final_df['patel_pulse_flag'] &\n",
    "    final_df['patel_resp_rate_flag'] &\n",
    "    final_df['patel_spo2_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['patel_flag_all_hours'] = (\n",
    "    final_df['patel_map_flag'] &\n",
    "    final_df['patel_sbp_flag'] &\n",
    "    final_df['patel_pulse_flag'] &\n",
    "    final_df['patel_resp_rate_flag'] &\n",
    "    final_df['patel_spo2_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "# patel weekday flag\n",
    "final_df['patel_flag_weekday'] = (\n",
    "    final_df['patel_map_flag'] &\n",
    "    final_df['patel_sbp_flag'] &\n",
    "    final_df['patel_pulse_flag'] &\n",
    "    final_df['patel_resp_rate_flag'] &\n",
    "    final_df['patel_spo2_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['is_weekday'] == True) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d3177f",
   "metadata": {
    "papermill": {
     "duration": 0.006093,
     "end_time": "2025-05-05T20:36:10.194856",
     "exception": false,
     "start_time": "2025-05-05T20:36:10.188763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TEAM criteria\n",
    "\n",
    "Cardio\n",
    "* Heart rate: ≤ 150 bpm\n",
    "* Most recent lactate: ≤ 4.0 mmol/L\n",
    "* Noradrenaline infusion rate: <0.2 mcg/kg/min or if infusion rate has increased by more than 25% in the last 6 hours, dose must be <0.1 mcg/kg/min.\n",
    "Respiratory\n",
    "* Sufficient respiratory stability:\n",
    "    *  FiO2: ≤ 0.6\n",
    "    *  PEEP: ≤ 16 cm H2O (use peep_observed)\n",
    "* Current respiratory rate: ≤ 45 (use resp_rate_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75262f90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:10.207774Z",
     "iopub.status.busy": "2025-05-05T20:36:10.207610Z",
     "iopub.status.idle": "2025-05-05T20:36:10.271977Z",
     "shell.execute_reply": "2025-05-05T20:36:10.271534Z"
    },
    "papermill": {
     "duration": 0.07213,
     "end_time": "2025-05-05T20:36:10.272954",
     "exception": false,
     "start_time": "2025-05-05T20:36:10.200824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Create TEAM Criteria flags\")\n",
    "# 1. Heart rate: ≤ 150 bpm\n",
    "final_df['team_pulse_flag'] = np.where(\n",
    "    final_df['max_heart_rate'].isna(),\n",
    "    1,\n",
    "    (final_df['max_heart_rate'] <= 150).astype(int)\n",
    ")\n",
    "\n",
    "# 2. Most recent lactate: ≤ 4.0 mmol/L\n",
    "final_df['team_lactate_flag'] = np.where(\n",
    "    final_df['lactate'].isna(),\n",
    "    1,\n",
    "    (final_df['lactate'] <= 4.0).astype(int)\n",
    ")\n",
    "\n",
    "# 3. Noradrenaline infusion rate: <0.2 mcg/kg/min \n",
    "final_df['team_ne_flag'] = np.where(\n",
    "    final_df['ne_calc_last'].isna(),\n",
    "    1,\n",
    "    (final_df['ne_calc_last'] <= 0.2).astype(int)\n",
    ")\n",
    "\n",
    "# print the number of team_ne_flag == 1\n",
    "print(\"TEAM NE flag counts when ne < 0.2\\n\", final_df['team_ne_flag'].value_counts(), \"\\n\")\n",
    " \n",
    "#3b. set the flag to 0 if infusion rate has increased by more than 25% in the last 6 hours and the dose is >0.1 mcg/kg/min.\n",
    "final_df['team_ne_flag'] = np.where(\n",
    "    (final_df['ne_calc_last'] > 1.25 * final_df['last_ne_dose_last_6_hours']) & (final_df['ne_calc_last'] > 0.1),\n",
    "    0,\n",
    "    final_df['team_ne_flag']\n",
    ")\n",
    "print(\"TEAM NE flag counts adjusting for change in the last 6 hrs\\n\", final_df['team_ne_flag'].value_counts(), \"\\n\")\n",
    "\n",
    "# 4. Sufficient respiratory stability:\n",
    "#    a. FiO2: ≤ 0.6\n",
    "final_df['team_fio2_flag'] = np.where(\n",
    "    final_df['min_fio2_set'].isna(),\n",
    "    1,\n",
    "    (final_df['min_fio2_set'] <= 0.6).astype(int)\n",
    ")\n",
    "\n",
    "#    b. PEEP: ≤ 16 cm H2O\n",
    "final_df['team_peep_flag'] = np.where(\n",
    "    final_df['max_peep_set'].isna(),\n",
    "    1,\n",
    "    (final_df['max_peep_set'] <= 16).astype(int)\n",
    ")\n",
    "\n",
    "# 5. Current respiratory rate: ≤ 45\n",
    "final_df['team_resp_rate_flag'] = np.where(\n",
    "    final_df['max_respiratory_rate'].isna(),\n",
    "    1,\n",
    "    (final_df['max_respiratory_rate'] <= 45).astype(int)\n",
    ")\n",
    "\n",
    "# Cardio flag: Combines heart rate, lactate, and norepinephrine criteria\n",
    "final_df['team_cardio_flag'] = (\n",
    "    final_df['team_pulse_flag'] &\n",
    "    final_df['team_lactate_flag'] &\n",
    "    final_df['team_ne_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "# Resp flag: Combines FiO2, PEEP, and respiratory rate criteria\n",
    "final_df['team_resp_flag'] = (\n",
    "    final_df['team_fio2_flag'] &\n",
    "    final_df['team_peep_flag'] &\n",
    "    final_df['team_resp_rate_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "# Create the overall TEAM flag\n",
    "final_df['team_flag'] = (\n",
    "    final_df['team_pulse_flag'] &\n",
    "    final_df['team_lactate_flag'] &\n",
    "    final_df['team_ne_flag'] &\n",
    "    final_df['team_fio2_flag'] &\n",
    "    final_df['team_peep_flag'] &\n",
    "    final_df['team_resp_rate_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) & \n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['team_flag_all_hours'] = (\n",
    "    final_df['team_pulse_flag'] &\n",
    "    final_df['team_lactate_flag'] &\n",
    "    final_df['team_ne_flag'] &\n",
    "    final_df['team_fio2_flag'] &\n",
    "    final_df['team_peep_flag'] &\n",
    "    final_df['team_resp_rate_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) & \n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "# TEAM weekday flag\n",
    "final_df['team_flag_weekday'] = (\n",
    "    final_df['team_pulse_flag'] &\n",
    "    final_df['team_lactate_flag'] &\n",
    "    final_df['team_ne_flag'] &\n",
    "    final_df['team_fio2_flag'] &\n",
    "    final_df['team_peep_flag'] &\n",
    "    final_df['team_resp_rate_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) & \n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['is_weekday'] == True) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647de90e",
   "metadata": {
    "papermill": {
     "duration": 0.006015,
     "end_time": "2025-05-05T20:36:10.285433",
     "exception": false,
     "start_time": "2025-05-05T20:36:10.279418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Consensus criteria\n",
    "\n",
    "* Green Criteria\n",
    "    * Respiratory\n",
    "        * Saturation  90% and\n",
    "        * Respiratory rate ≤ 30 breaths/min\n",
    "        * Current FiO2 ≤ 0.6 and\n",
    "        * PEEP≤ 10cm H20\n",
    "    * Cardiovascular:\n",
    "        * Blood pressure greater than lower limit of target range (MAP 65+) while on no or low level of support (low support- define as <0.1 μg/kg/min of Norepi equivalents)\n",
    "        * Heart rate <120 beats/min\n",
    "        * lactate < 4mmol/L\n",
    "        * HR > 40\n",
    "* Yellow Criteria\n",
    "    * Respiratory\n",
    "        * Sat >= 90%\n",
    "        * Current FiO2 >0.6\n",
    "        * Respiratory rate >30breaths/min\n",
    "        * PEEP >10cm H20\n",
    "    * Cardiovascular\n",
    "        * Blood pressure greater than lower limit of target range (MAP 65+) while receiving moderate level of support (medium-define as 0.1–0.3 μg/kg/min of Norepi equivalents)\n",
    "        * Heart rate 120-150 beats/min\n",
    "        * Shock of any cause with lactate >4mmol/L\n",
    "        * HR > 40\n",
    "* Red Criteria\n",
    "    * Respiratory\n",
    "        * Sat <90%\n",
    "    * Cardiovascular\n",
    "        * Below target MAP despite support (MAP <65) or\n",
    "        * greater than lower limit MAP (MAP 65+) but on high level support (high defined as >0.3 μg/kg/min of Norepi equivalents)\n",
    "        * IV therapy for hypertensive emergency (SBP >200mmHg or MAP >110 and on nicardipine, nitroprusside, or clevidipine gtt)\n",
    "        * HR >150 bpm\n",
    "        * Bradycardia <40\n",
    "\n",
    "\n",
    "**Consensus criteria - redefined**\n",
    "\n",
    "* all_red: All red subcomponents must be met.\n",
    "* all_green_no_red: All green subcomponents must be met, and no red subcomponents are met.\n",
    "* all_yellow: All yellow subcomponents must be met, no red subcomponents are met, and all green subcomponents are not met.\n",
    "* any_yellow: Any yellow subcomponent is met, no green subcomponents are fully met, and no red subcomponents are met.\n",
    "* any_yellow_or_green_no_red: Any yellow or green subcomponents are met, but no red subcomponents are met.\n",
    "* no_red: No red criteria is met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0053da4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:10.299060Z",
     "iopub.status.busy": "2025-05-05T20:36:10.298886Z",
     "iopub.status.idle": "2025-05-05T20:36:10.493923Z",
     "shell.execute_reply": "2025-05-05T20:36:10.493543Z"
    },
    "papermill": {
     "duration": 0.20315,
     "end_time": "2025-05-05T20:36:10.495001",
     "exception": false,
     "start_time": "2025-05-05T20:36:10.291851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Create Consensus Criteria flags\")\n",
    "# Red Cardiovascular Criteria\n",
    "final_df['red_resp_spo2_flag'] = ((final_df['min_spo2'] < 90) | final_df['min_spo2'].isna()).astype(int)\n",
    "final_df['red_map_flag'] = ((final_df['avg_map'] < 65) | final_df['avg_map'].isna()).astype(int)\n",
    "\n",
    "# High support (Norepinephrine equivalents > 0.3 μg/kg/min)\n",
    "final_df['red_high_support_flag'] = ((final_df['ne_calc_last'] > 0.3)).astype(int)\n",
    "\n",
    "# Hypertensive emergency criteria (SBP > 200 mmHg or MAP > 110 mmHg and on certain medications)\n",
    "final_df['red_hypertensive_flag'] = (\n",
    "    (((final_df['max_sbp'] > 200) | (final_df['avg_map'] > 110)) &\n",
    "    (final_df['red_meds_flag'] == 1)) \n",
    ").astype(int)\n",
    "\n",
    "# High heart rate criteria (HR > 150 bpm)\n",
    "final_df['red_pulse_high_flag'] = ((final_df['max_heart_rate'] > 150)).astype(int)\n",
    "# Low heart rate criteria (HR < 40 bpm)\n",
    "final_df['red_pulse_low_flag'] = ((final_df['min_heart_rate'] < 40) | final_df['min_heart_rate'].isna()).astype(int)\n",
    "\n",
    "# Yellow Respiratory Criteria\n",
    "final_df['yellow_resp_spo2_flag'] = ((final_df['min_spo2'] >= 90)| final_df['min_spo2'].isna()).astype(int)\n",
    "final_df['yellow_fio2_flag'] = ((final_df['min_fio2_set'] > 0.6)).astype(int)\n",
    "final_df['yellow_resp_rate_flag'] = ((final_df['max_respiratory_rate'] > 30)).astype(int)\n",
    "final_df['yellow_peep_flag'] = ((final_df['min_peep_set'] > 10)).astype(int)\n",
    "\n",
    "# Yellow Cardiovascular Criteria\n",
    "final_df['yellow_map_flag'] = (((final_df['avg_map'] >= 65) & (final_df['ne_calc_last'].between(0.1, 0.3)))).astype(int)\n",
    "final_df['yellow_pulse_flag'] = ((final_df['min_heart_rate'].between(120, 150))).astype(int)\n",
    "final_df['yellow_lactate_flag'] = ((final_df['lactate'] > 4)).astype(int)\n",
    "\n",
    "# Step 3: Implement Green Criteria\n",
    "final_df['green_resp_spo2_flag'] = ((final_df['min_spo2'] >= 90)| final_df['min_spo2'].isna()).astype(int)\n",
    "final_df['green_resp_rate_flag'] = ((final_df['max_respiratory_rate'] <= 30) | final_df['max_respiratory_rate'].isna()).astype(int)\n",
    "final_df['green_fio2_flag'] = ((final_df['min_fio2_set'] <= 0.6) | final_df['min_fio2_set'].isna()).astype(int)\n",
    "final_df['green_peep_flag'] = ((final_df['min_peep_set'] <= 10) | final_df['min_peep_set'].isna()).astype(int)\n",
    "\n",
    "# Green Cardiovascular Criteria\n",
    "final_df['green_map_flag'] = (((final_df['avg_map'] >= 65) & (final_df['ne_calc_last'] < 0.1)) | final_df['ne_calc_last'].isna()).astype(int)\n",
    "final_df['green_pulse_flag'] = ((final_df['min_heart_rate'] < 120) | final_df['min_heart_rate'].isna()).astype(int)\n",
    "final_df['green_lactate_flag'] = ((final_df['lactate'] < 4) | final_df['lactate'].isna()).astype(int)\n",
    "final_df['green_hr_flag'] = ((final_df['min_heart_rate'] > 40) | final_df['min_heart_rate'].isna()).astype(int)\n",
    "\n",
    "\n",
    "## Green subcomoponent flags\n",
    "final_df['green_resp_flag'] = (\n",
    "    final_df['green_resp_spo2_flag'] &\n",
    "    final_df['green_resp_rate_flag'] &\n",
    "    final_df['green_fio2_flag'] &\n",
    "    final_df['green_peep_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "# Green cardio flag: Combines MAP, SBP, lactate and Pulse criteria\n",
    "final_df['green_cardio_flag'] = (\n",
    "    final_df['green_map_flag'] &\n",
    "    final_df['green_pulse_flag'] &\n",
    "    final_df['green_lactate_flag'] &\n",
    "    final_df['green_hr_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "final_df['any_red'] = (\n",
    "    (final_df['red_resp_spo2_flag'] |\n",
    "    final_df['red_map_flag'] |\n",
    "    final_df['red_high_support_flag'] |\n",
    "    final_df['red_hypertensive_flag'] |\n",
    "    final_df['red_pulse_high_flag'] |\n",
    "    final_df['red_pulse_low_flag']) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['no_red'] = (~(final_df['red_resp_spo2_flag'] |\n",
    "       final_df['red_map_flag'] |\n",
    "       final_df['red_high_support_flag'] |\n",
    "       final_df['red_hypertensive_flag'] |\n",
    "       final_df['red_pulse_high_flag'] |\n",
    "       final_df['red_pulse_low_flag']) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['any_yellow'] = (\n",
    "    (final_df['yellow_resp_spo2_flag'] |\n",
    "    final_df['yellow_fio2_flag'] |\n",
    "    final_df['yellow_resp_rate_flag'] |\n",
    "    final_df['yellow_peep_flag'] |\n",
    "    final_df['yellow_map_flag'] |\n",
    "    final_df['yellow_pulse_flag'] |\n",
    "    final_df['yellow_lactate_flag']) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['any_green'] = (\n",
    "    (final_df['green_resp_spo2_flag'] |\n",
    "    final_df['green_resp_rate_flag'] |\n",
    "    final_df['green_fio2_flag'] |\n",
    "    final_df['green_peep_flag'] |\n",
    "    final_df['green_map_flag'] |\n",
    "    final_df['green_pulse_flag'] |\n",
    "    final_df['green_lactate_flag'] |\n",
    "    final_df['green_hr_flag']) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['all_green'] = (\n",
    "    final_df['green_resp_spo2_flag'] &\n",
    "    final_df['green_resp_rate_flag'] &\n",
    "    final_df['green_fio2_flag'] &\n",
    "    final_df['green_peep_flag'] &\n",
    "    final_df['green_map_flag'] &\n",
    "    final_df['green_pulse_flag'] &\n",
    "    final_df['green_lactate_flag'] &\n",
    "    final_df['green_hr_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['all_green_all_hours'] = (\n",
    "    final_df['green_resp_spo2_flag'] &\n",
    "    final_df['green_resp_rate_flag'] &\n",
    "    final_df['green_fio2_flag'] &\n",
    "    final_df['green_peep_flag'] &\n",
    "    final_df['green_map_flag'] &\n",
    "    final_df['green_pulse_flag'] &\n",
    "    final_df['green_lactate_flag'] &\n",
    "    final_df['green_hr_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['all_green_weekday'] = (\n",
    "    final_df['green_resp_spo2_flag'] &\n",
    "    final_df['green_resp_rate_flag'] &\n",
    "    final_df['green_fio2_flag'] &\n",
    "    final_df['green_peep_flag'] &\n",
    "    final_df['green_map_flag'] &\n",
    "    final_df['green_pulse_flag'] &\n",
    "    final_df['green_lactate_flag'] &\n",
    "    final_df['green_hr_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['is_weekday'] == True) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['all_green_no_red'] = (\n",
    "    final_df['green_resp_spo2_flag'] &\n",
    "    final_df['green_resp_rate_flag'] &\n",
    "    final_df['green_fio2_flag'] &\n",
    "    final_df['green_peep_flag'] &\n",
    "    final_df['green_map_flag'] &\n",
    "    final_df['green_pulse_flag'] &\n",
    "    final_df['green_lactate_flag'] &\n",
    "    final_df['green_hr_flag'] &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['all_green_no_red_all_hours'] = (\n",
    "    final_df['green_resp_spo2_flag'] &\n",
    "    final_df['green_resp_rate_flag'] &\n",
    "    final_df['green_fio2_flag'] &\n",
    "    final_df['green_peep_flag'] &\n",
    "    final_df['green_map_flag'] &\n",
    "    final_df['green_pulse_flag'] &\n",
    "    final_df['green_lactate_flag'] &\n",
    "    final_df['green_hr_flag'] &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['all_green_no_red_weekday'] = (\n",
    "    final_df['green_resp_spo2_flag'] &\n",
    "    final_df['green_resp_rate_flag'] &\n",
    "    final_df['green_fio2_flag'] &\n",
    "    final_df['green_peep_flag'] &\n",
    "    final_df['green_map_flag'] &\n",
    "    final_df['green_pulse_flag'] &\n",
    "    final_df['green_lactate_flag'] &\n",
    "    final_df['green_hr_flag'] &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['is_weekday'] == True) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['all_green_no_red_yellow'] = (\n",
    "    final_df['green_resp_spo2_flag'] &\n",
    "    final_df['green_resp_rate_flag'] &\n",
    "    final_df['green_fio2_flag'] &\n",
    "    final_df['green_peep_flag'] &\n",
    "    final_df['green_map_flag'] &\n",
    "    final_df['green_pulse_flag'] &\n",
    "    final_df['green_lactate_flag'] &\n",
    "    final_df['green_hr_flag'] &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['any_yellow'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['all_yellow_no_red_green'] = (\n",
    "    final_df['yellow_resp_spo2_flag'] &\n",
    "    final_df['yellow_fio2_flag'] &\n",
    "    final_df['yellow_resp_rate_flag'] &\n",
    "    final_df['yellow_peep_flag'] &\n",
    "    final_df['yellow_map_flag'] &\n",
    "    final_df['yellow_pulse_flag'] &\n",
    "    final_df['yellow_lactate_flag'] &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['any_green'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['any_yellow_no_red_green'] = (\n",
    "    (final_df['yellow_resp_spo2_flag'] |\n",
    "    final_df['yellow_fio2_flag'] |\n",
    "    final_df['yellow_resp_rate_flag'] |\n",
    "    final_df['yellow_peep_flag'] |\n",
    "    final_df['yellow_map_flag'] |\n",
    "    final_df['yellow_pulse_flag'] |\n",
    "    final_df['yellow_lactate_flag']) &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['any_green'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['any_yellow_or_green_no_red'] = (\n",
    "    (final_df['yellow_resp_spo2_flag'] |\n",
    "    final_df['yellow_fio2_flag'] |\n",
    "    final_df['yellow_resp_rate_flag'] |\n",
    "    final_df['yellow_peep_flag'] |\n",
    "    final_df['yellow_map_flag'] |\n",
    "    final_df['yellow_pulse_flag'] |\n",
    "    final_df['yellow_lactate_flag'] |\n",
    "    final_df['green_resp_spo2_flag'] |\n",
    "    final_df['green_resp_rate_flag'] |\n",
    "    final_df['green_fio2_flag'] |\n",
    "    final_df['green_peep_flag'] |\n",
    "    final_df['green_map_flag'] |\n",
    "    final_df['green_pulse_flag'] |\n",
    "    final_df['green_lactate_flag'] |\n",
    "    final_df['green_hr_flag']) &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['any_yellow_or_green_no_red_weekday'] = (\n",
    "    (final_df['yellow_resp_spo2_flag'] |\n",
    "    final_df['yellow_fio2_flag'] |\n",
    "    final_df['yellow_resp_rate_flag'] |\n",
    "    final_df['yellow_peep_flag'] |\n",
    "    final_df['yellow_map_flag'] |\n",
    "    final_df['yellow_pulse_flag'] |\n",
    "    final_df['yellow_lactate_flag'] |\n",
    "    final_df['green_resp_spo2_flag'] |\n",
    "    final_df['green_resp_rate_flag'] |\n",
    "    final_df['green_fio2_flag'] |\n",
    "    final_df['green_peep_flag'] |\n",
    "    final_df['green_map_flag'] |\n",
    "    final_df['green_pulse_flag'] |\n",
    "    final_df['green_lactate_flag'] |\n",
    "    final_df['green_hr_flag']) &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['is_weekday'] == True) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['any_yellow_or_green_no_red_all_hours'] = (\n",
    "    (final_df['yellow_resp_spo2_flag'] |\n",
    "    final_df['yellow_fio2_flag'] |\n",
    "    final_df['yellow_resp_rate_flag'] |\n",
    "    final_df['yellow_peep_flag'] |\n",
    "    final_df['yellow_map_flag'] |\n",
    "    final_df['yellow_pulse_flag'] |\n",
    "    final_df['yellow_lactate_flag'] |\n",
    "    final_df['green_resp_spo2_flag'] |\n",
    "    final_df['green_resp_rate_flag'] |\n",
    "    final_df['green_fio2_flag'] |\n",
    "    final_df['green_peep_flag'] |\n",
    "    final_df['green_map_flag'] |\n",
    "    final_df['green_pulse_flag'] |\n",
    "    final_df['green_lactate_flag'] |\n",
    "    final_df['green_hr_flag']) &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['yellow_resp_flag'] = (\n",
    "    (final_df['yellow_resp_spo2_flag'] |\n",
    "    final_df['yellow_fio2_flag'] |\n",
    "    final_df['yellow_resp_rate_flag'] |\n",
    "    final_df['yellow_peep_flag'] |\n",
    "    final_df['green_resp_spo2_flag'] |\n",
    "    final_df['green_resp_rate_flag'] |\n",
    "    final_df['green_fio2_flag'] |\n",
    "    final_df['green_peep_flag']) &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['yellow_cardio_flag'] = (\n",
    "    (final_df['yellow_map_flag'] |\n",
    "    final_df['yellow_pulse_flag'] |\n",
    "    final_df['yellow_lactate_flag'] |\n",
    "    final_df['green_map_flag'] |\n",
    "    final_df['green_pulse_flag'] |\n",
    "    final_df['green_lactate_flag'] |\n",
    "    final_df['green_hr_flag']) &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['yellow_all_green'] = (\n",
    "    final_df['all_green_no_red'] &\n",
    "    (final_df['any_yellow'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "final_df['yellow_not_all_green'] = (\n",
    "    final_df['any_yellow_or_green_no_red'] &\n",
    "    (final_df['all_green_no_red'] == 0)\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26646fa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:10.508580Z",
     "iopub.status.busy": "2025-05-05T20:36:10.508425Z",
     "iopub.status.idle": "2025-05-05T20:36:10.543671Z",
     "shell.execute_reply": "2025-05-05T20:36:10.543264Z"
    },
    "papermill": {
     "duration": 0.043148,
     "end_time": "2025-05-05T20:36:10.544614",
     "exception": false,
     "start_time": "2025-05-05T20:36:10.501466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print value counts for each flag\n",
    "print(final_df[['any_red', 'any_yellow', 'any_green' ,  'all_green',\n",
    "                'all_green_no_red', 'all_green_no_red_yellow', 'all_yellow_no_red_green', \n",
    "                'any_yellow_no_red_green','any_yellow_or_green_no_red','no_red' ,'yellow_all_green',\n",
    "                 'yellow_not_all_green' ]].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5530d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:10.558381Z",
     "iopub.status.busy": "2025-05-05T20:36:10.558201Z",
     "iopub.status.idle": "2025-05-05T20:36:11.986352Z",
     "shell.execute_reply": "2025-05-05T20:36:11.985967Z"
    },
    "papermill": {
     "duration": 1.43621,
     "end_time": "2025-05-05T20:36:11.987403",
     "exception": false,
     "start_time": "2025-05-05T20:36:10.551193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df.to_parquet(f'../output/intermediate/final_df_w_criteria.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be41bf",
   "metadata": {
    "papermill": {
     "duration": 0.006368,
     "end_time": "2025-05-05T20:36:12.000188",
     "exception": false,
     "start_time": "2025-05-05T20:36:11.993820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TableOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc5a48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:12.055155Z",
     "iopub.status.busy": "2025-05-05T20:36:12.054994Z",
     "iopub.status.idle": "2025-05-05T20:36:12.401969Z",
     "shell.execute_reply": "2025-05-05T20:36:12.401648Z"
    },
    "papermill": {
     "duration": 0.35474,
     "end_time": "2025-05-05T20:36:12.402856",
     "exception": false,
     "start_time": "2025-05-05T20:36:12.048116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Create TableOne\")\n",
    "# Create criteria_results df with one row per encounter showing if criteria were ever met\n",
    "criteria_block_results = final_df.groupby('encounter_block').agg({\n",
    "    'patel_flag': 'max',  # 1 if criteria ever met\n",
    "    'team_flag': 'max',\n",
    "    'any_yellow_or_green_no_red': 'max',\n",
    "    'all_green': 'max',\n",
    "    'all_green_no_red': 'max'\n",
    "}).reset_index()\n",
    "\n",
    "final_df_blocks_merged = final_df_blocks.merge(criteria_block_results, on='encounter_block', how='left')\n",
    "\n",
    "#  aggregate the values from final_df at encounter_block level\n",
    "vaso_peep_fio2_stats = final_df.groupby('encounter_block').agg({\n",
    "    'ne_calc_last': 'max',  # If any value is > 0, the block received vasopressors\n",
    "    'max_peep_set': 'mean',  # Average of max PEEP in the block\n",
    "    'min_fio2_set': 'mean'   # Average of min FiO2 in the block\n",
    "}).reset_index()\n",
    "\n",
    "# Merge these stats with final_df_blocks\n",
    "all_encounters = final_df_blocks_merged.copy()\n",
    "all_encounters = pd.merge(all_encounters, vaso_peep_fio2_stats, on='encounter_block', how='left')\n",
    "\n",
    "# Create subsets and map race for each\n",
    "def map_race_column(df, race_column='race'):\n",
    "    race_mapping = {\n",
    "        'Black or African-American': 'Black',\n",
    "        'Black or African American': 'Black',\n",
    "        'White': 'White',\n",
    "        'Asian': 'Other',\n",
    "        'American Indian or Alaska Native': 'Other',\n",
    "        'Native Hawaiian or Other Pacific Islander': 'Other',\n",
    "        'Other': 'Other',\n",
    "        'Unknown': 'Other'\n",
    "    }\n",
    "    df['race_new'] = df[race_column].map(race_mapping).fillna('Missing')\n",
    "    return df\n",
    "\n",
    "# Map race and create subsets\n",
    "all_encounters = map_race_column(all_encounters, 'race_category')\n",
    "patel_subset = map_race_column(all_encounters[all_encounters['patel_flag'] == 1].copy(), 'race_category')\n",
    "team_subset = map_race_column(all_encounters[all_encounters['team_flag'] == 1].copy(), 'race_category')\n",
    "yellow_subset = map_race_column(all_encounters[all_encounters['any_yellow_or_green_no_red'] == 1].copy(), 'race_category')\n",
    "green_subset = map_race_column(all_encounters[all_encounters['all_green'] == 1].copy(), 'race_category')\n",
    "green_no_red_subset = map_race_column(all_encounters[all_encounters['all_green_no_red'] == 1].copy(), 'race_category')\n",
    "\n",
    "# Calculate vasopressor usage for each subset\n",
    "def calculate_vasopressor_stats(df):\n",
    "    # Count encounters with any vasopressor use (ne_calc_last > 0)\n",
    "    vaso_usage = df['ne_calc_last'].notna() & (df['ne_calc_last'] > 0)\n",
    "    n_vaso = vaso_usage.sum()\n",
    "    n_zero = (df['ne_calc_last'] == 0).sum()\n",
    "    n_missing = df['ne_calc_last'].isna().sum()\n",
    "    total = len(df)\n",
    "    return n_vaso, n_zero, n_missing, total\n",
    "\n",
    "# Calculate stats for each group\n",
    "vaso_stats = {\n",
    "    'All Encounters': calculate_vasopressor_stats(all_encounters),\n",
    "    'Patel Criteria': calculate_vasopressor_stats(patel_subset),\n",
    "    'TEAM Criteria': calculate_vasopressor_stats(team_subset),\n",
    "    'Yellow Criteria': calculate_vasopressor_stats(yellow_subset),\n",
    "    'Green Criteria': calculate_vasopressor_stats(green_subset),\n",
    "    'Green-No-Red Criteria': calculate_vasopressor_stats(green_no_red_subset)\n",
    "}\n",
    "\n",
    "# Define variables for the table\n",
    "categorical = ['sex_category', 'race_new', 'ethnicity_category', \n",
    "              'location_category', 'is_dead']\n",
    "\n",
    "continuous = ['age_at_admission', 'sofa_cv_97', 'sofa_coag', 'sofa_renal',\n",
    "             'sofa_liver', 'sofa_resp', 'sofa_cns', 'sofa_total',\n",
    "             'ne_calc_last', 'max_peep_set', 'min_fio2_set']\n",
    "\n",
    "# Create individual tables\n",
    "# All Encounters - This will be our template\n",
    "table_all = TableOne(all_encounters, \n",
    "                    columns=categorical + continuous,\n",
    "                    categorical=categorical,\n",
    "                    groupby=None,\n",
    "                    nonnormal=continuous,\n",
    "                    pval=False)\n",
    "df_all = table_all.tableone.reset_index()\n",
    "\n",
    "# Filter out the 'n' row from the template\n",
    "# df_all = df_all[~((df_all['level_0'] == 'n') & (df_all['level_1'].isna()))]\n",
    "\n",
    "# Get the last column and the index columns\n",
    "df_template = pd.DataFrame({\n",
    "    'Characteristics': df_all['level_0'],\n",
    "    'Category': df_all['level_1'],\n",
    "    'All Encounters': df_all[df_all.columns[-1]]\n",
    "})\n",
    "\n",
    "# Function to process each criteria subset\n",
    "def process_criteria_subset(subset_df, criteria_name, template):\n",
    "    table = TableOne(subset_df,\n",
    "                    columns=categorical + continuous,\n",
    "                    categorical=categorical,\n",
    "                    groupby=None,\n",
    "                    nonnormal=continuous,\n",
    "                    pval=False)\n",
    "    df = table.tableone.reset_index()\n",
    "    \n",
    "    # Filter out the 'n' row\n",
    "    df = df[~((df['level_0'] == 'n') & (df['level_1'].isna()))]\n",
    "    \n",
    "    # Create a DataFrame with the same structure as template\n",
    "    result = pd.DataFrame({\n",
    "        'Characteristics': df['level_0'],\n",
    "        'Category': df['level_1'],\n",
    "        criteria_name: df[df.columns[-1]]\n",
    "    })\n",
    "    \n",
    "    # Merge with template to ensure all categories are present\n",
    "    merged = pd.merge(template[['Characteristics', 'Category']], \n",
    "                     result,\n",
    "                     on=['Characteristics', 'Category'],\n",
    "                     how='left')\n",
    "    \n",
    "    return merged[criteria_name]\n",
    "\n",
    "# Process each criteria subset\n",
    "patel_col = process_criteria_subset(patel_subset, 'Patel Criteria', df_template)\n",
    "team_col = process_criteria_subset(team_subset, 'TEAM Criteria', df_template)\n",
    "yellow_col = process_criteria_subset(yellow_subset, 'Yellow Criteria', df_template)\n",
    "green_col = process_criteria_subset(green_subset, 'Green Criteria', df_template)\n",
    "green_no_red_col = process_criteria_subset(green_no_red_subset, 'Green-No-Red Criteria', df_template)\n",
    "\n",
    "# Combine all columns\n",
    "final_table = pd.concat([\n",
    "    df_template[['Characteristics', 'Category', 'All Encounters']],\n",
    "    patel_col,\n",
    "    team_col,\n",
    "    yellow_col,\n",
    "    green_col,\n",
    "    green_no_red_col\n",
    "], axis=1)\n",
    "\n",
    "# Clean up the table\n",
    "# Remove the 'Missing' category if it exists and has count of 0\n",
    "final_table = final_table[~((final_table['Category'] == 'Missing') & \n",
    "                          (final_table['All Encounters'].str.startswith('0')))]\n",
    "\n",
    "# Format mortality rows\n",
    "mortality_rows = final_table.loc[final_table['Characteristics'] == 'is_dead']\n",
    "for col in final_table.columns[2:]:  # Skip 'Characteristics' and 'Category'\n",
    "    if col == 'All Encounters':\n",
    "        total = len(all_encounters)\n",
    "        deaths = all_encounters['is_dead'].sum()\n",
    "    elif col == 'Patel Criteria':\n",
    "        total = len(patel_subset)\n",
    "        deaths = patel_subset['is_dead'].sum()\n",
    "    elif col == 'TEAM Criteria':\n",
    "        total = len(team_subset)\n",
    "        deaths = team_subset['is_dead'].sum()\n",
    "    elif col == 'Yellow Criteria':\n",
    "        total = len(yellow_subset)\n",
    "        deaths = yellow_subset['is_dead'].sum()\n",
    "    elif col == 'Green Criteria':\n",
    "        total = len(green_subset)\n",
    "        deaths = green_subset['is_dead'].sum()\n",
    "    else:  # Green No Red Criteria\n",
    "        total = len(green_no_red_subset)\n",
    "        deaths = green_no_red_subset['is_dead'].sum()\n",
    "    \n",
    "    percentage = (deaths / total * 100) if total > 0 else 0\n",
    "    mortality_rows.loc[mortality_rows['Category'] == '1', col] = f\"{deaths} ({percentage:.1f})\"\n",
    "\n",
    "# Replace the original mortality rows\n",
    "final_table.loc[final_table['Characteristics'] == 'is_dead'] = mortality_rows\n",
    "\n",
    "# # Clean up labels\n",
    "final_table.loc[final_table['Characteristics'] == 'is_dead', 'Characteristics'] = 'Mortality'\n",
    "# final_table.loc[final_table['Category'] == '1', 'Category'] = ''\n",
    "\n",
    "# Add vasopressor usage rows\n",
    "vaso_rows = []\n",
    "for status in ['Received Vasopressors', 'No Vasopressors', 'Missing Vasopressor Data']:\n",
    "    row_data = {'Characteristics': 'Vasopressor Status', 'Category': status}\n",
    "    for col in final_table.columns[2:]:  # Skip 'Characteristics' and 'Category'\n",
    "        if col in vaso_stats:\n",
    "            n_vaso, n_zero, n_missing, total = vaso_stats[col]\n",
    "            if status == 'Received Vasopressors':\n",
    "                value = n_vaso\n",
    "            elif status == 'No Vasopressors':\n",
    "                value = n_zero\n",
    "            else:  # Missing Vasopressor Data\n",
    "                value = n_missing\n",
    "            \n",
    "            percentage = (value / total * 100) if total > 0 else 0\n",
    "            row_data[col] = f\"{value} ({percentage:.1f})\"\n",
    "    vaso_rows.append(row_data)\n",
    "\n",
    "vaso_df = pd.DataFrame(vaso_rows)\n",
    "final_table = pd.concat([final_table, vaso_df], ignore_index=True)\n",
    "\n",
    "# Add n row at the top (only once)\n",
    "n_row = pd.DataFrame({\n",
    "    'Characteristics': ['n'],\n",
    "    'Category': [''],\n",
    "    'All Encounters': [str(len(all_encounters))],\n",
    "    'Patel Criteria': [str(len(patel_subset))],\n",
    "    'TEAM Criteria': [str(len(team_subset))],\n",
    "    'Yellow Criteria': [str(len(yellow_subset))],\n",
    "    'Green Criteria': [str(len(green_subset))],\n",
    "    'Green-No-Red Criteria': [str(len(green_no_red_subset))]\n",
    "})\n",
    "\n",
    "final_table = pd.concat([n_row, final_table]).reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "final_table.to_csv('../output/final/table1_results.csv', index=False)\n",
    "\n",
    "print(\"Table 1 has been generated and saved to table1_results.csv\")\n",
    "final_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6815456d",
   "metadata": {},
   "source": [
    "## TableOne - 72 hours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a385a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Generating TableOne Results ===\")\n",
    "\n",
    "def create_tableone_analysis(\n",
    "    final_df, \n",
    "    final_df_blocks, \n",
    "    analysis_type=\"72h\",\n",
    "    time_filter=None,\n",
    "    output_suffix=\"\",\n",
    "    description=\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate TableOne analysis for different scenarios\n",
    "    \n",
    "    Parameters:\n",
    "    - final_df: Hourly data\n",
    "    - final_df_blocks: Block-level data  \n",
    "    - analysis_type: \"72h\", \"baseline\", or \"custom\"\n",
    "    - time_filter: Function to filter time data\n",
    "    - output_suffix: Suffix for output filename\n",
    "    - description: Description for logging\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n── {description} ──\")\n",
    "    \n",
    "    # 1. Apply time filtering\n",
    "    if time_filter:\n",
    "        final_df_filtered = time_filter(final_df)\n",
    "    else:\n",
    "        final_df_filtered = final_df.copy()\n",
    "    \n",
    "    print(f\"Encounters after filtering: {final_df_filtered['encounter_block'].nunique()}\")\n",
    "    \n",
    "    # 2. Get eligibility status based on analysis type\n",
    "    if analysis_type == \"72h\":\n",
    "        # Max eligibility over 72 hours\n",
    "        criteria_results = final_df_filtered.groupby('encounter_block').agg({\n",
    "            'patel_flag': 'max',\n",
    "            'team_flag': 'max',\n",
    "            'any_yellow_or_green_no_red': 'max',\n",
    "            'all_green': 'max',\n",
    "            'all_green_no_red': 'max'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Clinical values: aggregated over time\n",
    "        clinical_stats = final_df.groupby('encounter_block').agg({\n",
    "            'ne_calc_last': 'max',\n",
    "            'max_peep_set': 'mean',\n",
    "            'min_fio2_set': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "    elif analysis_type == \"baseline\":\n",
    "        # Eligibility over 72h, but clinical values at baseline\n",
    "        final_df_72h = final_df.query(\"time_from_vent <= 72\")\n",
    "        criteria_results = final_df_72h.groupby('encounter_block').agg({\n",
    "            'patel_flag': 'max',\n",
    "            'team_flag': 'max', \n",
    "            'any_yellow_or_green_no_red': 'max',\n",
    "            'all_green': 'max',\n",
    "            'all_green_no_red': 'max'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Clinical values: at time 0 only\n",
    "        final_df_t0 = final_df[final_df['time_from_vent'] == 0]\n",
    "        clinical_stats = final_df_t0.groupby('encounter_block').agg({\n",
    "            'ne_calc_last': 'first',\n",
    "            'max_peep_set': 'first', \n",
    "            'min_fio2_set': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "    else:  # custom\n",
    "        # Use provided filtered data as-is\n",
    "        criteria_results = final_df_filtered.groupby('encounter_block').agg({\n",
    "            'patel_flag': 'max',\n",
    "            'team_flag': 'max',\n",
    "            'any_yellow_or_green_no_red': 'max', \n",
    "            'all_green': 'max',\n",
    "            'all_green_no_red': 'max'\n",
    "        }).reset_index()\n",
    "        \n",
    "        clinical_stats = final_df_filtered.groupby('encounter_block').agg({\n",
    "            'ne_calc_last': 'max',\n",
    "            'max_peep_set': 'mean',\n",
    "            'min_fio2_set': 'mean'\n",
    "        }).reset_index()\n",
    "    \n",
    "    # 3. Merge everything\n",
    "    analysis_blocks = final_df_blocks[\n",
    "        final_df_blocks['encounter_block'].isin(criteria_results['encounter_block'])\n",
    "    ].copy()\n",
    "    \n",
    "    analysis_blocks = analysis_blocks.merge(criteria_results, on='encounter_block', how='left')\n",
    "    analysis_blocks = analysis_blocks.merge(clinical_stats, on='encounter_block', how='left')\n",
    "    \n",
    "    # Fill NaN eligibility flags with 0\n",
    "    eligibility_cols = ['patel_flag', 'team_flag', 'any_yellow_or_green_no_red', 'all_green', 'all_green_no_red']\n",
    "    for col in eligibility_cols:\n",
    "        analysis_blocks[col] = analysis_blocks[col].fillna(0)\n",
    "    \n",
    "    # 4. Create subsets and generate table\n",
    "    result_table = generate_tableone_for_subsets(analysis_blocks, description)\n",
    "    \n",
    "    # 5. Save results\n",
    "    filename = f'../output/final/table1_results{output_suffix}.csv'\n",
    "    result_table.to_csv(filename, index=False)\n",
    "    print(f\"[SAVED] {filename}\")\n",
    "    \n",
    "    return result_table, analysis_blocks\n",
    "\n",
    "def generate_tableone_for_subsets(all_encounters, description=\"\"):\n",
    "    \"\"\"Generate TableOne for all eligibility subsets\"\"\"\n",
    "    \n",
    "    # Race mapping function\n",
    "    def map_race_column(df, race_column='race_category'):\n",
    "        race_mapping = {\n",
    "            'Black or African-American': 'Black',\n",
    "            'Black or African American': 'Black',\n",
    "            'White': 'White',\n",
    "            'Asian': 'Other',\n",
    "            'American Indian or Alaska Native': 'Other',\n",
    "            'Native Hawaiian or Other Pacific Islander': 'Other',\n",
    "            'Other': 'Other',\n",
    "            'Unknown': 'Other'\n",
    "        }\n",
    "        df['race_new'] = df[race_column].map(race_mapping).fillna('Missing')\n",
    "        return df\n",
    "    \n",
    "    # Create all subsets\n",
    "    all_encounters = map_race_column(all_encounters)\n",
    "    subsets = {\n",
    "        'All Encounters': all_encounters,\n",
    "        'Patel Criteria': all_encounters[all_encounters['patel_flag'] == 1].copy(),\n",
    "        'TEAM Criteria': all_encounters[all_encounters['team_flag'] == 1].copy(),\n",
    "        'Yellow Criteria': all_encounters[all_encounters['any_yellow_or_green_no_red'] == 1].copy(),\n",
    "        'Green Criteria': all_encounters[all_encounters['all_green'] == 1].copy(),\n",
    "        'Green-No-Red Criteria': all_encounters[all_encounters['all_green_no_red'] == 1].copy()\n",
    "    }\n",
    "    \n",
    "    # Apply race mapping to all subsets\n",
    "    for name, df in subsets.items():\n",
    "        if len(df) > 0:\n",
    "            subsets[name] = map_race_column(df)\n",
    "    \n",
    "    # Print subset sizes\n",
    "    print(f\"\\nSubset sizes for {description}:\")\n",
    "    for name, df in subsets.items():\n",
    "        print(f\"  {name}: {len(df)}\")\n",
    "    \n",
    "    # Calculate vasopressor stats\n",
    "    def calculate_vasopressor_stats(df):\n",
    "        if len(df) == 0:\n",
    "            return 0, 0, 0, 0\n",
    "        vaso_usage = df['ne_calc_last'].notna() & (df['ne_calc_last'] > 0)\n",
    "        n_vaso = vaso_usage.sum()\n",
    "        n_zero = (df['ne_calc_last'] == 0).sum()\n",
    "        n_missing = df['ne_calc_last'].isna().sum()\n",
    "        total = len(df)\n",
    "        return n_vaso, n_zero, n_missing, total\n",
    "    \n",
    "    vaso_stats = {name: calculate_vasopressor_stats(df) for name, df in subsets.items()}\n",
    "    \n",
    "    # Define table variables\n",
    "    categorical = ['sex_category', 'race_new', 'ethnicity_category', 'location_category', 'is_dead']\n",
    "    continuous = ['age_at_admission', 'sofa_cv_97', 'sofa_coag', 'sofa_renal',\n",
    "                 'sofa_liver', 'sofa_resp', 'sofa_cns', 'sofa_total',\n",
    "                 'ne_calc_last', 'max_peep_set', 'min_fio2_set']\n",
    "    \n",
    "    # Create template from all encounters\n",
    "    table_all = TableOne(all_encounters,\n",
    "                        columns=categorical + continuous,\n",
    "                        categorical=categorical,\n",
    "                        groupby=None,\n",
    "                        nonnormal=continuous,\n",
    "                        pval=False)\n",
    "    df_template = table_all.tableone.reset_index()\n",
    "    df_template = pd.DataFrame({\n",
    "        'Characteristics': df_template['level_0'],\n",
    "        'Category': df_template['level_1'],\n",
    "        'All Encounters': df_template[df_template.columns[-1]]\n",
    "    })\n",
    "    \n",
    "    # Process each subset\n",
    "    def process_subset(subset_df, criteria_name, template):\n",
    "        if len(subset_df) == 0:\n",
    "            return pd.Series(['0'] * len(template), name=criteria_name)\n",
    "            \n",
    "        table = TableOne(subset_df,\n",
    "                        columns=categorical + continuous,\n",
    "                        categorical=categorical,\n",
    "                        groupby=None,\n",
    "                        nonnormal=continuous,\n",
    "                        pval=False)\n",
    "        df = table.tableone.reset_index()\n",
    "        \n",
    "        result = pd.DataFrame({\n",
    "            'Characteristics': df['level_0'],\n",
    "            'Category': df['level_1'],\n",
    "            criteria_name: df[df.columns[-1]]\n",
    "        })\n",
    "        \n",
    "        merged = pd.merge(template[['Characteristics', 'Category']], \n",
    "                         result,\n",
    "                         on=['Characteristics', 'Category'],\n",
    "                         how='left')\n",
    "        \n",
    "        return merged[criteria_name].fillna('0 (0.0)')\n",
    "    \n",
    "    # Generate columns for each subset\n",
    "    result_columns = [df_template[['Characteristics', 'Category', 'All Encounters']]]\n",
    "    \n",
    "    for name, df in list(subsets.items())[1:]:  # Skip 'All Encounters'\n",
    "        col = process_subset(df, name, df_template)\n",
    "        result_columns.append(col)\n",
    "    \n",
    "    # Combine all columns\n",
    "    final_table = pd.concat(result_columns, axis=1)\n",
    "    \n",
    "    # Add formatted mortality and vasopressor rows\n",
    "    final_table = add_formatted_rows(final_table, subsets, vaso_stats)\n",
    "    \n",
    "    return final_table\n",
    "\n",
    "def add_formatted_rows(final_table, subsets, vaso_stats):\n",
    "    \"\"\"Add properly formatted mortality and vasopressor rows\"\"\"\n",
    "    \n",
    "    # Format mortality rows\n",
    "    mortality_mask = final_table['Characteristics'] == 'is_dead'\n",
    "    for col in final_table.columns[2:]:  # Skip Characteristics and Category\n",
    "        if col in subsets:\n",
    "            df = subsets[col]\n",
    "            total = len(df)\n",
    "            deaths = df['is_dead'].sum() if len(df) > 0 else 0\n",
    "            percentage = (deaths / total * 100) if total > 0 else 0\n",
    "            final_table.loc[mortality_mask & (final_table['Category'] == '1'), col] = f\"{deaths} ({percentage:.1f})\"\n",
    "    \n",
    "    # Clean up mortality labels\n",
    "    final_table.loc[final_table['Characteristics'] == 'is_dead', 'Characteristics'] = 'Mortality'\n",
    "    \n",
    "    # Add vasopressor rows\n",
    "    vaso_rows = []\n",
    "    for status in ['Received Vasopressors', 'No Vasopressors', 'Missing Vasopressor Data']:\n",
    "        row_data = {'Characteristics': 'Vasopressor Status', 'Category': status}\n",
    "        for col in final_table.columns[2:]:\n",
    "            if col in vaso_stats:\n",
    "                n_vaso, n_zero, n_missing, total = vaso_stats[col]\n",
    "                if status == 'Received Vasopressors':\n",
    "                    value = n_vaso\n",
    "                elif status == 'No Vasopressors':\n",
    "                    value = n_zero\n",
    "                else:\n",
    "                    value = n_missing\n",
    "                \n",
    "                percentage = (value / total * 100) if total > 0 else 0\n",
    "                row_data[col] = f\"{value} ({percentage:.1f})\"\n",
    "            else:\n",
    "                row_data[col] = \"0 (0.0)\"\n",
    "        vaso_rows.append(row_data)\n",
    "    \n",
    "    vaso_df = pd.DataFrame(vaso_rows)\n",
    "    final_table = pd.concat([final_table, vaso_df], ignore_index=True)\n",
    "    \n",
    "    # Add n row at the top\n",
    "    n_row = pd.DataFrame({\n",
    "        'Characteristics': ['n'],\n",
    "        'Category': [''],\n",
    "        **{col: [str(len(subsets[col]))] for col in final_table.columns[2:] if col in subsets}\n",
    "    })\n",
    "    \n",
    "    final_table = pd.concat([n_row, final_table]).reset_index(drop=True)\n",
    "    \n",
    "    return final_table\n",
    "\n",
    "# ── GENERATE ALL TABLEONE ANALYSES ──\n",
    "\n",
    "# 1. Standard 72-hour analysis\n",
    "table1_72h, blocks_72h = create_tableone_analysis(\n",
    "    final_df=final_df,\n",
    "    final_df_blocks=final_df_blocks,\n",
    "    analysis_type=\"72h\",\n",
    "    time_filter=lambda df: df.query(\"time_from_vent <= 72\"),\n",
    "    output_suffix=\"_72hrs\",\n",
    "    description=\"TableOne: 72-Hour Analysis\"\n",
    ")\n",
    "\n",
    "# 2. Baseline characteristics of eventually eligible patients\n",
    "table1_baseline, blocks_baseline = create_tableone_analysis(\n",
    "    final_df=final_df,\n",
    "    final_df_blocks=final_df_blocks,\n",
    "    analysis_type=\"baseline\", \n",
    "    time_filter=None,\n",
    "    output_suffix=\"_baseline_72hrs\",\n",
    "    description=\"TableOne: Baseline Characteristics (Eventually Eligible)\"\n",
    ")\n",
    "\n",
    "print(\"\\n[OK] All TableOne analyses completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce578b",
   "metadata": {
    "papermill": {
     "duration": 0.006866,
     "end_time": "2025-05-05T20:36:12.721557",
     "exception": false,
     "start_time": "2025-05-05T20:36:12.714691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Missingess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f1256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:12.736050Z",
     "iopub.status.busy": "2025-05-05T20:36:12.735862Z",
     "iopub.status.idle": "2025-05-05T20:36:12.740106Z",
     "shell.execute_reply": "2025-05-05T20:36:12.739676Z"
    },
    "papermill": {
     "duration": 0.012951,
     "end_time": "2025-05-05T20:36:12.741178",
     "exception": false,
     "start_time": "2025-05-05T20:36:12.728227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Check missingness by criteria\")\n",
    "key_variables =['encounter_block','hospitalization_id', \n",
    "                'recorded_date'\t,'recorded_hour', 'time_from_vent',\n",
    "                'hourly_trach','paralytics_flag',]\n",
    "\n",
    "reqd_team_fields = ['hourly_trach','paralytics_flag',\n",
    "                    'lactate', 'max_heart_rate', 'ne_calc_last',\n",
    "                    'last_ne_dose_last_6_hours', 'min_fio2_set', 'max_peep_set', \n",
    "                    'max_resp_rate_obs', \"team_pulse_flag\", \"team_lactate_flag\", \"team_ne_flag\",\n",
    "                    \"team_fio2_flag\",  \"team_peep_flag\",    \"team_resp_rate_flag\"]\n",
    "\n",
    "reqd_yellow_fields =[\n",
    "    # Clinical Measurements\n",
    "    'min_spo2', 'min_map', 'max_map', 'ne_calc_last', 'max_sbp', \"avg_map\",\n",
    "    'max_heart_rate', 'min_heart_rate', 'min_fio2_set',\n",
    "    'max_resp_rate_obs', 'min_peep_set', 'lactate',\n",
    "    \n",
    "    # Red Flags\n",
    "    'red_resp_spo2_flag', 'red_map_flag', 'red_high_support_flag',\n",
    "    'red_hypertensive_flag', 'red_pulse_high_flag', 'red_pulse_low_flag',\n",
    "    'red_meds_flag',\n",
    "    \n",
    "    # Yellow Flags\n",
    "    'yellow_resp_spo2_flag', 'yellow_fio2_flag', 'yellow_resp_rate_flag',\n",
    "    'yellow_peep_flag', 'yellow_map_flag', 'yellow_pulse_flag',\n",
    "    'yellow_lactate_flag',\n",
    "    \n",
    "    # Green Flags\n",
    "    'green_resp_spo2_flag', 'green_resp_rate_flag', 'green_fio2_flag',\n",
    "    'green_peep_flag', 'green_map_flag', 'green_pulse_flag',\n",
    "    'green_lactate_flag', 'green_hr_flag',\n",
    "    \n",
    "    # Composite Flags\n",
    "    'any_red', 'any_yellow', 'any_green', 'all_green',\n",
    "    'all_green_no_red', 'all_green_no_red_yellow',\n",
    "    'all_yellow_no_red_green', 'any_yellow_no_red_green',\n",
    "    'any_yellow_or_green_no_red', 'yellow_resp_flag',\n",
    "    'yellow_cardio_flag', 'yellow_all_green', 'yellow_not_all_green'\n",
    "]\n",
    "\n",
    "reqd_patel_fields = ['min_map', 'max_map','max_sbp', 'min_sbp',\"avg_map\",\n",
    "                   'min_heart_rate','max_heart_rate', 'min_respiratory_rate','min_spo2', \n",
    "                    'max_respiratory_rate','patel_map_flag','patel_sbp_flag',\n",
    "                    'patel_pulse_flag', 'patel_resp_rate_flag' , 'patel_spo2_flag', \n",
    "                    'patel_resp_flag', 'patel_cardio_flag' ]\n",
    "\n",
    "reqd_green_fields =[\n",
    "    # Clinical Measurements\n",
    "    'min_spo2', 'min_map', 'max_map', 'ne_calc_last', 'max_sbp',\"avg_map\",\n",
    "    'max_heart_rate', 'min_heart_rate', 'min_fio2_set',\n",
    "    'max_resp_rate_obs', 'min_peep_set', 'lactate',\n",
    "\n",
    "    \n",
    "    # Green Flags\n",
    "    'green_resp_spo2_flag', 'green_resp_rate_flag', 'green_fio2_flag',\n",
    "    'green_peep_flag', 'green_map_flag', 'green_pulse_flag',\n",
    "    'green_lactate_flag', 'green_hr_flag',\n",
    "    \n",
    "    # Composite Flags\n",
    "     'all_green',\n",
    "    'all_green_no_red', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879a9ff6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:12.755773Z",
     "iopub.status.busy": "2025-05-05T20:36:12.755569Z",
     "iopub.status.idle": "2025-05-05T20:36:20.444049Z",
     "shell.execute_reply": "2025-05-05T20:36:20.443737Z"
    },
    "papermill": {
     "duration": 7.696713,
     "end_time": "2025-05-05T20:36:20.444842",
     "exception": false,
     "start_time": "2025-05-05T20:36:12.748129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate percentage of missing values per encounter block\n",
    "def calculate_missing_percentage(df, variable_list, exclude_flags=True):\n",
    "    # Filter out flag variables if requested\n",
    "    if exclude_flags:\n",
    "        vars_to_check = [var for var in variable_list if 'flag' not in var.lower()]\n",
    "    else:\n",
    "        vars_to_check = variable_list\n",
    "        \n",
    "    # Remove key variables that are administrative\n",
    "    vars_to_check = [var for var in vars_to_check if var not in ['encounter_block', 'hospitalization_id', \n",
    "                                                                'recorded_dttm', 'recorded_date', 'recorded_hour', \n",
    "                                                                'time_from_vent', 'hourly_trach', 'paralytics_flag']]\n",
    "    \n",
    "    # Calculate percentage of blocks where variable was never measured\n",
    "    missing_pct = {}\n",
    "    total_blocks = df['encounter_block'].nunique()\n",
    "    \n",
    "    for var in vars_to_check:\n",
    "        blocks_never_measured = df.groupby('encounter_block')[var].apply(lambda x: x.isna().all()).sum()\n",
    "        missing_pct[var] = (blocks_never_measured / total_blocks) * 100\n",
    "        \n",
    "    return pd.Series(missing_pct).sort_values(ascending=False)\n",
    "\n",
    "# Calculate for each criteria set\n",
    "team_missing = calculate_missing_percentage(final_df, reqd_team_fields)\n",
    "yellow_missing = calculate_missing_percentage(final_df, reqd_yellow_fields)\n",
    "patel_missing = calculate_missing_percentage(final_df, reqd_patel_fields)\n",
    "green_missing = calculate_missing_percentage(final_df, reqd_green_fields)\n",
    "green_no_red_missing = calculate_missing_percentage(final_df, reqd_yellow_fields)\n",
    "\n",
    "# Print tabular summaries\n",
    "print(\"\\nTEAM Criteria Missing Data Summary:\")\n",
    "print(team_missing.round(2))\n",
    "print(\"\\nYellow Criteria Missing Data Summary:\")\n",
    "print(yellow_missing.round(2))\n",
    "print(\"\\nPatel Criteria Missing Data Summary:\")\n",
    "print(patel_missing.round(2))\n",
    "print(\"\\nGreen No Red Criteria Missing Data Summary:\")\n",
    "print(green_no_red_missing.round(2))\n",
    "\n",
    "team_missing.to_csv('../output/final/team_missing_data.csv')\n",
    "yellow_missing.to_csv('../output/final/yellow_missing_data.csv')\n",
    "patel_missing.to_csv('../output/final/patel_missing_data.csv')\n",
    "green_missing.to_csv('../output/final/green_missing_data.csv')\n",
    "green_no_red_missing.to_csv('../output/final/green_no_red_missing_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342498d1",
   "metadata": {
    "papermill": {
     "duration": 0.008376,
     "end_time": "2025-05-05T20:36:20.461061",
     "exception": false,
     "start_time": "2025-05-05T20:36:20.452685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Competing Risk Analysis Setup\n",
    "\n",
    "Create a dataframe for each criteria with the following columns \n",
    "\n",
    "1. encounter_block: identify the patient encounter\n",
    "2. time_eligibility: earliest eligibility time from first intubation episode per encounter block\n",
    "3. time_death: time from ventilation start to death, if applicable. Missing if not dead\n",
    "4. time_discharge_alive: time from ventilation start to discharge. If not dead, assumed discharged and the last recorded vital time is discharge time.\n",
    "5. t_event: earliest of the above three times\n",
    "6. outcome: 1(eligibility), 2(death), 3(discharge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f477c1",
   "metadata": {
    "papermill": {
     "duration": 0.008038,
     "end_time": "2025-05-05T20:36:20.476880",
     "exception": false,
     "start_time": "2025-05-05T20:36:20.468842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Competing risk updated (4/14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fa796",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:20.493080Z",
     "iopub.status.busy": "2025-05-05T20:36:20.492901Z",
     "iopub.status.idle": "2025-05-05T20:36:20.499638Z",
     "shell.execute_reply": "2025-05-05T20:36:20.499369Z"
    },
    "papermill": {
     "duration": 0.016036,
     "end_time": "2025-05-05T20:36:20.500412",
     "exception": false,
     "start_time": "2025-05-05T20:36:20.484376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Competing Risk Analysis setup\")\n",
    "##############################################################################\n",
    "# Helper: build block‑level data set for competing‑risk analysis\n",
    "##############################################################################\n",
    "def create_competing_risk_dataset(\n",
    "    criteria_df: pd.DataFrame,\n",
    "    all_ids_w_outcome: pd.DataFrame,\n",
    "    flag_col: str = \"patel_flag\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One row per encounter_block with\n",
    "      time_eligibility        - first hour where <flag_col> == 1\n",
    "      time_death              - hours from vent start to death   (NaN if alive)\n",
    "      time_discharge_alive    - hours from vent start to discharge (NaN if died)\n",
    "      t_event                 - min of the three times\n",
    "      outcome                 - 1(eligible)/2(death)/3(discharge)\n",
    "\n",
    "    Assumptions\n",
    "    -----------\n",
    "    • time_from_vent (in hours) is already *after* the 4hour cool-off.  \n",
    "    • all_ids_w_outcome has one row per encounter_block.\n",
    "    \"\"\"\n",
    "\n",
    "    ###################################################################\n",
    "    # 0) Basic column checks\n",
    "    ###################################################################\n",
    "    needed_cols = [\n",
    "        \"encounter_block\",\n",
    "        \"time_from_vent\",          # raw hours since intubation (already cooled‑off)\n",
    "        \"recorded_date\",          \n",
    "        \"recorded_hour\",\n",
    "        flag_col\n",
    "    ]\n",
    "    missing = [c for c in needed_cols if c not in criteria_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"criteria_df is missing columns: {missing}\")\n",
    "\n",
    "    ###################################################################\n",
    "    # 1) FIRST ELIGIBILITY TIME  (earliest hour where flag==1)\n",
    "    ###################################################################\n",
    "    first_elig = (\n",
    "        criteria_df\n",
    "        .loc[criteria_df[flag_col] == 1, [\"encounter_block\", \"time_from_vent\"]]\n",
    "        .groupby(\"encounter_block\", as_index=False)\n",
    "        .min()\n",
    "        .rename(columns={\"time_from_vent\": \"time_eligibility\"})\n",
    "    )\n",
    "\n",
    "    ###################################################################\n",
    "    # 2) BLOCK‑LEVEL death / discharge times\n",
    "    ###################################################################\n",
    "    block_cols = [\n",
    "        \"encounter_block\",\n",
    "        \"block_vent_start_dttm\",\n",
    "        \"final_outcome_dttm\",\n",
    "        \"is_dead\"\n",
    "    ]\n",
    "    block_level = (\n",
    "        all_ids_w_outcome\n",
    "        .loc[all_ids_w_outcome[\"encounter_block\"].isin(criteria_df[\"encounter_block\"]),\n",
    "             block_cols]\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    # convert to datetime once\n",
    "    block_level[\"block_vent_start_dttm\"] = pd.to_datetime(\n",
    "        block_level[\"block_vent_start_dttm\"], errors=\"coerce\"\n",
    "    )\n",
    "    block_level[\"final_outcome_dttm\"] = pd.to_datetime(\n",
    "        block_level[\"final_outcome_dttm\"], errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # hours from vent start to the *final* outcome\n",
    "    hrs_from_vent = (\n",
    "        (block_level[\"final_outcome_dttm\"] - block_level[\"block_vent_start_dttm\"])\n",
    "        .dt.total_seconds() / 3600\n",
    "    )\n",
    "\n",
    "    block_level[\"time_death\"]            = np.where(\n",
    "        block_level[\"is_dead\"] == 1, hrs_from_vent, np.nan\n",
    "    )\n",
    "    block_level[\"time_discharge_alive\"]  = np.where(\n",
    "        block_level[\"is_dead\"] == 0, hrs_from_vent, np.nan\n",
    "    )\n",
    "\n",
    "    ###################################################################\n",
    "    # 3) MERGE and decide which event happened first\n",
    "    ###################################################################\n",
    "    final_df = (\n",
    "        block_level[[\"encounter_block\", \"time_death\", \"time_discharge_alive\"]]\n",
    "        .merge(first_elig, on=\"encounter_block\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # earliest non‑NaN time\n",
    "    final_df[\"t_event\"] = final_df[[\"time_eligibility\",\n",
    "                                    \"time_death\",\n",
    "                                    \"time_discharge_alive\"]].min(axis=1, skipna=True)\n",
    "\n",
    "    # outcome code\n",
    "    def pick_outcome(r):\n",
    "        if np.isfinite(r[\"time_eligibility\"]) and r[\"t_event\"] == r[\"time_eligibility\"]:\n",
    "            return 1\n",
    "        if np.isfinite(r[\"time_death\"])       and r[\"t_event\"] == r[\"time_death\"]:\n",
    "            return 2\n",
    "        return 3   # discharge must be earliest\n",
    "\n",
    "    final_df[\"outcome\"] = final_df.apply(pick_outcome, axis=1)\n",
    "\n",
    "    return final_df[\n",
    "        [\"encounter_block\",\n",
    "         \"time_eligibility\",\n",
    "         \"time_death\",\n",
    "         \"time_discharge_alive\",\n",
    "         \"t_event\",\n",
    "         \"outcome\"]\n",
    "    ].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f530310",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:20.515930Z",
     "iopub.status.busy": "2025-05-05T20:36:20.515810Z",
     "iopub.status.idle": "2025-05-05T20:36:22.480919Z",
     "shell.execute_reply": "2025-05-05T20:36:22.480514Z"
    },
    "papermill": {
     "duration": 1.974011,
     "end_time": "2025-05-05T20:36:22.482034",
     "exception": false,
     "start_time": "2025-05-05T20:36:20.508023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_merged = pd.merge(\n",
    "        final_df,\n",
    "        all_ids_w_outcome[['encounter_block',\n",
    "       'block_vent_start_dttm', 'block_vent_end_dttm',\n",
    "       'block_first_vital_dttm', 'block_last_vital_dttm', 'discharge_dttm',\n",
    "       'discharge_category', 'death_dttm', 'final_outcome_dttm', 'is_dead']],\n",
    "        on=  'encounter_block',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "df_merged_team   = df_merged.copy()\n",
    "df_merged_yellow = df_merged.copy()\n",
    "df_merged_patel  = df_merged.copy()\n",
    "df_merged_green = df_merged.copy()\n",
    "df_merged_green_no_red = df_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8716aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:22.499062Z",
     "iopub.status.busy": "2025-05-05T20:36:22.498925Z",
     "iopub.status.idle": "2025-05-05T20:36:22.811199Z",
     "shell.execute_reply": "2025-05-05T20:36:22.810865Z"
    },
    "papermill": {
     "duration": 0.32192,
     "end_time": "2025-05-05T20:36:22.812260",
     "exception": false,
     "start_time": "2025-05-05T20:36:22.490340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_patel_competing = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_patel,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"patel_flag\"\n",
    ")\n",
    "df_patel_competing.to_parquet(\"../output/intermediate/competing_risk_patel_final.parquet\")\n",
    "\n",
    "df_team_competing = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_team,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"team_flag\"\n",
    ")\n",
    "df_team_competing.to_parquet(\"../output/intermediate/competing_risk_team_final.parquet\")\n",
    "\n",
    "df_yellow_competing = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_yellow,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"any_yellow_or_green_no_red\"\n",
    ")\n",
    "df_yellow_competing.to_parquet(\"../output/intermediate/competing_risk_yellow_final.parquet\")\n",
    "\n",
    "df_green_competing = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_green,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"all_green\"\n",
    ")\n",
    "df_green_competing.to_parquet(\"../output/intermediate/competing_risk_green_final.parquet\")\n",
    "\n",
    "df_green_no_red_competing = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_green_no_red,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"all_green_no_red\"\n",
    ")\n",
    "df_green_no_red_competing.to_parquet(\"../output/intermediate/competing_risk_green_no_red_final.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b135ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## weekday competing risk analysis\n",
    "df_patel_competing_weekday = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_patel,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"patel_flag_weekday\"\n",
    ")\n",
    "df_patel_competing_weekday.to_parquet(\"../output/intermediate/competing_risk_patel_final_weekday.parquet\")\n",
    "\n",
    "df_team_competing_weekday = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_team,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"team_flag_weekday\"\n",
    ")\n",
    "df_team_competing_weekday.to_parquet(\"../output/intermediate/competing_risk_team_final_weekday.parquet\")\n",
    "\n",
    "df_yellow_competing_weekday = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_yellow,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"any_yellow_or_green_no_red_weekday\"\n",
    ")\n",
    "df_yellow_competing_weekday.to_parquet(\"../output/intermediate/competing_risk_yellow_final_weekday.parquet\")\n",
    "\n",
    "df_green_competing_weekday = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_green,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"all_green_weekday\"\n",
    ")\n",
    "df_green_competing_weekday.to_parquet(\"../output/intermediate/competing_risk_green_final_weekday.parquet\")\n",
    "\n",
    "df_green_no_red_competing_weekday = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_green_no_red,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"all_green_no_red_weekday\"\n",
    ")\n",
    "df_green_no_red_competing_weekday.to_parquet(\"../output/intermediate/competing_risk_green_no_red_final_weekday.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c2f3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:22.829640Z",
     "iopub.status.busy": "2025-05-05T20:36:22.829468Z",
     "iopub.status.idle": "2025-05-05T20:36:23.629326Z",
     "shell.execute_reply": "2025-05-05T20:36:23.629053Z"
    },
    "papermill": {
     "duration": 0.809844,
     "end_time": "2025-05-05T20:36:23.630135",
     "exception": false,
     "start_time": "2025-05-05T20:36:22.820291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyse_discharge_without_elig(competing_df, flag_prefix, title):\n",
    "    # ── prepare output directory ──\n",
    "    out_dir = \"../output/final/graphs\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # ── 1a) select blocks that failed entirely ──\n",
    "    no_elig_blocks = competing_df.loc[\n",
    "        competing_df[\"outcome\"] == 3, \"encounter_block\"\n",
    "    ].unique()\n",
    "    df_fail = final_df[final_df[\"encounter_block\"].isin(no_elig_blocks)]\n",
    "\n",
    "    # ── 1b) pick sub-criteria columns ──\n",
    "    crit_cols = [\n",
    "        c for c in df_fail.columns\n",
    "        if c.startswith(flag_prefix) and c.endswith(\"_flag\")\n",
    "           and c not in (f\"{flag_prefix}flag\", f\"{flag_prefix}cardio_flag\", f\"{flag_prefix}resp_flag\")\n",
    "    ]\n",
    "\n",
    "    # ── 1c) for each block, did it ever FAIL? ──\n",
    "    ever_fail = (df_fail[crit_cols] == 0).groupby(df_fail[\"encounter_block\"]).max()\n",
    "\n",
    "    # ── 2) BAR PLOT & CSV ──\n",
    "    freq = ever_fail.mean().sort_values(ascending=True)\n",
    "    # save raw data\n",
    "    freq.to_csv(\n",
    "        os.path.join(out_dir, f\"{title}_eligibility_failures_freq.csv\"),\n",
    "        header=[\"prop_blocks_failed\"]\n",
    "    )\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.barh(freq.index.str.replace(flag_prefix, \"\"), freq.values, color=\"#4c72b0\")\n",
    "    plt.xlabel(\"Proportion of encounter-blocks where criterion ever failed\")\n",
    "    plt.title(f\"{title}: which sub-criteria blocked eligibility?\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, f\"{title}_eligibility_failures.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 3) UPSET PLOT & CSV\n",
    "    # ----------------------------------------------------------------------\n",
    "    upset_data = from_indicators(ever_fail.columns, ever_fail)\n",
    "\n",
    "    # Manually build a DataFrame of each combination + its count\n",
    "    # ------------------------------------------------------------\n",
    "    #  a) pull out the indicator combinations as a DataFrame\n",
    "    combos = pd.DataFrame(list(upset_data.index), columns=upset_data.index.names)\n",
    "\n",
    "    #  b) pull out the counts as a 1D array\n",
    "    if isinstance(upset_data, pd.Series):\n",
    "        counts = upset_data.values\n",
    "    else:\n",
    "        # if it's a DataFrame, assume the last column is the count\n",
    "        counts = upset_data.iloc[:, -1].values\n",
    "\n",
    "    #  c) assemble\n",
    "    upset_df = combos.copy()\n",
    "    upset_df[\"count\"] = counts\n",
    "\n",
    "    # save to CSV\n",
    "    # upset_df.to_csv(\n",
    "    #     os.path.join(out_dir, f\"{title}_eligibility_failures_upset_data.csv\"),\n",
    "    #     index=False\n",
    "    # )\n",
    "\n",
    "    # now plot\n",
    "    UpSet(\n",
    "        upset_data,\n",
    "        show_counts=True,\n",
    "        sort_by=\"cardinality\",\n",
    "        intersection_plot_elements=15,\n",
    "        element_size=None\n",
    "    ).plot()\n",
    "    plt.suptitle(f\"{title}: top combinations of failed sub-criteria\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(out_dir, f\"{title}_eligibility_failures_upset.png\"),\n",
    "        dpi=300\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    return freq.to_frame(\"prop_blocks_failed\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2.  Run for each criterion\n",
    "# ---------------------------------------------------------------------------\n",
    "team_summary   = analyse_discharge_without_elig(df_team_competing,\n",
    "                                               flag_prefix=\"team_\",\n",
    "                                               title=\"TEAM\")\n",
    "\n",
    "patel_summary  = analyse_discharge_without_elig(df_patel_competing,\n",
    "                                               flag_prefix=\"patel_\",\n",
    "                                               title=\"Patel\")\n",
    "\n",
    "yellow_summary = analyse_discharge_without_elig(df_yellow_competing,\n",
    "                                               flag_prefix=\"yellow_\",\n",
    "                                               title=\"Yellow\")\n",
    "\n",
    "green_summary = analyse_discharge_without_elig(df_green_competing,\n",
    "                                               flag_prefix=\"green_\",\n",
    "                                               title=\"Green\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.  Example: compare the three summaries side‑by‑side\n",
    "# ---------------------------------------------------------------------------\n",
    "compare = (team_summary.rename(columns={\"prop_blocks_failed\": \"TEAM\"})\n",
    "           .join(patel_summary.rename(columns={\"prop_blocks_failed\": \"Patel\"}), how=\"outer\")\n",
    "           .join(yellow_summary.rename(columns={\"prop_blocks_failed\": \"Yellow\"}), how=\"outer\")\n",
    "           .join(green_summary.rename(columns={\"prop_blocks_failed\": \"Green\"}), how=\"outer\")\n",
    "           .fillna(0)\n",
    "           .sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65244691",
   "metadata": {},
   "source": [
    "### Discharge alive without eligibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d7932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:23.650225Z",
     "iopub.status.busy": "2025-05-05T20:36:23.650045Z",
     "iopub.status.idle": "2025-05-05T20:36:23.663568Z",
     "shell.execute_reply": "2025-05-05T20:36:23.663256Z"
    },
    "papermill": {
     "duration": 0.024539,
     "end_time": "2025-05-05T20:36:23.664408",
     "exception": false,
     "start_time": "2025-05-05T20:36:23.639869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Assess encounters discharged without ever becoming eligible\")\n",
    "reqd_team_fields = ['encounter_block', 'recorded_date'\t,'recorded_hour',\n",
    "                    'time_from_vent','hourly_trach','paralytics_flag',\n",
    "                    'lactate', 'max_heart_rate', 'ne_calc_max','ne_calc_last','last_ne_dose_last_6_hours', 'min_fio2_set', 'max_peep_set', \n",
    "                    'max_resp_rate_obs', \"team_pulse_flag\", \"team_lactate_flag\", \"team_ne_flag\",\n",
    "                    \"team_fio2_flag\",  \"team_peep_flag\",    \"team_resp_rate_flag\", \"team_flag\"]\n",
    "\n",
    "\n",
    "no_elig_blocks_team = df_team_competing.loc[df_team_competing[\"outcome\"] == 3,\n",
    "                                      \"encounter_block\"].unique()\n",
    "df_fail_team = final_df[final_df[\"encounter_block\"].isin(no_elig_blocks_team)]\n",
    "df_fail_team_filtered = df_fail_team[reqd_team_fields].copy()\n",
    "df_fail_team_filtered = df_fail_team_filtered.merge(all_ids_w_outcome[['discharge_category', 'encounter_block']], on='encounter_block', how='inner')\n",
    "\n",
    "failure_discharge_cats_team = df_fail_team_filtered.groupby('discharge_category')['encounter_block'].nunique().sort_values(ascending=False)\n",
    "#write to csv\n",
    "failure_discharge_cats_team.to_csv('../output/final/failure_discharge_cats_team.csv')\n",
    "failure_discharge_cats_team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709404c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:23.684491Z",
     "iopub.status.busy": "2025-05-05T20:36:23.684290Z",
     "iopub.status.idle": "2025-05-05T20:36:23.696408Z",
     "shell.execute_reply": "2025-05-05T20:36:23.696145Z"
    },
    "papermill": {
     "duration": 0.023156,
     "end_time": "2025-05-05T20:36:23.697174",
     "exception": false,
     "start_time": "2025-05-05T20:36:23.674018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reqd_patel_fields = ['encounter_block','recorded_date'\t,'recorded_hour',\n",
    "                    'time_from_vent', 'hourly_trach','paralytics_flag','min_spo2',\n",
    "                    'min_map', 'max_map','max_sbp', 'min_sbp','avg_map',    \n",
    "                   'min_heart_rate','max_heart_rate', 'min_respiratory_rate','min_spo2', \n",
    "                    'max_respiratory_rate','patel_map_flag','patel_sbp_flag','patel_pulse_flag', \n",
    "                    'patel_resp_rate_flag' , 'patel_spo2_flag', 'patel_resp_flag', 'patel_cardio_flag', 'patel_flag' ]\n",
    "no_elig_blocks_patel = df_patel_competing.loc[df_patel_competing[\"outcome\"] == 3,\n",
    "                                      \"encounter_block\"].unique()\n",
    "df_fail_patel = final_df[final_df[\"encounter_block\"].isin(no_elig_blocks_patel)]\n",
    "df_fail_patel_filtered = df_fail_patel[reqd_patel_fields].copy()\n",
    "df_fail_patel_filtered = df_fail_patel_filtered.merge(all_ids_w_outcome[['discharge_category', 'encounter_block']], \n",
    "                                                    on='encounter_block', how='inner')\n",
    "failure_discharge_cats_patel = df_fail_patel_filtered.groupby('discharge_category')['encounter_block'].nunique().sort_values(ascending=False)\n",
    "failure_discharge_cats_patel.to_csv('../output/final/failure_discharge_cats_patel.csv')\n",
    "failure_discharge_cats_patel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ad404",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:23.717169Z",
     "iopub.status.busy": "2025-05-05T20:36:23.717045Z",
     "iopub.status.idle": "2025-05-05T20:36:23.731308Z",
     "shell.execute_reply": "2025-05-05T20:36:23.731051Z"
    },
    "papermill": {
     "duration": 0.025189,
     "end_time": "2025-05-05T20:36:23.732081",
     "exception": false,
     "start_time": "2025-05-05T20:36:23.706892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reqd_yellow_fields =[\n",
    "    'encounter_block','recorded_date'\t,'recorded_hour',\n",
    "    'time_from_vent', 'hourly_trach','paralytics_flag',\n",
    "    # Clinical Measurements\n",
    "    'min_spo2', 'min_map', 'max_map', 'ne_calc_max', 'max_sbp', 'avg_map',\n",
    "    'max_heart_rate', 'min_heart_rate', 'min_fio2_set',\n",
    "    'max_resp_rate_obs', 'min_peep_set', 'lactate',\n",
    "    \n",
    "    # Administrative/Timing\n",
    "    'hourly_trach', 'paralytics_flag', 'recorded_hour',\n",
    "    'time_from_vent_adjusted', 'red_meds_flag',\n",
    "    # Red Flags\n",
    "    'red_resp_spo2_flag', 'red_map_flag', 'red_high_support_flag',\n",
    "    'red_hypertensive_flag', 'red_pulse_high_flag', 'red_pulse_low_flag',\n",
    "    \n",
    "    # Yellow Flags\n",
    "    'yellow_resp_spo2_flag', 'yellow_fio2_flag', 'yellow_resp_rate_flag',\n",
    "    'yellow_peep_flag', 'yellow_map_flag', 'yellow_pulse_flag',\n",
    "    'yellow_lactate_flag',\n",
    "    \n",
    "    # Green Flags\n",
    "    'green_resp_spo2_flag', 'green_resp_rate_flag', 'green_fio2_flag',\n",
    "    'green_peep_flag', 'green_map_flag', 'green_pulse_flag',\n",
    "    'green_lactate_flag', 'green_hr_flag',\n",
    "    \n",
    "    # Composite Flags\n",
    "    'any_red', 'any_yellow', 'any_green', 'all_green',\n",
    "    'all_green_no_red', 'all_green_no_red_yellow',\n",
    "    'all_yellow_no_red_green', 'any_yellow_no_red_green',\n",
    "    'any_yellow_or_green_no_red', 'yellow_resp_flag',\n",
    "    'yellow_cardio_flag', 'yellow_all_green', 'yellow_not_all_green'\n",
    "]\n",
    "no_elig_blocks_yellow = df_yellow_competing.loc[df_yellow_competing[\"outcome\"] == 3,\n",
    "                                      \"encounter_block\"].unique()\n",
    "df_fail_yellow = final_df[final_df[\"encounter_block\"].isin(no_elig_blocks_yellow)]\n",
    "df_fail_yellow_filtered = df_fail_yellow[reqd_yellow_fields].copy()\n",
    "df_fail_yellow_filtered = df_fail_yellow_filtered.merge(all_ids_w_outcome[['discharge_category', 'encounter_block']], \n",
    "                                                    on='encounter_block', how='inner')\n",
    "failure_discharge_cats_yellow = df_fail_yellow_filtered.groupby('discharge_category')['encounter_block'].nunique().sort_values(ascending=False)\n",
    "failure_discharge_cats_yellow.to_csv('../output/final/failure_discharge_cats_yellow.csv')\n",
    "failure_discharge_cats_yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce594997",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:23.771987Z",
     "iopub.status.busy": "2025-05-05T20:36:23.771792Z",
     "iopub.status.idle": "2025-05-05T20:36:23.801906Z",
     "shell.execute_reply": "2025-05-05T20:36:23.801551Z"
    },
    "papermill": {
     "duration": 0.041979,
     "end_time": "2025-05-05T20:36:23.802779",
     "exception": false,
     "start_time": "2025-05-05T20:36:23.760800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reqd_green_fields =[\n",
    "    'encounter_block','recorded_date'\t,'recorded_hour',\n",
    "    'time_from_vent', 'hourly_trach','paralytics_flag',\n",
    "    # Clinical Measurements\n",
    "    'min_spo2', 'min_map', 'max_map', 'ne_calc_max', 'max_sbp',\n",
    "    'max_heart_rate', 'min_heart_rate', 'min_fio2_set','avg_map',\n",
    "    'max_resp_rate_obs', 'min_peep_set', 'lactate',\n",
    "    \n",
    "    # Administrative/Timing\n",
    "    'hourly_trach', 'paralytics_flag', 'recorded_hour',\n",
    "    'time_from_vent_adjusted', 'red_meds_flag',\n",
    "    # Red Flags\n",
    "    'red_resp_spo2_flag', 'red_map_flag', 'red_high_support_flag',\n",
    "    'red_hypertensive_flag', 'red_pulse_high_flag', 'red_pulse_low_flag',\n",
    "    \n",
    "    # Green Flags\n",
    "    'green_resp_spo2_flag', 'green_resp_rate_flag', 'green_fio2_flag',\n",
    "    'green_peep_flag', 'green_map_flag', 'green_pulse_flag',\n",
    "    'green_lactate_flag', 'green_hr_flag',\n",
    "    'all_green',\n",
    "    'all_green_no_red', \n",
    "]\n",
    "no_elig_blocks_green = df_green_no_red_competing.loc[df_green_no_red_competing[\"outcome\"] == 3,\n",
    "                                      \"encounter_block\"].unique()\n",
    "df_fail_green = final_df[final_df[\"encounter_block\"].isin(no_elig_blocks_green)]\n",
    "df_fail_green_filtered = df_fail_green[reqd_green_fields].copy()\n",
    "df_fail_green_filtered = df_fail_green_filtered.merge(all_ids_w_outcome[['discharge_category', 'encounter_block']], \n",
    "                                                    on='encounter_block', how='inner')\n",
    "failure_discharge_cats_green = df_fail_green_filtered.groupby('discharge_category')['encounter_block'].nunique().sort_values(ascending=False)\n",
    "failure_discharge_cats_green.to_csv('../output/final/failure_discharge_cats_green.csv')\n",
    "failure_discharge_cats_green"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4c46e5",
   "metadata": {},
   "source": [
    "### Discharged alive without eligibility- % Encounters on Trach and other components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10ece89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encounters discharged alive without becoming eligible- Proportion of Encounters on Trach and other components\")\n",
    "def analyze_trach_blocks(df, criteria_name):\n",
    "    print(\"\\nAnalyze Trach blocks for df:\", criteria_name)\n",
    "    total_blocks = df['encounter_block'].nunique()\n",
    "    trach_blocks = df[df['hourly_trach']==1]['encounter_block'].nunique()\n",
    "\n",
    "    print(f\"Total encounter blocks: {total_blocks}\")\n",
    "    print(f\"Blocks with trach: {trach_blocks}\")\n",
    "    print(f\"Percentage with trach: {(trach_blocks/total_blocks)*100:.1f}%\\n\")\n",
    "\n",
    "# Analyze trach blocks for green criteria\n",
    "analyze_trach_blocks(df_fail_patel, \"Patel\")\n",
    "analyze_trach_blocks(df_fail_team, \"TEAM\")\n",
    "analyze_trach_blocks(df_fail_yellow, \"Yellow\")\n",
    "analyze_trach_blocks(df_fail_green, \"Green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b359e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_trach_blocks(df, criteria_name):\n",
    "    total_blocks = df['encounter_block'].nunique()\n",
    "    trach_blocks = df[df['hourly_trach']==1]['encounter_block'].nunique()\n",
    "    trach_percentage = (trach_blocks/total_blocks)*100\n",
    "    \n",
    "    return {\n",
    "        'criteria': criteria_name,\n",
    "        'total_blocks': total_blocks,\n",
    "        'trach_blocks': trach_blocks,\n",
    "        'trach_percentage': trach_percentage\n",
    "    }\n",
    "\n",
    "# Analyze trach blocks for each criteria\n",
    "results = [\n",
    "    analyze_trach_blocks(df_fail_patel, \"Patel\"),\n",
    "    analyze_trach_blocks(df_fail_team, \"TEAM\"), \n",
    "    analyze_trach_blocks(df_fail_yellow, \"Yellow\"),\n",
    "    analyze_trach_blocks(df_fail_green, \"Green\")\n",
    "]\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('../output/final/never_eligible_trach_analysis.csv', index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_failure_analysis(df_fail, criteria_name, flag_fields, all_ids_w_outcome=None, save_outputs=True):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of why encounters failed to achieve eligibility.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_fail : pd.DataFrame\n",
    "        Failure dataframe containing all hourly records for non-eligible encounters\n",
    "    criteria_name : str\n",
    "        Name of the criteria (e.g., 'Patel', 'TEAM', 'Green', 'Yellow')\n",
    "    flag_fields : list\n",
    "        List of component flag column names to analyze\n",
    "    all_ids_w_outcome : pd.DataFrame, optional\n",
    "        Dataframe with outcome information for additional context\n",
    "    save_outputs : bool\n",
    "        Whether to save CSV outputs and plots\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Comprehensive results dictionary\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPREHENSIVE FAILURE ANALYSIS: {criteria_name.upper()} CRITERIA\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. OVERALL FAILURE COUNTS\n",
    "    print(f\"\\n1. OVERALL FAILURE SUMMARY\")\n",
    "    print(f\"-\" * 30)\n",
    "    \n",
    "    total_encounters = df_fail['encounter_block'].nunique()\n",
    "    total_hours = len(df_fail)\n",
    "    \n",
    "    print(f\"Total encounters that never became eligible: {total_encounters:,}\")\n",
    "    print(f\"Total hours of observation: {total_hours:,}\")\n",
    "    print(f\"Average hours per encounter: {total_hours/total_encounters:.1f}\")\n",
    "    \n",
    "    results['overall'] = {\n",
    "        'total_encounters': total_encounters,\n",
    "        'total_hours': total_hours,\n",
    "        'avg_hours_per_encounter': total_hours/total_encounters\n",
    "    }\n",
    "    \n",
    "    # 2. TRACHEOSTOMY ANALYSIS\n",
    "    print(f\"\\n2. TRACHEOSTOMY ANALYSIS\")\n",
    "    print(f\"-\" * 30)\n",
    "    \n",
    "    # Get trach status per encounter (any trach during stay)\n",
    "    trach_by_encounter = df_fail.groupby('encounter_block')['hourly_trach'].max().reset_index()\n",
    "    trach_encounters = trach_by_encounter[trach_by_encounter['hourly_trach'] == 1]['encounter_block'].nunique()\n",
    "    non_trach_encounters = trach_by_encounter[trach_by_encounter['hourly_trach'] == 0]['encounter_block'].nunique()\n",
    "    \n",
    "    trach_pct = (trach_encounters / total_encounters) * 100\n",
    "    non_trach_pct = (non_trach_encounters / total_encounters) * 100\n",
    "    \n",
    "    print(f\"Encounters with tracheostomy: {trach_encounters:,} ({trach_pct:.1f}%)\")\n",
    "    print(f\"Encounters without tracheostomy: {non_trach_encounters:,} ({non_trach_pct:.1f}%)\")\n",
    "    \n",
    "    results['tracheostomy'] = {\n",
    "        'with_trach': trach_encounters,\n",
    "        'without_trach': non_trach_encounters,\n",
    "        'with_trach_pct': trach_pct,\n",
    "        'without_trach_pct': non_trach_pct\n",
    "    }\n",
    "    \n",
    "    # 3. BUSINESS HOURS ANALYSIS  \n",
    "    print(f\"\\n3. BUSINESS HOURS ANALYSIS\")\n",
    "    print(f\"-\" * 30)\n",
    "    \n",
    "    # Define business hours (8 AM to 5 PM = hours 8-16)\n",
    "    business_hours = list(range(8, 17))\n",
    "    df_fail['is_business_hours'] = df_fail['recorded_hour'].isin(business_hours)\n",
    "    \n",
    "    # Analyze by encounter\n",
    "    bh_by_encounter = df_fail.groupby('encounter_block').agg({\n",
    "        'is_business_hours': ['sum', 'count']\n",
    "    }).round(2)\n",
    "    bh_by_encounter.columns = ['business_hours_count', 'total_hours']\n",
    "    bh_by_encounter['business_hours_pct'] = (bh_by_encounter['business_hours_count'] / \n",
    "                                           bh_by_encounter['total_hours'] * 100).round(1)\n",
    "    \n",
    "    # Encounters with different business hours exposure\n",
    "    never_bh = (bh_by_encounter['business_hours_count'] == 0).sum()\n",
    "    some_bh = ((bh_by_encounter['business_hours_count'] > 0) & \n",
    "               (bh_by_encounter['business_hours_count'] < bh_by_encounter['total_hours'])).sum()\n",
    "    only_bh = (bh_by_encounter['business_hours_count'] == bh_by_encounter['total_hours']).sum()\n",
    "    \n",
    "    print(f\"Encounters NEVER assessed during business hours: {never_bh:,} ({never_bh/total_encounters*100:.1f}%)\")\n",
    "    print(f\"Encounters with MIXED business/non-business hours: {some_bh:,} ({some_bh/total_encounters*100:.1f}%)\")\n",
    "    print(f\"Encounters ONLY assessed during business hours: {only_bh:,} ({only_bh/total_encounters*100:.1f}%)\")\n",
    "    print(f\"Median business hours exposure: {bh_by_encounter['business_hours_pct'].median():.1f}%\")\n",
    "    \n",
    "    results['business_hours'] = {\n",
    "        'never_business_hours': never_bh,\n",
    "        'mixed_hours': some_bh,\n",
    "        'only_business_hours': only_bh,\n",
    "        'median_bh_exposure_pct': bh_by_encounter['business_hours_pct'].median()\n",
    "    }\n",
    "    \n",
    "    # 4. COMPONENT-LEVEL FAILURE ANALYSIS (Non-trach encounters)\n",
    "    print(f\"\\n4. COMPONENT FAILURE ANALYSIS (Non-trach encounters only)\")\n",
    "    print(f\"-\" * 50)\n",
    "    \n",
    "    # Focus on non-trach encounters for component analysis\n",
    "    df_non_trach = df_fail[df_fail.groupby('encounter_block')['hourly_trach'].transform('max') == 0]\n",
    "    non_trach_encounters_actual = df_non_trach['encounter_block'].nunique()\n",
    "    \n",
    "    if non_trach_encounters_actual > 0:\n",
    "        print(f\"Analyzing {non_trach_encounters_actual:,} non-trach encounters...\")\n",
    "        \n",
    "        # Component failure rates\n",
    "        component_failures = {}\n",
    "        \n",
    "        for flag in flag_fields:\n",
    "            if flag in df_non_trach.columns:\n",
    "                # Calculate failure rate: proportion of hours where flag = 0\n",
    "                total_obs = len(df_non_trach[flag].dropna())\n",
    "                failed_obs = (df_non_trach[flag] == 0).sum()\n",
    "                failure_rate = (failed_obs / total_obs * 100) if total_obs > 0 else 0\n",
    "                \n",
    "                # Calculate encounter-level failure rate\n",
    "                encounter_failures = (df_non_trach.groupby('encounter_block')[flag].max() == 0).sum()\n",
    "                encounter_failure_rate = (encounter_failures / non_trach_encounters_actual * 100)\n",
    "                \n",
    "                component_failures[flag] = {\n",
    "                    'hourly_failure_rate': failure_rate,\n",
    "                    'encounter_failure_rate': encounter_failure_rate,\n",
    "                    'encounters_affected': encounter_failures\n",
    "                }\n",
    "                \n",
    "                print(f\"{flag:30s}: {failure_rate:5.1f}% of hours failed | \"\n",
    "                      f\"{encounter_failure_rate:5.1f}% of encounters never met\")\n",
    "        \n",
    "        results['component_failures'] = component_failures\n",
    "        \n",
    "        # 5. MOST PROBLEMATIC COMPONENTS\n",
    "        print(f\"\\n5. MOST PROBLEMATIC COMPONENTS\")\n",
    "        print(f\"-\" * 35)\n",
    "        \n",
    "        # Sort by encounter failure rate\n",
    "        sorted_components = sorted(component_failures.items(), \n",
    "                                 key=lambda x: x[1]['encounter_failure_rate'], \n",
    "                                 reverse=True)\n",
    "        \n",
    "        print(\"Top barriers to eligibility (by % encounters never meeting criteria):\")\n",
    "        for i, (component, stats) in enumerate(sorted_components[:5], 1):\n",
    "            print(f\"{i}. {component}: {stats['encounter_failure_rate']:.1f}% of encounters\")\n",
    "    \n",
    "    else:\n",
    "        print(\"No non-trach encounters found for component analysis.\")\n",
    "        component_failures = {}\n",
    "    \n",
    "    # 6. DISCHARGE CATEGORY ANALYSIS (if available)\n",
    "    if all_ids_w_outcome is not None:\n",
    "        print(f\"\\n6. DISCHARGE CATEGORY ANALYSIS\")\n",
    "        print(f\"-\" * 35)\n",
    "        \n",
    "        # Merge with discharge categories\n",
    "        df_with_discharge = df_fail.merge(\n",
    "            all_ids_w_outcome[['encounter_block', 'discharge_category']], \n",
    "            on='encounter_block', how='left'\n",
    "        )\n",
    "        \n",
    "        discharge_counts = (df_with_discharge.groupby('discharge_category')['encounter_block']\n",
    "                          .nunique().sort_values(ascending=False))\n",
    "        \n",
    "        print(\"Discharge categories for non-eligible encounters:\")\n",
    "        for category, count in discharge_counts.items():\n",
    "            pct = count / total_encounters * 100\n",
    "            print(f\"{category:30s}: {count:4d} ({pct:5.1f}%)\")\n",
    "        \n",
    "        results['discharge_categories'] = discharge_counts.to_dict()\n",
    "    \n",
    "    # 7. SAVE OUTPUTS\n",
    "    if save_outputs:\n",
    "        print(f\"\\n7. SAVING OUTPUTS\")\n",
    "        print(f\"-\" * 20)\n",
    "        \n",
    "        # Create output directory\n",
    "        output_dir = f\"../output/final/{criteria_name.lower()}_failure_analysis\"\n",
    "        import os\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save component failure summary\n",
    "        if component_failures:\n",
    "            comp_df = pd.DataFrame(component_failures).T\n",
    "            comp_df.to_csv(f\"{output_dir}/component_failure_rates.csv\")\n",
    "            print(f\"✓ Component failure rates saved\")\n",
    "        \n",
    "        # Save business hours analysis\n",
    "        bh_by_encounter.to_csv(f\"{output_dir}/business_hours_by_encounter.csv\")\n",
    "        print(f\"✓ Business hours analysis saved\")\n",
    "        \n",
    "        # Save trach analysis\n",
    "        trach_summary = pd.DataFrame([results['tracheostomy']])\n",
    "        trach_summary.to_csv(f\"{output_dir}/tracheostomy_summary.csv\", index=False)\n",
    "        print(f\"✓ Tracheostomy summary saved\")\n",
    "        \n",
    "        # Save overall summary\n",
    "        summary_df = pd.DataFrame([{\n",
    "            'criteria': criteria_name,\n",
    "            'total_encounters': total_encounters,\n",
    "            'with_trach': trach_encounters,\n",
    "            'without_trach': non_trach_encounters,\n",
    "            'never_business_hours': never_bh,\n",
    "            'mixed_hours': some_bh,\n",
    "            'only_business_hours': only_bh\n",
    "        }])\n",
    "        summary_df.to_csv(f\"{output_dir}/overall_summary.csv\", index=False)\n",
    "        print(f\"✓ Overall summary saved\")\n",
    "        \n",
    "        print(f\"All outputs saved to: {output_dir}/\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define component fields for each criteria\n",
    "reqd_patel_fields = ['patel_map_flag','patel_sbp_flag','patel_pulse_flag', \n",
    "                    'patel_resp_rate_flag' , 'patel_spo2_flag', 'patel_resp_flag', 'patel_cardio_flag', 'patel_flag',\n",
    "                     'hourly_trach', 'paralytics_flag' ]\n",
    "\n",
    "reqd_green_fields =[    \n",
    "    # Green Flags\n",
    "    'green_resp_spo2_flag', 'green_resp_rate_flag', 'green_fio2_flag',\n",
    "    'green_peep_flag', 'green_map_flag', 'green_pulse_flag',\n",
    "    'green_lactate_flag', 'green_hr_flag',\n",
    "    \n",
    "    # Composite Flags\n",
    "    'all_green_no_red', \n",
    "    'hourly_trach', 'paralytics_flag'\n",
    "]\n",
    "\n",
    "reqd_yellow_fields =[\n",
    "    'hourly_trach', 'paralytics_flag'\n",
    "    # Red Flags\n",
    "    'red_resp_spo2_flag', 'red_map_flag', 'red_high_support_flag',\n",
    "    'red_hypertensive_flag', 'red_pulse_high_flag', 'red_pulse_low_flag',\n",
    "    \n",
    "    # Yellow Flags\n",
    "    'yellow_resp_spo2_flag', 'yellow_fio2_flag', 'yellow_resp_rate_flag',\n",
    "    'yellow_peep_flag', 'yellow_map_flag', 'yellow_pulse_flag',\n",
    "    'yellow_lactate_flag',\n",
    "    \n",
    "    # Green Flags\n",
    "    'green_resp_spo2_flag', 'green_resp_rate_flag', 'green_fio2_flag',\n",
    "    'green_peep_flag', 'green_map_flag', 'green_pulse_flag',\n",
    "    'green_lactate_flag', 'green_hr_flag',\n",
    "    \n",
    "    # Composite Flags\n",
    "    'any_red', 'any_yellow', 'any_green', 'all_green',\n",
    "    'all_green_no_red', 'all_green_no_red_yellow',\n",
    "    'all_yellow_no_red_green', 'any_yellow_no_red_green',\n",
    "    'any_yellow_or_green_no_red', 'yellow_resp_flag',\n",
    "    'yellow_cardio_flag', 'yellow_all_green', 'yellow_not_all_green'\n",
    "]\n",
    "\n",
    "team_flags = [\"team_pulse_flag\", \"team_lactate_flag\", \"team_ne_flag\",\n",
    "              \"team_fio2_flag\",  \"team_peep_flag\",    \"team_resp_rate_flag\", 'hourly_trach', \n",
    "              'paralytics_flag']\n",
    "\n",
    "# Run the analysis for each criteria\n",
    "print(\"🔍 COMPREHENSIVE FAILURE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze Patel criteria failures\n",
    "patel_results = comprehensive_failure_analysis(\n",
    "    df_fail_patel, \n",
    "    'Patel', \n",
    "    reqd_patel_fields,\n",
    "    all_ids_w_outcome\n",
    ")\n",
    "\n",
    "# Analyze TEAM criteria failures  \n",
    "team_results = comprehensive_failure_analysis(\n",
    "    df_fail_team,\n",
    "    'TEAM', \n",
    "    team_flags,\n",
    "    all_ids_w_outcome\n",
    ")\n",
    "\n",
    "# Analyze Yellow criteria failures\n",
    "yellow_results = comprehensive_failure_analysis(\n",
    "    df_fail_yellow,\n",
    "    'Yellow',\n",
    "    reqd_yellow_fields, \n",
    "    all_ids_w_outcome\n",
    ")\n",
    "\n",
    "# Analyze Green criteria failures\n",
    "green_results = comprehensive_failure_analysis(\n",
    "    df_fail_green,\n",
    "    'Green',\n",
    "    reqd_green_fields,\n",
    "    all_ids_w_outcome\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✅ COMPREHENSIVE FAILURE ANALYSIS COMPLETE\")\n",
    "print(\"📁 Check ../output/final/*_failure_analysis/ for detailed outputs\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4843668d",
   "metadata": {
    "papermill": {
     "duration": 0.010201,
     "end_time": "2025-05-05T20:36:56.517925",
     "exception": false,
     "start_time": "2025-05-05T20:36:56.507724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb7142",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:56.538761Z",
     "iopub.status.busy": "2025-05-05T20:36:56.538596Z",
     "iopub.status.idle": "2025-05-05T20:36:56.600120Z",
     "shell.execute_reply": "2025-05-05T20:36:56.599793Z"
    },
    "papermill": {
     "duration": 0.073623,
     "end_time": "2025-05-05T20:36:56.601065",
     "exception": false,
     "start_time": "2025-05-05T20:36:56.527442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Percentage of encounters that died without becoming eligible\")\n",
    "def analyze_death_without_eligibility(df, criteria_name):\n",
    "    # Total number of blocks\n",
    "    total_blocks = len(df)\n",
    "    \n",
    "    # Blocks that died without eligibility (outcome=2)\n",
    "    died_without_elig = df[df['outcome'] == 2].shape[0]\n",
    "    \n",
    "    # Calculate percentage\n",
    "    percent = (died_without_elig / total_blocks) * 100\n",
    "    \n",
    "    return {\n",
    "        'criteria': criteria_name,\n",
    "        'total_blocks': total_blocks,\n",
    "        'died_without_eligibility': died_without_elig,\n",
    "        'percentage': percent\n",
    "    }\n",
    "\n",
    "# Analyze each dataset\n",
    "results = [\n",
    "    analyze_death_without_eligibility(df_team_competing, 'TEAM'),\n",
    "    analyze_death_without_eligibility(df_yellow_competing, 'Yellow'),\n",
    "    analyze_death_without_eligibility(df_patel_competing, 'Patel'),\n",
    "    analyze_death_without_eligibility(df_green_competing, 'Green'),\n",
    "    analyze_death_without_eligibility(df_green_no_red_competing, 'Green No Red')\n",
    "]\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create the visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create grouped bar plot\n",
    "x = range(len(results_df['criteria']))\n",
    "width = 0.35\n",
    "\n",
    "# Plot bars\n",
    "bars = plt.bar(x, results_df['percentage'], width, label='Percentage')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Percentage of Blocks that Died Without Becoming Eligible by Criteria', pad=20)\n",
    "plt.xlabel('Criteria')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.xticks(x, results_df['criteria'])\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}%\\n({results_df[\"died_without_eligibility\"][i]:,}/{results_df[\"total_blocks\"][i]:,})',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Print detailed summary\n",
    "print(\"\\nDetailed Summary:\")\n",
    "print(\"=\" * 80)\n",
    "for result in results:\n",
    "    print(f\"\\n{result['criteria']} Criteria:\")\n",
    "    print(f\"Total blocks: {result['total_blocks']:,}\")\n",
    "    print(f\"Died without eligibility: {result['died_without_eligibility']:,}\")\n",
    "    print(f\"Percentage: {result['percentage']:.1f}%\")\n",
    "\n",
    "pd.DataFrame(results).to_csv('../output/final/death_without_eligibility_summary.csv', index=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beaef25",
   "metadata": {
    "papermill": {
     "duration": 0.010272,
     "end_time": "2025-05-05T20:36:56.647037",
     "exception": false,
     "start_time": "2025-05-05T20:36:56.636765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Final figures and tables\n",
    "\n",
    "1. Figure 1: Percentage of encounter satisfying Patel, TEAM, and any yellow or GREEN criteria\n",
    "2. Figure 2: Percentage of business hours each encounter was eligible for different criteria\n",
    "3. Figure 3: Percentage of business hours not eligible for each criteria broken down by subcomponent failure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2179f22",
   "metadata": {},
   "source": [
    "### Aggregates - 72 hours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating final figures\")\n",
    "\n",
    "# ── flag dictionaries ------------------------------------------------\n",
    "CRITS_ALL = {\n",
    "    'Patel' : 'patel_flag_all_hours',\n",
    "    'TEAM'  : 'team_flag_all_hours',\n",
    "    'Yellow': 'any_yellow_or_green_no_red_all_hours',\n",
    "    'Green' : 'all_green_no_red_all_hours',\n",
    "}\n",
    "BUSINESS_FLAGS = {\n",
    "    'Patel' : 'patel_flag',\n",
    "    'TEAM'  : 'team_flag',\n",
    "    'Yellow': 'any_yellow_or_green_no_red',\n",
    "    'Green' : 'all_green_no_red'\n",
    "}\n",
    "\n",
    "BUS_HRS = range(8, 17)  # 08:00–16:59 inclusive\n",
    "\n",
    "# ── restrict to first 72 hours only ----------------------------------\n",
    "df_72h = final_df[final_df['time_from_vent'] <= 72].copy()\n",
    "\n",
    "# ── denominators -----------------------------------------------------\n",
    "total_patients        = df_72h['encounter_block'].nunique()\n",
    "total_observed_hours  = len(df_72h)\n",
    "total_business_hours  = len(df_72h[df_72h['recorded_hour'].isin(BUS_HRS)])\n",
    "\n",
    "# ── build aggregate rows --------------------------------------------\n",
    "rows = []\n",
    "for crit in CRITS_ALL:\n",
    "    f_all = CRITS_ALL[crit]\n",
    "    f_bus = BUSINESS_FLAGS[crit]\n",
    "\n",
    "    elig_all_df = df_72h[df_72h[f_all] == 1]\n",
    "    eligible_hours_all = len(elig_all_df)\n",
    "    eligible_patients = elig_all_df['encounter_block'].nunique()\n",
    "\n",
    "    elig_bus_df = df_72h[\n",
    "        (df_72h[f_bus] == 1) & (df_72h['recorded_hour'].isin(BUS_HRS))\n",
    "    ]\n",
    "    eligible_business_hours = len(elig_bus_df)\n",
    "\n",
    "    rows.append({\n",
    "        'Criteria'                      : crit,\n",
    "        'Total Patients'                : total_patients,\n",
    "        'Eligible Patients'             : eligible_patients,\n",
    "        'Total Observed Hours'          : total_observed_hours,\n",
    "        'Eligible Hours (all hrs)'      : eligible_hours_all,\n",
    "        'Total Business Hours'          : total_business_hours,\n",
    "        'Eligible Business Hours'       : eligible_business_hours,\n",
    "        'Proportion Eligible Hours %'   : 100 * eligible_hours_all / total_observed_hours,\n",
    "        'Proportion Eligible BusHrs %'  : 100 * eligible_business_hours / total_business_hours,\n",
    "        'Proportion Eligible Patients %': 100 * eligible_patients / total_patients\n",
    "    })\n",
    "\n",
    "aggregate_df = pd.DataFrame(rows)\n",
    "\n",
    "# Save the aggregate data to CSV\n",
    "timestamp = datetime.now().date()\n",
    "aggregate_df.to_csv(f'../output/final/aggregates_72hrs_{pyCLIF.helper[\"site_name\"]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d97c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colors\n",
    "custom_colors = ['#983232', '#003f5c', '#fdfd96', '#98FB98']  # Maroon, Dark Blue, Pastel Yellow, Pastel Green\n",
    "# Plot proportion of eligible encounters\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(\n",
    "    x='Criteria', \n",
    "    y='Proportion Eligible Patients %', \n",
    "    data=aggregate_df, \n",
    "    palette=custom_colors\n",
    ")\n",
    "\n",
    "# Add percentages on top of the bars\n",
    "for index, row in aggregate_df.iterrows():\n",
    "    barplot.text(index, row['Proportion Eligible Patients %'], f\"{row['Proportion Eligible Patients %']:.1f}%\", \n",
    "                 color='black', ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Eligibility by Encounter (During first 72 hours)')\n",
    "plt.xlabel('Criteria')\n",
    "plt.ylabel('Percentage of Encounters Eligible')\n",
    "\n",
    "# Save the graph\n",
    "plt.savefig(f'../output/final/graphs/eligibility_by_encounters_72hrs_{pyCLIF.helper[\"site_name\"]}.png')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(\n",
    "    x='Criteria', \n",
    "    y='Proportion Eligible Hours %', \n",
    "    data=aggregate_df, \n",
    "    palette=custom_colors\n",
    ")\n",
    "\n",
    "# Add percentages on top of the bars\n",
    "for index, row in aggregate_df.iterrows():\n",
    "    barplot.text(index, row['Proportion Eligible Hours %'], f\"{row['Proportion Eligible Hours %']:.1f}%\", \n",
    "                 color='black', ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Eligibility by Total Observed Hours (During first 72 hours)')\n",
    "plt.xlabel('Criteria')\n",
    "plt.ylabel('Percentage of Observed Hours Eligible')\n",
    "\n",
    "# Save the graph\n",
    "plt.savefig(f'../output/final/graphs/eligibility_by_total_hours_72hrs_{pyCLIF.helper[\"site_name\"]}.png')\n",
    "plt.close()\n",
    "\n",
    "# Define custom colors\n",
    "custom_colors = ['#983232', '#003f5c', '#fdfd96', '#98FB98']  # Maroon, Dark Blue, Pastel Yellow, Pastel Green\n",
    "\n",
    "# Plot proportion of eligible business hours\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(\n",
    "    x='Criteria', \n",
    "    y='Proportion Eligible BusHrs %', \n",
    "    data=aggregate_df, \n",
    "    palette=custom_colors\n",
    ")\n",
    "\n",
    "# Add percentages on top of the bars\n",
    "for index, row in aggregate_df.iterrows():\n",
    "    barplot.text(index, row['Proportion Eligible BusHrs %'], f\"{row['Proportion Eligible BusHrs %']:.1f}%\", \n",
    "                 color='black', ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Eligibility by Business Hours (During first 72 hours)')\n",
    "plt.xlabel('Criteria')\n",
    "plt.ylabel('Percentage of Business Hours Eligible')\n",
    "\n",
    "# Save the graph\n",
    "plt.savefig(f'../output/final/graphs/eligibility_by_business_hours_72hrs_{pyCLIF.helper[\"site_name\"]}.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e6a1b",
   "metadata": {
    "papermill": {
     "duration": 0.010439,
     "end_time": "2025-05-05T20:36:56.667523",
     "exception": false,
     "start_time": "2025-05-05T20:36:56.657084",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Aggregates for comparison across sites- Full Encounter Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8190d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:56.688459Z",
     "iopub.status.busy": "2025-05-05T20:36:56.688092Z",
     "iopub.status.idle": "2025-05-05T20:36:57.650763Z",
     "shell.execute_reply": "2025-05-05T20:36:57.650407Z"
    },
    "papermill": {
     "duration": 0.974292,
     "end_time": "2025-05-05T20:36:57.651852",
     "exception": false,
     "start_time": "2025-05-05T20:36:56.677560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "#  eligibility aggregates – ALL hours  vs  BUSINESS hours\n",
    "# --------------------------------------------------------------------\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# ── flag dictionaries ------------------------------------------------\n",
    "CRITS_ALL = {                         # eligibility at *any* hour\n",
    "    'Patel' : 'patel_flag_all_hours',\n",
    "    'TEAM'  : 'team_flag_all_hours',\n",
    "    'Yellow': 'any_yellow_or_green_no_red_all_hours',\n",
    "    'Green' : 'all_green_no_red_all_hours',\n",
    "}\n",
    "# business‑hour flags you already calculate in final_df\n",
    "BUSINESS_FLAGS = dict([                 # eligibility at 8-16 inclusive hour\n",
    "    ('Patel' , 'patel_flag'),\n",
    "    ('TEAM'  , 'team_flag'),\n",
    "    ('Yellow', 'any_yellow_or_green_no_red'),\n",
    "    ('Green' , 'all_green_no_red')\n",
    "])\n",
    "\n",
    "BUS_HRS = range(8, 17)   # 08:00–16:59 inclusive\n",
    "\n",
    "# ── denominators -----------------------------------------------------\n",
    "total_patients        = final_df['encounter_block'].nunique()\n",
    "total_observed_hours  = len(final_df)\n",
    "total_business_hours  = len(final_df[final_df['recorded_hour'].isin(BUS_HRS)])\n",
    "\n",
    "# ── build aggregate rows --------------------------------------------\n",
    "rows = []\n",
    "for crit in CRITS_ALL:                # guarantees consistent order\n",
    "    f_all   = CRITS_ALL[crit]\n",
    "    f_bus   = BUSINESS_FLAGS[crit]\n",
    "    \n",
    "    # ALL‑hour eligibility\n",
    "    elig_all_df   = final_df[final_df[f_all] == 1]\n",
    "    eligible_hours_all = len(elig_all_df)\n",
    "    eligible_patients  = elig_all_df['encounter_block'].nunique()\n",
    "    \n",
    "    # BUSINESS‑hour eligibility\n",
    "    elig_bus_df = final_df[\n",
    "        (final_df[f_bus] == 1) & (final_df['recorded_hour'].isin(BUS_HRS))\n",
    "    ]\n",
    "    eligible_business_hours = len(elig_bus_df)\n",
    "    \n",
    "    rows.append({\n",
    "        'Criteria'                      : crit,\n",
    "        'Total Patients'                : total_patients,\n",
    "        'Eligible Patients'             : eligible_patients,\n",
    "        'Total Observed Hours'          : total_observed_hours,\n",
    "        'Eligible Hours (all hrs)'      : eligible_hours_all,\n",
    "        'Total Business Hours'          : total_business_hours,\n",
    "        'Eligible Business Hours'       : eligible_business_hours,\n",
    "        'Proportion Eligible Hours %'   : 100*eligible_hours_all/total_observed_hours,\n",
    "        'Proportion Eligible BusHrs %'  : 100*eligible_business_hours/total_business_hours,\n",
    "        'Proportion Eligible Patients %': 100*eligible_patients/total_patients\n",
    "    })\n",
    "\n",
    "aggregate_df = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Save the aggregate data to CSV\n",
    "timestamp = datetime.now().date()\n",
    "aggregate_df.to_csv(f'../output/final/aggregates_{pyCLIF.helper[\"site_name\"]}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e43ed97",
   "metadata": {
    "papermill": {
     "duration": 0.010666,
     "end_time": "2025-05-05T20:36:57.672894",
     "exception": false,
     "start_time": "2025-05-05T20:36:57.662228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Eligibility by encounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b37027",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:57.694268Z",
     "iopub.status.busy": "2025-05-05T20:36:57.694098Z",
     "iopub.status.idle": "2025-05-05T20:36:57.768036Z",
     "shell.execute_reply": "2025-05-05T20:36:57.767641Z"
    },
    "papermill": {
     "duration": 0.085894,
     "end_time": "2025-05-05T20:36:57.768894",
     "exception": false,
     "start_time": "2025-05-05T20:36:57.683000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define custom colors\n",
    "custom_colors = ['#983232', '#003f5c', '#fdfd96', '#98FB98']  # Maroon, Dark Blue, Pastel Yellow, Pastel Green\n",
    "# Plot proportion of eligible encounters\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(\n",
    "    x='Criteria', \n",
    "    y='Proportion Eligible Patients %', \n",
    "    data=aggregate_df, \n",
    "    palette=custom_colors\n",
    ")\n",
    "\n",
    "# Add percentages on top of the bars\n",
    "for index, row in aggregate_df.iterrows():\n",
    "    barplot.text(index, row['Proportion Eligible Patients %'], f\"{row['Proportion Eligible Patients %']:.1f}%\", \n",
    "                 color='black', ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Eligibility by Encounter')\n",
    "plt.xlabel('Criteria')\n",
    "plt.ylabel('Percentage of Encounters Eligible')\n",
    "\n",
    "# Save the graph\n",
    "plt.savefig(f'../output/final/graphs/eligibility_by_encounters_{pyCLIF.helper[\"site_name\"]}.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd8051",
   "metadata": {},
   "source": [
    "### Eligibility by all hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a96f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(\n",
    "    x='Criteria', \n",
    "    y='Proportion Eligible Hours %', \n",
    "    data=aggregate_df, \n",
    "    palette=custom_colors\n",
    ")\n",
    "\n",
    "# Add percentages on top of the bars\n",
    "for index, row in aggregate_df.iterrows():\n",
    "    barplot.text(index, row['Proportion Eligible Hours %'], f\"{row['Proportion Eligible Hours %']:.1f}%\", \n",
    "                 color='black', ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Eligibility by Total Observed Hours')\n",
    "plt.xlabel('Criteria')\n",
    "plt.ylabel('Percentage of Observed Hours Eligible')\n",
    "\n",
    "# Save the graph\n",
    "plt.savefig(f'../output/final/graphs/eligibility_by_total_hours_{pyCLIF.helper[\"site_name\"]}.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71e73f9",
   "metadata": {
    "papermill": {
     "duration": 0.010056,
     "end_time": "2025-05-05T20:36:57.789368",
     "exception": false,
     "start_time": "2025-05-05T20:36:57.779312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Eligibility by business hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3edaf77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:57.810867Z",
     "iopub.status.busy": "2025-05-05T20:36:57.810707Z",
     "iopub.status.idle": "2025-05-05T20:36:57.888351Z",
     "shell.execute_reply": "2025-05-05T20:36:57.888018Z"
    },
    "papermill": {
     "duration": 0.089409,
     "end_time": "2025-05-05T20:36:57.889245",
     "exception": false,
     "start_time": "2025-05-05T20:36:57.799836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define custom colors\n",
    "custom_colors = ['#983232', '#003f5c', '#fdfd96', '#98FB98']  # Maroon, Dark Blue, Pastel Yellow, Pastel Green\n",
    "\n",
    "# Plot proportion of eligible business hours\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(\n",
    "    x='Criteria', \n",
    "    y='Proportion Eligible BusHrs %', \n",
    "    data=aggregate_df, \n",
    "    palette=custom_colors\n",
    ")\n",
    "\n",
    "# Add percentages on top of the bars\n",
    "for index, row in aggregate_df.iterrows():\n",
    "    barplot.text(index, row['Proportion Eligible BusHrs %'], f\"{row['Proportion Eligible BusHrs %']:.1f}%\", \n",
    "                 color='black', ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Eligibility by Business Hours')\n",
    "plt.xlabel('Criteria')\n",
    "plt.ylabel('Percentage of Business Hours Eligible')\n",
    "\n",
    "# Save the graph\n",
    "plt.savefig(f'../output/final/graphs/eligibility_by_business_hours_{pyCLIF.helper[\"site_name\"]}.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3190b",
   "metadata": {},
   "source": [
    "### Eligibility by business hour - One week trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44637a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUSINESS_FLAGS = dict([                 # eligibility at 8-16 inclusive hour\n",
    "    ('Patel' , 'patel_flag'),\n",
    "    ('TEAM'  , 'team_flag'),\n",
    "    ('Yellow', 'any_yellow_or_green_no_red'),\n",
    "    ('Green' , 'all_green')\n",
    "])\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1) Restrict to first week (≤ 7 days = 168 hours) and bin by day\n",
    "# ---------------------------------------------------------------\n",
    "df_week = final_df.query(\"time_from_vent <= 168\").copy()\n",
    "df_week[\"day_bin\"] = (df_week[\"time_from_vent\"] // 24).astype(int)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2) Build trend DataFrame\n",
    "# ---------------------------------------------------------------\n",
    "trend_rows = []\n",
    "for crit, flag in BUSINESS_FLAGS.items():          # e.g. {'Patel': 'patel_flag', ...}\n",
    "    for day, g in df_week.groupby(\"day_bin\", sort=True):\n",
    "        # only count rows during business hours (8–16)\n",
    "        bus = g[g[\"recorded_hour\"].between(8, 16)]\n",
    "        denom = bus.shape[0]\n",
    "        num   = bus[bus[flag] == 1].shape[0]\n",
    "        prop  = num / denom if denom else np.nan\n",
    "        trend_rows.append({\n",
    "            \"criterion\": crit,\n",
    "            \"day\":       day,\n",
    "            \"prop_bus_hrs\": prop\n",
    "        })\n",
    "\n",
    "trend_df = pd.DataFrame(trend_rows)\n",
    "# save trend df\n",
    "trend_df.to_csv(f'../output/final/eligibility_trend_first_week_{pyCLIF.helper[\"site_name\"]}.csv', index=False)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3) Plot with custom colors\n",
    "# ---------------------------------------------------------------\n",
    "custom_colors = {\n",
    "    'Patel':  '#983232',  # maroon\n",
    "    'TEAM':   '#003f5c',  # dark blue\n",
    "    'Yellow': '#c9b037',  # pastel yellow\n",
    "    'Green':  '#2e8b57'   # pastel green\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(\n",
    "    data=trend_df,\n",
    "    x=\"day\",\n",
    "    y=\"prop_bus_hrs\",\n",
    "    hue=\"criterion\",\n",
    "    hue_order=list(custom_colors.keys()),\n",
    "    palette=custom_colors,\n",
    "    marker=\"o\",\n",
    "    linewidth=2\n",
    ")\n",
    "plt.xlabel(\"Days since intubation\")\n",
    "plt.ylabel(\"Proportion of business hours eligible\")\n",
    "plt.title(\"Eligibility trend — first week (Day 0-6)\")\n",
    "plt.xticks(range(0, 7))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title=\"Criterion\")\n",
    "plt.tight_layout()\n",
    "# save figure\n",
    "plt.savefig(\"../output/final/graphs/eligibility_trend_first_week.png\", dpi=300)\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c80bb4",
   "metadata": {},
   "source": [
    "#### ECDF \n",
    "\n",
    "x‑axis (fraction of encounter hours eligible) – for each patient, what proportion of their ventilated hours satisfied the rule.\n",
    "y‑axis (proportion of patients ≤ x) – at any x, the height of a curve tells you what fraction of patients have eligibility no greater than x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e241cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (final_df\n",
    "       .groupby(['encounter_block'])\n",
    "       .agg(**{name: (flag, 'mean') for name, flag in CRITS_ALL.items()}))\n",
    "\n",
    "sns.ecdfplot(data=tmp.melt(var_name='criterion', value_name='pct'),\n",
    "             x='pct', hue='criterion')\n",
    "plt.xlabel('Fraction of encounter hours eligible')\n",
    "#save this plot\n",
    "plt.savefig(f'../output/final/graphs/eligibility_hour_ecdf_{pyCLIF.helper[\"site_name\"]}.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb80d25",
   "metadata": {},
   "source": [
    "### Hourly distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1527539",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITS = {\n",
    "    'Patel' : 'patel_flag_all_hours',\n",
    "    'TEAM'  : 'team_flag_all_hours',\n",
    "    'Yellow': 'any_yellow_or_green_no_red_all_hours',\n",
    "    'Green' : 'all_green_all_hours',\n",
    "    'Green_no_red': 'all_green_no_red_all_hours'\n",
    "}\n",
    "BUS_HRS = range(8, 17)\n",
    "custom_colors = [\n",
    "    '#983232',  # Maroon\n",
    "    '#003f5c',  # Dark Blue\n",
    "    '#c9b037',  # Darker Yellow (Gold/Mustard tone)\n",
    "    '#2e8b57',  # Darker Green (Sea Green)\n",
    "    '#006400'   # Dark Green\n",
    "]\n",
    "color_map = dict(zip(CRITS.keys(), custom_colors))\n",
    "\n",
    "# ── create combined hourly data (blocks per hour) ---------------------\n",
    "hourly_data = pd.DataFrame({'hour': range(24)})\n",
    "for name, flag in CRITS.items():\n",
    "    by_hour = (\n",
    "        final_df\n",
    "        .loc[final_df[flag] == 1, ['encounter_block','recorded_hour']]\n",
    "        .drop_duplicates()                                   # one row per block/hour\n",
    "        .groupby('recorded_hour')['encounter_block']\n",
    "        .nunique()                                          # count blocks\n",
    "        .reindex(range(24), fill_value=0)                   # ensure 0–23\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    hourly_data[name] = by_hour\n",
    "\n",
    "# ── melt for seaborn plotting ------------------------------------------\n",
    "hourly_melted = hourly_data.melt(\n",
    "    id_vars='hour',\n",
    "    var_name='Criteria',\n",
    "    value_name='Eligible Blocks'\n",
    ")\n",
    "#SAVE hourly_melted TO CSV\n",
    "hourly_melted.to_csv(f'../output/final/eligibility_hourly_melted_{pyCLIF.helper[\"site_name\"]}.csv', index=False)\n",
    "\n",
    "# ── plot ---------------------------------------------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "for crit, color in color_map.items():\n",
    "    sub = hourly_melted[hourly_melted['Criteria'] == crit]\n",
    "    plt.plot(sub['hour'], sub['Eligible Blocks'],\n",
    "             label=crit, marker='o', color=color)\n",
    "\n",
    "plt.axvspan(8, 17, color='orange', alpha=0.1, label='Business Hours')\n",
    "plt.title(\"Hourly Distribution of Eligible Blocks by Criteria\")\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"Number of Eligible Blocks\")\n",
    "plt.xticks(range(24))\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1,1), title=\"Criterion\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# save\n",
    "plt.savefig(\n",
    "    f'../output/final/graphs/eligibility_hourly_distribution_blocks_{pyCLIF.helper[\"site_name\"]}.png',\n",
    "    dpi=300\n",
    ")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5231f3",
   "metadata": {
    "papermill": {
     "duration": 0.010549,
     "end_time": "2025-05-05T20:36:57.910857",
     "exception": false,
     "start_time": "2025-05-05T20:36:57.900308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Failure by subcomponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7753e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:57.933105Z",
     "iopub.status.busy": "2025-05-05T20:36:57.932926Z",
     "iopub.status.idle": "2025-05-05T20:36:59.144883Z",
     "shell.execute_reply": "2025-05-05T20:36:59.144592Z"
    },
    "papermill": {
     "duration": 1.223828,
     "end_time": "2025-05-05T20:36:59.145679",
     "exception": false,
     "start_time": "2025-05-05T20:36:57.921851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define your criteria and corresponding subcomponent flags\n",
    "criteria_info = {\n",
    "    'patel_flag': {'resp_flag': 'patel_resp_flag', 'cardio_flag': 'patel_cardio_flag'},\n",
    "    'team_flag': {'resp_flag': 'team_resp_flag', 'cardio_flag': 'team_cardio_flag'},\n",
    "    'any_yellow_or_green_no_red': {'resp_flag': 'yellow_resp_flag', 'cardio_flag': 'yellow_cardio_flag'},\n",
    "    'all_green_no_red': {'resp_flag': 'green_resp_flag', 'cardio_flag': 'green_cardio_flag'}\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Loop over each criterion\n",
    "for criterion, flags in criteria_info.items():\n",
    "    resp_flag = flags['resp_flag']\n",
    "    cardio_flag = flags['cardio_flag']\n",
    "    \n",
    "    # Calculate total hours per hospitalization_id\n",
    "    total_hours = final_df.groupby('encounter_block').size().rename('total_hours')\n",
    "    \n",
    "    # Create failure indicators\n",
    "    df_failure = final_df.copy()\n",
    "    df_failure['resp_only_failure'] = ((df_failure[resp_flag] == 0) & (df_failure[cardio_flag] == 1)).astype(int)\n",
    "    df_failure['cardio_only_failure'] = ((df_failure[resp_flag] == 1) & (df_failure[cardio_flag] == 0)).astype(int)\n",
    "    df_failure['both_failures'] = ((df_failure[resp_flag] == 0) & (df_failure[cardio_flag] == 0)).astype(int)\n",
    "    \n",
    "    # Aggregate the counts per hospitalization_id\n",
    "    failure_counts = df_failure.groupby('encounter_block')[['resp_only_failure', 'cardio_only_failure', 'both_failures']].sum()\n",
    "    \n",
    "    # Merge with total hours\n",
    "    failure_counts = failure_counts.merge(total_hours, left_index=True, right_index=True)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    failure_counts['resp_only_failure_perc'] = (failure_counts['resp_only_failure'] * 100 / failure_counts['total_hours']).round(3)\n",
    "    failure_counts['cardio_only_failure_perc'] = (failure_counts['cardio_only_failure'] * 100 / failure_counts['total_hours']).round(3)\n",
    "    failure_counts['both_failures_perc'] = (failure_counts['both_failures'] * 100 / failure_counts['total_hours']).round(3)\n",
    "    \n",
    "    # Calculate total failure percentage\n",
    "    failure_counts['total_failure_perc'] = (\n",
    "        failure_counts['resp_only_failure'] + failure_counts['cardio_only_failure'] + failure_counts['both_failures']\n",
    "    ) * 100 / failure_counts['total_hours']\n",
    "    \n",
    "    # Calculate criterion met percentage\n",
    "    criterion_met = final_df.groupby('encounter_block')[criterion].sum().rename('criterion_met_hours')\n",
    "    failure_counts = failure_counts.merge(criterion_met, left_index=True, right_index=True)\n",
    "    failure_counts['criterion_met_perc'] = (failure_counts['criterion_met_hours'] * 100 / failure_counts['total_hours']).round(3)\n",
    "    \n",
    "    # Add criterion name to the DataFrame\n",
    "    failure_counts['Criteria'] = criterion\n",
    "    \n",
    "    # Append to results\n",
    "    results.append(failure_counts.reset_index())\n",
    "\n",
    "# Concatenate results for all criteria\n",
    "all_failure_counts = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Now, calculate the average percentages across all hospitalizations for each criterion\n",
    "avg_failure_percentages = all_failure_counts.groupby('Criteria').agg({\n",
    "    'resp_only_failure_perc': 'mean',\n",
    "    'cardio_only_failure_perc': 'mean',\n",
    "    'both_failures_perc': 'mean',\n",
    "    'total_failure_perc': 'mean',\n",
    "    'criterion_met_perc': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "avg_failure_percentages = avg_failure_percentages.rename(columns={\n",
    "    'resp_only_failure_perc': 'Resp Failure Only',\n",
    "    'cardio_only_failure_perc': 'Cardio Failure Only',\n",
    "    'both_failures_perc': 'Both Failures',\n",
    "    'total_failure_perc': 'Total Failure',\n",
    "    'criterion_met_perc': 'Criterion Met'\n",
    "})\n",
    "\n",
    "# Display the average failure percentages\n",
    "criteria_mapping = {\n",
    "    'patel_flag': 'Patel',\n",
    "    'team_flag': 'TEAM',\n",
    "    'any_yellow_or_green_no_red': 'Yellow',\n",
    "    'all_green_no_red': 'Green'\n",
    "}\n",
    "\n",
    "avg_failure_percentages['Criteria'] = avg_failure_percentages['Criteria'].replace(criteria_mapping)\n",
    "avg_failure_percentages['site_name'] = pyCLIF.helper[\"site_name\"]\n",
    "pd.DataFrame(avg_failure_percentages).to_csv(f'../output/final/avg_failure_percentages_{pyCLIF.helper[\"site_name\"]}.csv',index=False)\n",
    "avg_failure_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04691f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:36:59.168310Z",
     "iopub.status.busy": "2025-05-05T20:36:59.168158Z",
     "iopub.status.idle": "2025-05-05T20:37:00.142583Z",
     "shell.execute_reply": "2025-05-05T20:37:00.142184Z"
    },
    "papermill": {
     "duration": 1.033448,
     "end_time": "2025-05-05T20:37:00.190042",
     "exception": false,
     "start_time": "2025-05-05T20:36:59.156594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kaleido\n",
    "import plotly.graph_objects as go\n",
    "# Create a stacked bar plot using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add bars for Cardio Failure Only\n",
    "fig.add_trace(go.Bar(\n",
    "    x=avg_failure_percentages['Criteria'],\n",
    "    y=avg_failure_percentages['Cardio Failure Only'],\n",
    "    name='Cardio Failure Only',\n",
    "    marker_color='#003366'  # Dark Blue\n",
    "))\n",
    "\n",
    "# Add bars for Resp Failure Only\n",
    "fig.add_trace(go.Bar(\n",
    "    x=avg_failure_percentages['Criteria'],\n",
    "    y=avg_failure_percentages['Resp Failure Only'],\n",
    "    name='Resp Failure Only',\n",
    "    marker_color='#983232'  # Maroon\n",
    "))\n",
    "\n",
    "# Add bars for Both Failures\n",
    "fig.add_trace(go.Bar(\n",
    "    x=avg_failure_percentages['Criteria'],\n",
    "    y=avg_failure_percentages['Both Failures'],\n",
    "    name='Both Failures',\n",
    "    marker_color='#fdfd96'  # Pastel Yellow\n",
    "))\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    xaxis_title='Criteria',\n",
    "    yaxis_title='Average Percentage of Business Hours Not Met (%)',\n",
    "    yaxis=dict(range=[0, 100]),\n",
    "    template='plotly_white',\n",
    "    legend_title='Failure Type'\n",
    ")\n",
    "# Save the plot\n",
    "fig.write_image(f'../output/final/graphs/avg_failure_components_{pyCLIF.helper[\"site_name\"]}_{datetime.now().date()}.png')\n",
    "# Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd2f43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:37:00.300456Z",
     "iopub.status.busy": "2025-05-05T20:37:00.300291Z",
     "iopub.status.idle": "2025-05-05T20:37:00.308364Z",
     "shell.execute_reply": "2025-05-05T20:37:00.308037Z"
    },
    "papermill": {
     "duration": 0.064543,
     "end_time": "2025-05-05T20:37:00.309250",
     "exception": false,
     "start_time": "2025-05-05T20:37:00.244707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────\n",
    "#  Enhanced analyse_criterion with image size controls and data export\n",
    "# ────────────────────────────────────────────────────────────\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "from upsetplot import from_indicators, UpSet\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def analyse_criterion_enhanced(\n",
    "    df: pd.DataFrame,\n",
    "    crit_name: str,\n",
    "    *,\n",
    "    flag_cols: list,             # list of sub‑criterion flags (0/1)\n",
    "    master_flag: str,            # overall eligibility flag (0/1)\n",
    "    id_col: str = \"encounter_block\",\n",
    "    time_col: str = \"time_from_vent\",\n",
    "    out_dir = \"../output/final\",\n",
    "    save_fig_data: bool = True,\n",
    "    max_upset_combinations: int = 50,  # Limit UpSet plot size\n",
    "    figure_width: int = 12,            # Control figure width\n",
    "    figure_height: int = 8,            # Control figure height\n",
    "    site_name: str = None              # For data export labeling\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced criterion failure analysis with image size controls and data export.\n",
    "    \n",
    "    • Find blocks that *never* satisfy `master_flag`\n",
    "    • For those blocks:\n",
    "        - how often is each sub-flag FALSE?\n",
    "        - which sub-flag is the *latest* to turn TRUE (primary blocker)\n",
    "        - which combinations of sub-flags ever fail (UpSet)\n",
    "        - export aggregated data for future multi-site analysis\n",
    "    \n",
    "    Features:\n",
    "    - Controls image sizes to prevent memory issues\n",
    "    - Limits UpSet plot complexity\n",
    "    - Exports combination data for multi-site sharing\n",
    "    - Better error handling\n",
    "    - Comprehensive failure analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    out_dir = Path(out_dir, crit_name.lower())\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"[{crit_name}] Starting enhanced failure analysis...\")\n",
    "    \n",
    "    # ── 1 · Find blocks that NEVER became eligible ─────────────────────────\n",
    "    try:\n",
    "        never = (df.groupby(id_col)[master_flag].max()\n",
    "                   .reset_index(name=\"ever\")[lambda d: d[\"ever\"] == 0]\n",
    "                   .drop(columns=\"ever\"))\n",
    "        fail = never.merge(df, on=id_col, how=\"inner\")\n",
    "        \n",
    "        n_failed_blocks = fail[id_col].nunique()\n",
    "        n_total_hours = len(fail)\n",
    "        n_total_blocks = df[id_col].nunique()\n",
    "        \n",
    "        print(f\"[{crit_name}] {n_failed_blocks:,} blocks never became eligible out of {n_total_blocks:,} total blocks\")\n",
    "        print(f\"[{crit_name}] {n_total_hours:,} total hours of observation in failed blocks\")\n",
    "        \n",
    "        if n_failed_blocks == 0:\n",
    "            print(f\"[{crit_name}] No failed blocks found - all encounters achieved eligibility!\")\n",
    "            return None, None, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[{crit_name}] ERROR in block identification: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    # ── 2 · Component failure analysis ──────────────────────────────────\n",
    "    print(f\"[{crit_name}] Calculating component failure rates...\")\n",
    "    \n",
    "    try:\n",
    "        # Filter flag_cols to only include columns that exist in the dataframe\n",
    "        available_flags = [col for col in flag_cols if col in fail.columns]\n",
    "        missing_flags = [col for col in flag_cols if col not in fail.columns]\n",
    "        \n",
    "        if missing_flags:\n",
    "            print(f\"[{crit_name}] WARNING: Missing columns: {missing_flags}\")\n",
    "        \n",
    "        if not available_flags:\n",
    "            print(f\"[{crit_name}] ERROR: No flag columns found in dataframe\")\n",
    "            return None, None, None\n",
    "            \n",
    "        # Calculate hourly failure rates\n",
    "        long = fail.melt(id_vars=[id_col], value_vars=available_flags,\n",
    "                         var_name=\"criterion\", value_name=\"flag\")\n",
    "        \n",
    "        hourly_summary = (long.groupby(\"criterion\")[\"flag\"]\n",
    "                             .apply(lambda s: (s == 0).mean())\n",
    "                             .rename(\"prop_hours_failed\")\n",
    "                             .reset_index()\n",
    "                             .sort_values(\"prop_hours_failed\", ascending=False))\n",
    "        \n",
    "        # Calculate encounter-level failure rates\n",
    "        encounter_failures = []\n",
    "        for flag in available_flags:\n",
    "            # For each encounter, check if flag was EVER 1 (True)\n",
    "            encounter_ever_met = fail.groupby(id_col)[flag].max()\n",
    "            encounter_never_met = (encounter_ever_met == 0).sum()\n",
    "            encounter_failure_rate = encounter_never_met / n_failed_blocks if n_failed_blocks > 0 else 0\n",
    "            \n",
    "            encounter_failures.append({\n",
    "                'criterion': flag,\n",
    "                'encounters_never_met': encounter_never_met,\n",
    "                'encounter_failure_rate': encounter_failure_rate,\n",
    "                'total_failed_encounters': n_failed_blocks\n",
    "            })\n",
    "        \n",
    "        encounter_df = pd.DataFrame(encounter_failures)\n",
    "        summary = hourly_summary.merge(encounter_df, on='criterion')\n",
    "        \n",
    "        # Add additional metadata\n",
    "        summary['criteria_name'] = crit_name\n",
    "        summary['site_name'] = site_name if site_name else \"unknown\"\n",
    "        summary['total_encounters'] = n_total_blocks\n",
    "        summary['failed_encounters'] = n_failed_blocks\n",
    "        \n",
    "        if save_fig_data:\n",
    "            summary.to_csv(out_dir/\"component_failure_analysis.csv\", index=False)\n",
    "            print(f\"[{crit_name}] ✓ Component failure analysis saved\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{crit_name}] ERROR in failure rate calculation: {e}\")\n",
    "        summary = pd.DataFrame()\n",
    "\n",
    "    # ── 3 · Create failure rate plots with size controls ───────────────\n",
    "    try:\n",
    "        if len(summary) > 0:\n",
    "            # Limit number of bars to prevent oversized plots\n",
    "            max_bars = min(15, len(summary))\n",
    "            plot_summary = summary.head(max_bars)\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(figure_width, figure_height))\n",
    "            \n",
    "            # Hourly failure rates\n",
    "            bars1 = ax1.barh(range(len(plot_summary)), plot_summary[\"prop_hours_failed\"])\n",
    "            ax1.set_yticks(range(len(plot_summary)))\n",
    "            ax1.set_yticklabels(plot_summary[\"criterion\"], fontsize=8)\n",
    "            ax1.invert_yaxis()\n",
    "            ax1.set_xlabel(\"Proportion of hours NOT satisfied\")\n",
    "            ax1.set_title(f\"{crit_name}: Hourly Failure Rates\")\n",
    "            ax1.grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # Add values on bars\n",
    "            for i, bar in enumerate(bars1):\n",
    "                width = bar.get_width()\n",
    "                ax1.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                        f'{width:.2f}', ha='left', va='center', fontsize=7)\n",
    "            \n",
    "            # Encounter failure rates  \n",
    "            bars2 = ax2.barh(range(len(plot_summary)), plot_summary[\"encounter_failure_rate\"])\n",
    "            ax2.set_yticks(range(len(plot_summary)))\n",
    "            ax2.set_yticklabels(plot_summary[\"criterion\"], fontsize=8)\n",
    "            ax2.invert_yaxis()\n",
    "            ax2.set_xlabel(\"Proportion of encounters never meeting criteria\")\n",
    "            ax2.set_title(f\"{crit_name}: Encounter Failure Rates\")\n",
    "            ax2.grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # Add values on bars\n",
    "            for i, bar in enumerate(bars2):\n",
    "                width = bar.get_width()\n",
    "                ax2.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                        f'{width:.2f}', ha='left', va='center', fontsize=7)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(out_dir/\"component_failure_rates.png\", dpi=200, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(f\"[{crit_name}] ✓ Failure rate plots saved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[{crit_name}] ERROR in plotting: {e}\")\n",
    "\n",
    "    # ── 4 · Primary blocker analysis ─────────────────────────────────────\n",
    "    print(f\"[{crit_name}] Analyzing primary blockers...\")\n",
    "    \n",
    "    try:\n",
    "        def first_true(g, col):\n",
    "            if col not in g.columns:\n",
    "                return np.inf\n",
    "            hit = g.loc[g[col] == 1, time_col]\n",
    "            return hit.min() if not hit.empty else np.inf\n",
    "\n",
    "        prim = []\n",
    "        for blk, g in fail.groupby(id_col):\n",
    "            lags = {c: first_true(g, c) for c in available_flags}\n",
    "            # Find the flag that took longest to become True (primary blocker)\n",
    "            valid_lags = {k: v for k, v in lags.items() if v != np.inf}\n",
    "            if valid_lags:\n",
    "                prim_blk = max(valid_lags, key=valid_lags.get)\n",
    "                prim_time = valid_lags[prim_blk]\n",
    "            else:\n",
    "                prim_blk = \"never_achieved\"\n",
    "                prim_time = np.inf\n",
    "            prim.append([blk, prim_blk, prim_time])\n",
    "\n",
    "        prim_df = pd.DataFrame(prim, columns=[id_col, \"primary_blocker\", \"time_to_first_true\"])\n",
    "        \n",
    "        # Primary blocker frequency\n",
    "        blocker_counts = prim_df['primary_blocker'].value_counts()\n",
    "        blocker_summary = pd.DataFrame({\n",
    "            'primary_blocker': blocker_counts.index,\n",
    "            'encounter_count': blocker_counts.values,\n",
    "            'proportion': blocker_counts.values / len(prim_df),\n",
    "            'criteria_name': crit_name,\n",
    "            'site_name': site_name if site_name else \"unknown\"\n",
    "        })\n",
    "        \n",
    "        if save_fig_data:\n",
    "            prim_df.to_csv(out_dir/\"primary_blockers_by_encounter.csv\", index=False)\n",
    "            blocker_summary.to_csv(out_dir/\"primary_blocker_summary.csv\", index=False)\n",
    "            print(f\"[{crit_name}] ✓ Primary blocker analysis saved\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[{crit_name}] ERROR in primary blocker analysis: {e}\")\n",
    "        prim_df = pd.DataFrame()\n",
    "\n",
    "    # ── 5 · Combination pattern analysis and UpSet plot ────────────────\n",
    "    print(f\"[{crit_name}] Analyzing failure combinations...\")\n",
    "    \n",
    "    combination_data = None\n",
    "    try:\n",
    "        # Create failure matrix (True = criterion failed at any point)\n",
    "        failed_matrix = fail[available_flags].eq(0)\n",
    "        block_fail = failed_matrix.groupby(fail[id_col]).max()  # Any failure in block\n",
    "        \n",
    "        # Get combination counts\n",
    "        combination_counts = block_fail.value_counts()\n",
    "        \n",
    "        # Limit combinations for UpSet plot to prevent memory issues\n",
    "        top_combinations = combination_counts.head(max_upset_combinations)\n",
    "        \n",
    "        if len(top_combinations) > 0:\n",
    "            # Create subset for UpSet plot\n",
    "            top_combination_tuples = top_combinations.index.tolist()\n",
    "            subset_mask = block_fail.apply(lambda x: tuple(x) in top_combination_tuples, axis=1)\n",
    "            subset_data = block_fail[subset_mask]\n",
    "            \n",
    "            if len(subset_data) > 5:  # Need minimum data for UpSet plot\n",
    "                upset_data = from_indicators(subset_data.columns, subset_data)\n",
    "                \n",
    "                # Create UpSet plot with controlled size\n",
    "                fig = plt.figure(figsize=(min(figure_width, 14), min(figure_height, 10)))\n",
    "                upset = UpSet(upset_data, show_counts=True, sort_by=\"cardinality\")\n",
    "                upset.plot(fig=fig)\n",
    "                plt.suptitle(f\"{crit_name}: Top {len(top_combinations)} Failure Combinations\\n\"\n",
    "                           f\"({len(subset_data)} encounters shown)\", fontsize=12)\n",
    "                plt.savefig(out_dir/\"failure_combinations_upset.png\", dpi=200, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                print(f\"[{crit_name}] ✓ UpSet plot saved (showing {len(top_combinations)} combinations)\")\n",
    "            else:\n",
    "                print(f\"[{crit_name}] WARNING: Insufficient data for UpSet plot ({len(subset_data)} encounters)\")\n",
    "        \n",
    "        # ── 6 · Export combination data for multi-site analysis ───────\n",
    "        combination_data = export_combination_data(\n",
    "            block_fail, combination_counts, crit_name, site_name, out_dir, save_fig_data\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[{crit_name}] ERROR in combination analysis: {e}\")\n",
    "\n",
    "    # ── 7 · Business hours and tracheostomy analysis ───────────────────\n",
    "    try:\n",
    "        additional_analysis = {}\n",
    "        \n",
    "        # Business hours analysis\n",
    "        if 'recorded_hour' in fail.columns:\n",
    "            business_hours = list(range(8, 17))  # 8 AM to 5 PM\n",
    "            fail['is_business_hours'] = fail['recorded_hour'].isin(business_hours)\n",
    "            \n",
    "            bh_by_encounter = fail.groupby(id_col).agg({\n",
    "                'is_business_hours': ['sum', 'count', 'mean']\n",
    "            }).round(3)\n",
    "            bh_by_encounter.columns = ['business_hours_count', 'total_hours', 'business_hours_proportion']\n",
    "            \n",
    "            bh_summary = {\n",
    "                'never_business_hours': (bh_by_encounter['business_hours_count'] == 0).sum(),\n",
    "                'only_business_hours': (bh_by_encounter['business_hours_count'] == bh_by_encounter['total_hours']).sum(),\n",
    "                'mixed_hours': len(bh_by_encounter) - (bh_by_encounter['business_hours_count'] == 0).sum() - (bh_by_encounter['business_hours_count'] == bh_by_encounter['total_hours']).sum(),\n",
    "                'median_bh_proportion': bh_by_encounter['business_hours_proportion'].median(),\n",
    "                'mean_bh_proportion': bh_by_encounter['business_hours_proportion'].mean()\n",
    "            }\n",
    "            additional_analysis['business_hours'] = bh_summary\n",
    "            \n",
    "            if save_fig_data:\n",
    "                bh_by_encounter.to_csv(out_dir/\"business_hours_by_encounter.csv\")\n",
    "                pd.DataFrame([bh_summary]).to_csv(out_dir/\"business_hours_summary.csv\", index=False)\n",
    "        \n",
    "        # Tracheostomy analysis\n",
    "        if 'hourly_trach' in fail.columns:\n",
    "            trach_by_encounter = fail.groupby(id_col)['hourly_trach'].max()\n",
    "            trach_summary = {\n",
    "                'encounters_with_trach': (trach_by_encounter == 1).sum(),\n",
    "                'encounters_without_trach': (trach_by_encounter == 0).sum(),\n",
    "                'trach_proportion': (trach_by_encounter == 1).mean()\n",
    "            }\n",
    "            additional_analysis['tracheostomy'] = trach_summary\n",
    "            \n",
    "            if save_fig_data:\n",
    "                pd.DataFrame([trach_summary]).to_csv(out_dir/\"tracheostomy_summary.csv\", index=False)\n",
    "        \n",
    "        # Paralytic analysis\n",
    "        if 'paralytics_flag' in fail.columns:\n",
    "            paralytic_by_encounter = fail.groupby(id_col)['paralytics_flag'].max()\n",
    "            paralytic_summary = {\n",
    "                'encounters_with_paralytics': (paralytic_by_encounter == 1).sum(),\n",
    "                'encounters_without_paralytics': (paralytic_by_encounter == 0).sum(),\n",
    "                'paralytic_proportion': (paralytic_by_encounter == 1).mean()\n",
    "            }\n",
    "            additional_analysis['paralytics'] = paralytic_summary\n",
    "            \n",
    "            if save_fig_data:\n",
    "                pd.DataFrame([paralytic_summary]).to_csv(out_dir/\"paralytic_summary.csv\", index=False)\n",
    "        \n",
    "        if save_fig_data and additional_analysis:\n",
    "            print(f\"[{crit_name}] ✓ Additional analyses saved\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[{crit_name}] ERROR in additional analysis: {e}\")\n",
    "\n",
    "    print(f\"[{crit_name}] ✅ Analysis complete. Results saved to {out_dir}/\")\n",
    "    \n",
    "    return summary, prim_df, combination_data\n",
    "\n",
    "def export_combination_data(block_fail, combination_counts, crit_name, site_name, out_dir, save_data=True):\n",
    "    \"\"\"\n",
    "    Export combination pattern data for future multi-site analysis.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create detailed combination data\n",
    "        combination_details = []\n",
    "        \n",
    "        for combination_tuple, count in combination_counts.items():\n",
    "            # Convert combination to readable format\n",
    "            combination_dict = dict(zip(block_fail.columns, combination_tuple))\n",
    "            \n",
    "            # Create combination signature for multi-site matching\n",
    "            signature = '|'.join([f\"{col}:{int(val)}\" for col, val in combination_dict.items()])\n",
    "            \n",
    "            combination_details.append({\n",
    "                'combination_signature': signature,\n",
    "                'encounter_count': count,\n",
    "                'proportion_of_failed': count / len(block_fail),\n",
    "                'criteria_name': crit_name,\n",
    "                'site_name': site_name if site_name else \"unknown\",\n",
    "                'total_failed_encounters': len(block_fail),\n",
    "                **combination_dict  # Include individual flag values\n",
    "            })\n",
    "        \n",
    "        combination_df = pd.DataFrame(combination_details)\n",
    "        combination_df = combination_df.sort_values('encounter_count', ascending=False)\n",
    "        \n",
    "        if save_data:\n",
    "            # Save detailed data\n",
    "            combination_df.to_csv(out_dir/\"combination_patterns_detailed.csv\", index=False)\n",
    "            \n",
    "            # Save simplified version for multi-site sharing\n",
    "            multisite_df = combination_df[['combination_signature', 'encounter_count', 'proportion_of_failed',\n",
    "                                         'criteria_name', 'site_name', 'total_failed_encounters']]\n",
    "            multisite_df.to_csv(out_dir/\"combination_patterns_for_multisite.csv\", index=False)\n",
    "            \n",
    "            # Save summary statistics\n",
    "            summary_stats = {\n",
    "                'criteria_name': crit_name,\n",
    "                'site_name': site_name if site_name else \"unknown\",\n",
    "                'total_failed_encounters': len(block_fail),\n",
    "                'unique_combinations': len(combination_counts),\n",
    "                'most_common_combination': combination_df.iloc[0]['combination_signature'] if len(combination_df) > 0 else \"none\",\n",
    "                'most_common_count': combination_df.iloc[0]['encounter_count'] if len(combination_df) > 0 else 0,\n",
    "                'combinations_affecting_50pct': (combination_df['proportion_of_failed'].cumsum() <= 0.5).sum() + 1\n",
    "            }\n",
    "            pd.DataFrame([summary_stats]).to_csv(out_dir/\"combination_summary_stats.csv\", index=False)\n",
    "            \n",
    "        print(f\"[{crit_name}] ✓ Exported {len(combination_df)} unique failure combinations\")\n",
    "        return combination_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[{crit_name}] ERROR in combination data export: {e}\")\n",
    "        return None\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Run enhanced analysis for all criteria\n",
    "# ────────────────────────────────────────────────────────────\n",
    "\n",
    "# Get site name from pyCLIF helper\n",
    "try:\n",
    "    site_name = pyCLIF.helper.get(\"site_name\", \"unknown_site\")\n",
    "except:\n",
    "    site_name = \"unknown_site\"\n",
    "\n",
    "print(\"🔍 ENHANCED CRITERION FAILURE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define flag lists with trach and paralytics included\n",
    "team_flags_enhanced = [\"team_pulse_flag\", \"team_lactate_flag\", \"team_ne_flag\",\n",
    "                      \"team_fio2_flag\", \"team_peep_flag\", \"team_resp_rate_flag\", \n",
    "                      'hourly_trach', 'paralytics_flag']\n",
    "\n",
    "patel_flags_enhanced = ['patel_map_flag','patel_sbp_flag','patel_pulse_flag', \n",
    "                       'patel_resp_rate_flag', 'patel_spo2_flag', 'patel_resp_flag', \n",
    "                       'patel_cardio_flag', 'hourly_trach', 'paralytics_flag']\n",
    "\n",
    "yellow_flags_enhanced = [\n",
    "    'hourly_trach', 'paralytics_flag',\n",
    "    # Red Flags\n",
    "    'red_resp_spo2_flag', 'red_map_flag', 'red_high_support_flag',\n",
    "    'red_hypertensive_flag', 'red_pulse_high_flag', 'red_pulse_low_flag',\n",
    "    # Yellow Flags\n",
    "    'yellow_resp_spo2_flag', 'yellow_fio2_flag', 'yellow_resp_rate_flag',\n",
    "    'yellow_peep_flag', 'yellow_map_flag', 'yellow_pulse_flag',\n",
    "    'yellow_lactate_flag',\n",
    "    # Green Flags\n",
    "    'green_resp_spo2_flag', 'green_resp_rate_flag', 'green_fio2_flag',\n",
    "    'green_peep_flag', 'green_map_flag', 'green_pulse_flag',\n",
    "    'green_lactate_flag', 'green_hr_flag'\n",
    "]\n",
    "\n",
    "green_flags_enhanced = [\n",
    "    'hourly_trach', 'paralytics_flag',\n",
    "    # Green Flags\n",
    "    'green_resp_spo2_flag', 'green_resp_rate_flag', 'green_fio2_flag',\n",
    "    'green_peep_flag', 'green_map_flag', 'green_pulse_flag',\n",
    "    'green_lactate_flag', 'green_hr_flag'\n",
    "]\n",
    "\n",
    "# Run enhanced analysis for each criteria\n",
    "print(\"\\n📊 Analyzing TEAM criteria failures...\")\n",
    "summary_team, primary_team, combo_team = analyse_criterion_enhanced(\n",
    "    final_df,\n",
    "    crit_name=\"TEAM\",\n",
    "    flag_cols=team_flags_enhanced,\n",
    "    master_flag=\"team_flag\",\n",
    "    site_name=site_name,\n",
    "    max_upset_combinations=40,\n",
    "    figure_width=14,\n",
    "    figure_height=8\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Analyzing Patel criteria failures...\")\n",
    "summary_patel, primary_patel, combo_patel = analyse_criterion_enhanced(\n",
    "    final_df,\n",
    "    crit_name=\"Patel\",\n",
    "    flag_cols=patel_flags_enhanced,\n",
    "    master_flag=\"patel_flag\",\n",
    "    site_name=site_name,\n",
    "    max_upset_combinations=40,\n",
    "    figure_width=14,\n",
    "    figure_height=8\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Analyzing Yellow criteria failures...\")\n",
    "summary_yellow, primary_yellow, combo_yellow = analyse_criterion_enhanced(\n",
    "    final_df,\n",
    "    crit_name=\"Yellow\",\n",
    "    flag_cols=yellow_flags_enhanced,\n",
    "    master_flag=\"any_yellow_or_green_no_red\",\n",
    "    site_name=site_name,\n",
    "    max_upset_combinations=30,  # Yellow has more flags, limit more strictly\n",
    "    figure_width=16,\n",
    "    figure_height=10\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Analyzing Green criteria failures...\")\n",
    "summary_green, primary_green, combo_green = analyse_criterion_enhanced(\n",
    "    final_df,\n",
    "    crit_name=\"Green\",\n",
    "    flag_cols=green_flags_enhanced,\n",
    "    master_flag=\"all_green_no_red\",\n",
    "    site_name=site_name,\n",
    "    max_upset_combinations=40,\n",
    "    figure_width=14,\n",
    "    figure_height=8\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✅ ENHANCED FAILURE ANALYSIS COMPLETE\")\n",
    "print(f\"📁 Individual analysis results saved to ../output/final/{{criteria_name}}/\")\n",
    "print(f\"📤 Multi-site sharing files: combination_patterns_for_multisite.csv\")\n",
    "print(f\"🎯 Site: {site_name}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Create summary table of all criteria\n",
    "# ────────────────────────────────────────────────────────────\n",
    "\n",
    "try:\n",
    "    summary_data = []\n",
    "    \n",
    "    for criteria, summary_df in [('TEAM', summary_team), ('Patel', summary_patel), \n",
    "                                ('Yellow', summary_yellow), ('Green', summary_green)]:\n",
    "        if summary_df is not None and len(summary_df) > 0:\n",
    "            summary_data.append({\n",
    "                'criteria': criteria,\n",
    "                'total_encounters': summary_df['total_encounters'].iloc[0],\n",
    "                'failed_encounters': summary_df['failed_encounters'].iloc[0],\n",
    "                'failure_rate': summary_df['failed_encounters'].iloc[0] / summary_df['total_encounters'].iloc[0],\n",
    "                'most_problematic_component': summary_df.loc[summary_df['encounter_failure_rate'].idxmax(), 'criterion'],\n",
    "                'highest_component_failure_rate': summary_df['encounter_failure_rate'].max(),\n",
    "                'site_name': site_name\n",
    "            })\n",
    "    \n",
    "    if summary_data:\n",
    "        overall_summary = pd.DataFrame(summary_data)\n",
    "        overall_summary.to_csv(\"../output/final/overall_failure_summary_by_criteria.csv\", index=False)\n",
    "        \n",
    "        print(f\"\\n📋 OVERALL SUMMARY\")\n",
    "        print(f\"-\" * 50)\n",
    "        for _, row in overall_summary.iterrows():\n",
    "            print(f\"{row['criteria']:8s}: {row['failed_encounters']:4.0f}/{row['total_encounters']:4.0f} \"\n",
    "                  f\"({row['failure_rate']*100:5.1f}%) failed | \"\n",
    "                  f\"Top issue: {row['most_problematic_component']}\")\n",
    "        \n",
    "        print(f\"\\n✅ Overall summary saved to ../output/final/overall_failure_summary_by_criteria.csv\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR creating overall summary: {e}\")\n",
    "\n",
    "print(f\"\\n🎉 ANALYSIS COMPLETE - Check output folders for detailed results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8f6d4",
   "metadata": {
    "papermill": {
     "duration": 0.056541,
     "end_time": "2025-05-05T20:37:15.482773",
     "exception": false,
     "start_time": "2025-05-05T20:37:15.426232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Average Hours Criteria Met on Days 1, 2, and 3\n",
    "\n",
    "Determine how many hours the criteria are met on specific calendar days (Day 1, Day 2, Day 3 after intubation).\n",
    "\n",
    "1. First, assign a calendar_day column that represents the calendar day relative to intubation.\n",
    "2. Use the recorded_date and recorded hour to calculate the difference from the intubation time, and categorize rows into Day 1, Day 2, Day 3.\n",
    "3. For each encounter, group the data by calendar_day and hospitalization_id and sum the hours that meet each criterion.\n",
    "4. Compute the average number of hours for each criterion per day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a477d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:37:15.701276Z",
     "iopub.status.busy": "2025-05-05T20:37:15.699062Z",
     "iopub.status.idle": "2025-05-05T20:37:16.399843Z",
     "shell.execute_reply": "2025-05-05T20:37:16.399545Z"
    },
    "papermill": {
     "duration": 0.830447,
     "end_time": "2025-05-05T20:37:16.400676",
     "exception": false,
     "start_time": "2025-05-05T20:37:15.570229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge final_df with vent_start_end to get 'vent_start_time'\n",
    "visualization_df = pd.merge(\n",
    "    final_df,\n",
    "    all_ids_w_outcome[['encounter_block', 'block_vent_start_dttm']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Ensure 'vent_start_time' and 'recorded_date' are in datetime format\n",
    "visualization_df['block_vent_start_dttm'] = pd.to_datetime(visualization_df['block_vent_start_dttm'])\n",
    "visualization_df['recorded_date'] = pd.to_datetime(visualization_df['recorded_date'])\n",
    "\n",
    "# Combine 'recorded_date' and 'recorded_hour' to create 'recorded_dttm'\n",
    "visualization_df['recorded_dttm'] = visualization_df['recorded_date'] + pd.to_timedelta(visualization_df['recorded_hour'], unit='h')\n",
    "\n",
    "# Verify the data types\n",
    "# print(\"Verify data types\\n\", visualization_df[['vent_start_time', 'recorded_dttm']].dtypes)\n",
    "\n",
    "# Remove timezone information from 'vent_start_time' if it's timezone-aware\n",
    "if visualization_df['block_vent_start_dttm'].dt.tz is not None:\n",
    "    visualization_df['block_vent_start_dttm'] = visualization_df['block_vent_start_dttm'].dt.tz_localize(None)\n",
    "\n",
    "# Similarly, remove timezone information from 'recorded_dttm' if needed\n",
    "if visualization_df['recorded_dttm'].dt.tz is not None:\n",
    "    visualization_df['recorded_dttm'] = visualization_df['recorded_dttm'].dt.tz_localize(None)\n",
    "\n",
    "# print(\"\\nConverted data type if not tz naive\\n\", visualization_df[['vent_start_time', 'recorded_dttm']].dtypes)\n",
    "\n",
    "def assign_calendar_day(df, intubation_col, recorded_col):\n",
    "    # Calculate the difference in days between intubation and recorded time\n",
    "    df['calendar_day'] = (df[recorded_col] - df[intubation_col]).dt.days + 1\n",
    "    return df\n",
    "\n",
    "# Assign calendar day for each encounter\n",
    "visualization_df = assign_calendar_day(visualization_df, 'block_vent_start_dttm', 'recorded_dttm')\n",
    "\n",
    "visualization_df = visualization_df[['encounter_block', 'block_vent_start_dttm', 'recorded_dttm', \n",
    "                  'calendar_day', 'patel_flag', 'team_flag', 'any_yellow_or_green_no_red', 'all_green', 'all_green_no_red',\n",
    "                  'any_green']]\n",
    "\n",
    "def compute_avg_hours_by_day(df, criteria_columns):\n",
    "    # Ensure hospitalization_id is handled as string/object and numeric columns as numbers\n",
    "    hours_per_day = df.groupby(['encounter_block', 'calendar_day']).agg({\n",
    "        'patel_flag': 'sum',\n",
    "        'team_flag': 'sum',\n",
    "        'any_yellow_or_green_no_red': 'sum',\n",
    "        'all_green': 'sum',\n",
    "    }).reset_index()\n",
    "    # Filter for Day 1, Day 2, Day 3\n",
    "    hours_per_day = hours_per_day[hours_per_day['calendar_day'].isin([1, 2, 3])]\n",
    "    \n",
    "    # Calculate the average number of hours for each day\n",
    "    avg_hours_by_day = hours_per_day.groupby('calendar_day').agg({\n",
    "        'patel_flag': 'mean',\n",
    "        'team_flag': 'mean',\n",
    "        'any_yellow_or_green_no_red': 'mean',\n",
    "        'all_green': 'mean',\n",
    "    }).reset_index()\n",
    "    \n",
    "    return avg_hours_by_day\n",
    "\n",
    "# Define your criteria columns\n",
    "criteria_columns = ['patel_flag', 'team_flag', 'any_yellow_or_green_no_red', 'all_green']\n",
    "# Calculate the average number of hours each criterion is met on Day 1, 2, and 3\n",
    "avg_hours_by_day = compute_avg_hours_by_day(visualization_df, criteria_columns)\n",
    "avg_hours_by_day['site_name'] = pyCLIF.helper[\"site_name\"]\n",
    "pd.DataFrame(avg_hours_by_day).to_csv(f'../output/final/avg_hours_by_day_{pyCLIF.helper[\"site_name\"]}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv',index=False)\n",
    "\n",
    "def plot_avg_hours_by_day_bar(avg_hours_by_day, criteria_columns):\n",
    "    # Melt the DataFrame for easier plotting with seaborn\n",
    "    melted_df = avg_hours_by_day.melt(id_vars='calendar_day', value_vars=criteria_columns, var_name='Criteria', value_name='Average Hours Met')\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create a bar plot\n",
    "    sns.barplot(x='calendar_day', y='Average Hours Met', hue='Criteria', data=melted_df, palette='viridis')\n",
    "    \n",
    "    # Add custom x-axis labels for Day 1, Day 2, Day 3\n",
    "    plt.xticks(ticks=[0, 1, 2], labels=[\"Day 1\", \"Day 2\", \"Day 3\"])\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.title('Average Hours Criteria Met per Day')\n",
    "    plt.xlabel('Calendar Day')\n",
    "    plt.ylabel('Average Hours Criteria Met')\n",
    "    \n",
    "    # Move the legend to the bottom\n",
    "    plt.legend(title='Criteria', loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    # Save the plot\n",
    "    plt.savefig(f'../output/final/graphs/avg_hours_by_day_{pyCLIF.helper[\"site_name\"]}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Plot the average hours by day using a bar plot\n",
    "plot_avg_hours_by_day_bar(avg_hours_by_day, criteria_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa9f7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:37:16.515265Z",
     "iopub.status.busy": "2025-05-05T20:37:16.515093Z",
     "iopub.status.idle": "2025-05-05T20:37:16.758084Z",
     "shell.execute_reply": "2025-05-05T20:37:16.757450Z"
    },
    "papermill": {
     "duration": 0.301423,
     "end_time": "2025-05-05T20:37:16.759290",
     "exception": false,
     "start_time": "2025-05-05T20:37:16.457867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge final_df with vent_start_end to get 'vent_start_time'\n",
    "visualization_df = pd.merge(\n",
    "    df_72h,\n",
    "    all_ids_w_outcome[['encounter_block', 'block_vent_start_dttm']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Ensure 'vent_start_time' and 'recorded_date' are in datetime format\n",
    "visualization_df['block_vent_start_dttm'] = pd.to_datetime(visualization_df['block_vent_start_dttm'])\n",
    "visualization_df['recorded_date'] = pd.to_datetime(visualization_df['recorded_date'])\n",
    "\n",
    "# Combine 'recorded_date' and 'recorded_hour' to create 'recorded_dttm'\n",
    "visualization_df['recorded_dttm'] = visualization_df['recorded_date'] + pd.to_timedelta(visualization_df['recorded_hour'], unit='h')\n",
    "\n",
    "# Verify the data types\n",
    "# print(\"Verify data types\\n\", visualization_df[['vent_start_time', 'recorded_dttm']].dtypes)\n",
    "\n",
    "# Remove timezone information from 'vent_start_time' if it's timezone-aware\n",
    "if visualization_df['block_vent_start_dttm'].dt.tz is not None:\n",
    "    visualization_df['block_vent_start_dttm'] = visualization_df['block_vent_start_dttm'].dt.tz_localize(None)\n",
    "\n",
    "# Similarly, remove timezone information from 'recorded_dttm' if needed\n",
    "if visualization_df['recorded_dttm'].dt.tz is not None:\n",
    "    visualization_df['recorded_dttm'] = visualization_df['recorded_dttm'].dt.tz_localize(None)\n",
    "\n",
    "# print(\"\\nConverted data type if not tz naive\\n\", visualization_df[['vent_start_time', 'recorded_dttm']].dtypes)\n",
    "\n",
    "def assign_calendar_day(df, intubation_col, recorded_col):\n",
    "    # Calculate the difference in days between intubation and recorded time\n",
    "    df['calendar_day'] = (df[recorded_col] - df[intubation_col]).dt.days + 1\n",
    "    return df\n",
    "\n",
    "# Assign calendar day for each encounter\n",
    "visualization_df = assign_calendar_day(visualization_df, 'block_vent_start_dttm', 'recorded_dttm')\n",
    "\n",
    "visualization_df = visualization_df[['encounter_block', 'block_vent_start_dttm', 'recorded_dttm', \n",
    "                  'calendar_day', 'patel_flag', 'team_flag', 'any_yellow_or_green_no_red', 'all_green', 'all_green_no_red',\n",
    "                  'any_green']]\n",
    "\n",
    "def compute_avg_hours_by_day(df, criteria_columns):\n",
    "    # Ensure hospitalization_id is handled as string/object and numeric columns as numbers\n",
    "    hours_per_day = df.groupby(['encounter_block', 'calendar_day']).agg({\n",
    "        'patel_flag': 'sum',\n",
    "        'team_flag': 'sum',\n",
    "        'any_yellow_or_green_no_red': 'sum',\n",
    "        'all_green': 'sum',\n",
    "    }).reset_index()\n",
    "    # Filter for Day 1, Day 2, Day 3\n",
    "    hours_per_day = hours_per_day[hours_per_day['calendar_day'].isin([1, 2, 3])]\n",
    "    \n",
    "    # Calculate the average number of hours for each day\n",
    "    avg_hours_by_day = hours_per_day.groupby('calendar_day').agg({\n",
    "        'patel_flag': 'mean',\n",
    "        'team_flag': 'mean',\n",
    "        'any_yellow_or_green_no_red': 'mean',\n",
    "        'all_green': 'mean',\n",
    "    }).reset_index()\n",
    "    \n",
    "    return avg_hours_by_day\n",
    "\n",
    "# Define your criteria columns\n",
    "criteria_columns = ['patel_flag', 'team_flag', 'any_yellow_or_green_no_red', 'all_green']\n",
    "# Calculate the average number of hours each criterion is met on Day 1, 2, and 3\n",
    "avg_hours_by_day = compute_avg_hours_by_day(visualization_df, criteria_columns)\n",
    "avg_hours_by_day['site_name'] = pyCLIF.helper[\"site_name\"]\n",
    "pd.DataFrame(avg_hours_by_day).to_csv(f'../output/final/avg_hours_by_day_72h_{pyCLIF.helper[\"site_name\"]}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv',index=False)\n",
    "\n",
    "def plot_avg_hours_by_day_bar(avg_hours_by_day, criteria_columns):\n",
    "    # Melt the DataFrame for easier plotting with seaborn\n",
    "    melted_df = avg_hours_by_day.melt(id_vars='calendar_day', value_vars=criteria_columns, var_name='Criteria', value_name='Average Hours Met')\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create a bar plot\n",
    "    sns.barplot(x='calendar_day', y='Average Hours Met', hue='Criteria', data=melted_df, palette='viridis')\n",
    "    \n",
    "    # Add custom x-axis labels for Day 1, Day 2, Day 3\n",
    "    plt.xticks(ticks=[0, 1, 2], labels=[\"Day 1\", \"Day 2\", \"Day 3\"])\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.title('Average Hours Criteria Met per Day')\n",
    "    plt.xlabel('Calendar Day')\n",
    "    plt.ylabel('Average Hours Criteria Met')\n",
    "    \n",
    "    # Move the legend to the bottom\n",
    "    plt.legend(title='Criteria', loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    # Save the plot\n",
    "    plt.savefig(f'../output/final/graphs/avg_hours_by_day_72h_{pyCLIF.helper[\"site_name\"]}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Plot the average hours by day using a bar plot\n",
    "plot_avg_hours_by_day_bar(avg_hours_by_day, criteria_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e14e32",
   "metadata": {
    "papermill": {
     "duration": 0.064638,
     "end_time": "2025-05-05T20:37:16.904678",
     "exception": false,
     "start_time": "2025-05-05T20:37:16.840040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Parallel categories plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc89cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:37:17.020495Z",
     "iopub.status.busy": "2025-05-05T20:37:17.020288Z",
     "iopub.status.idle": "2025-05-05T20:37:19.945729Z",
     "shell.execute_reply": "2025-05-05T20:37:19.945318Z"
    },
    "papermill": {
     "duration": 2.98513,
     "end_time": "2025-05-05T20:37:19.946835",
     "exception": false,
     "start_time": "2025-05-05T20:37:16.961705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Useful for EDA \n",
    "# Create a DataFrame for parallel categories plot\n",
    "parallel_df = final_df[['patel_flag', 'team_flag', 'any_yellow_or_green_no_red', 'all_green']].copy()\n",
    "parallel_df['patel_flag'] = parallel_df['patel_flag'].apply(lambda x: 1 if x else 0)\n",
    "parallel_df['team_flag'] = parallel_df['team_flag'].apply(lambda x: 1 if x else 0)\n",
    "parallel_df['any_yellow_or_green_no_red'] = parallel_df['any_yellow_or_green_no_red'].apply(lambda x: 1 if x else 0)\n",
    "parallel_df['all_green'] = parallel_df['all_green'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "# Create parallel categories plot\n",
    "fig = px.parallel_categories(parallel_df, dimensions=['patel_flag', 'team_flag', 'any_yellow_or_green_no_red', 'all_green'],\n",
    "                             color=\"patel_flag\",\n",
    "                             labels={'patel_flag': 'Patel Met', 'team_flag': 'TEAM Met', 'any_yellow_or_green_no_red': 'Yellow Flag', 'all_green': 'Green Flag'},\n",
    "                             color_continuous_scale=px.colors.sequential.Inferno)\n",
    "\n",
    "fig.update_layout(title=\"Parallel Categories Plot: Comparison of Criteria Satisfaction\")\n",
    "\n",
    "# Save the final figure\n",
    "fig.write_image(f'../output/final/graphs/parallel_categories_{pyCLIF.helper[\"site_name\"]}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7558437d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:37:20.295865Z",
     "iopub.status.busy": "2025-05-05T20:37:20.295019Z",
     "iopub.status.idle": "2025-05-05T20:37:20.396770Z",
     "shell.execute_reply": "2025-05-05T20:37:20.396321Z"
    },
    "papermill": {
     "duration": 0.280801,
     "end_time": "2025-05-05T20:37:20.397748",
     "exception": false,
     "start_time": "2025-05-05T20:37:20.116947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# look at encounters when Patel flag is not met but team flag is met\n",
    "## sanity check\n",
    "patel_fail_team_pass = final_df[(final_df['patel_flag'] == 0) & (final_df['team_flag'] == 1)]\n",
    "# Verify the filter\n",
    "print(f\"\\nTotal number of hours where Patel failed and Team passed: {len(patel_fail_team_pass)}\\n\")\n",
    "\n",
    "if len(patel_fail_team_pass) > 0:\n",
    "    # Dictionary to store our failure counts\n",
    "    print(\"Primary cause of Patel Criteria non-compliance\")\n",
    "    failure_counts = {\n",
    "            'MAP': sum(patel_fail_team_pass['patel_map_flag'] == 0),\n",
    "            'SBP': sum(patel_fail_team_pass['patel_sbp_flag'] == 0),\n",
    "            'Pulse': sum(patel_fail_team_pass['patel_pulse_flag'] == 0),\n",
    "            'Respiratory Rate': sum(patel_fail_team_pass['patel_resp_rate_flag'] == 0),\n",
    "            'SpO2': sum(patel_fail_team_pass['patel_spo2_flag'] == 0)\n",
    "        }\n",
    "    failure_df = pd.DataFrame(list(failure_counts.items()),columns = ['Criteria','Count'])\n",
    "    failure_df.to_csv(f'../output/final/patel_fail_team_pass_subcomponents_{pyCLIF.helper[\"site_name\"]}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv',index=False)\n",
    "    print(failure_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8312bc",
   "metadata": {
    "papermill": {
     "duration": 0.177862,
     "end_time": "2025-05-05T20:37:20.793285",
     "exception": false,
     "start_time": "2025-05-05T20:37:20.615423",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Yellow-Green spectrum criteria distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca6640",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:37:21.318270Z",
     "iopub.status.busy": "2025-05-05T20:37:21.317719Z",
     "iopub.status.idle": "2025-05-05T20:37:21.937724Z",
     "shell.execute_reply": "2025-05-05T20:37:21.937253Z"
    },
    "papermill": {
     "duration": 0.812387,
     "end_time": "2025-05-05T20:37:21.938873",
     "exception": false,
     "start_time": "2025-05-05T20:37:21.126486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# which sub‑criteria are green / yellow\n",
    "green_cols  = [c for c in final_df.columns if c.startswith(\"green_\") and c.endswith(\"_flag\")]\n",
    "yellow_cols = [c for c in final_df.columns if c.startswith(\"yellow_\") and c.endswith(\"_flag\")]\n",
    "\n",
    "# one row per block at the moment it first became eligible\n",
    "first_hit = (\n",
    "    final_df.loc[final_df[\"any_yellow_or_green_no_red\"] == 1]\n",
    "             .sort_values([\"encounter_block\", \"time_from_vent\"])\n",
    "             .groupby(\"encounter_block\")\n",
    "             .first()\n",
    ")\n",
    "\n",
    "# count how many green / yellow sub‑criteria were satisfied at that hour\n",
    "first_hit[\"n_green\"]  = first_hit[green_cols].sum(axis=1)\n",
    "first_hit[\"n_yellow\"] = first_hit[yellow_cols].sum(axis=1)\n",
    "\n",
    "# yellow‑fraction: 0 = all satisfied criteria were green, 1 = all yellow\n",
    "first_hit[\"yellow_frac\"] = (\n",
    "    first_hit[\"n_yellow\"] /\n",
    "    (first_hit[\"n_green\"] + first_hit[\"n_yellow\"])\n",
    ").fillna(0)              # guard against division by zero\n",
    "\n",
    "# yellow‑fraction: 0 = all satisfied criteria were green, 1 = all yellow\n",
    "first_hit[\"green_frac\"] = (\n",
    "    first_hit[\"n_green\"] /\n",
    "    (first_hit[\"n_green\"] + first_hit[\"n_yellow\"])\n",
    ").fillna(0)              # guard against division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2932ad8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:37:22.283936Z",
     "iopub.status.busy": "2025-05-05T20:37:22.283744Z",
     "iopub.status.idle": "2025-05-05T20:37:22.875162Z",
     "shell.execute_reply": "2025-05-05T20:37:22.874659Z"
    },
    "papermill": {
     "duration": 0.785929,
     "end_time": "2025-05-05T20:37:22.876437",
     "exception": false,
     "start_time": "2025-05-05T20:37:22.090508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  Build the jittered scatter data  (unchanged)\n",
    "# ------------------------------------------------------------\n",
    "x = np.random.normal(0, 0.002, size=len(first_hit))   # tiny horizontal jitter\n",
    "y = first_hit[\"green_frac\"].values                    # 1 = pure green, 0 = pure yellow\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  Custom colormap: pure‑green  →  pure‑yellow\n",
    "# ------------------------------------------------------------\n",
    "green_yellow = mcolors.LinearSegmentedColormap.from_list(\n",
    "    \"YellowGreen\", [\"#ffeb3b\", \"#2ca02c\"]   #   0 (yellow)   →   1 (green)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  Plot\n",
    "# ------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "sc = ax.scatter(x, y,\n",
    "                s=14, alpha=0.7,\n",
    "                c=y, cmap=green_yellow, vmin=0, vmax=1)\n",
    "\n",
    "ax.set_xlim(-0.02, 0.02)\n",
    "ax.set_xticks([])\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_ylabel(\"Green fraction  (1=pure green  |  0=pure yellow)\")\n",
    "ax.set_title(\"Eligibility colour spectrum per encounter block\", pad=12)\n",
    "\n",
    "cbar = fig.colorbar(sc, ax=ax, pad=0.02, shrink=0.8)\n",
    "cbar.set_label(\"Green fraction\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4.  Caption (automatic wrap)\n",
    "# ------------------------------------------------------------\n",
    "caption = (\n",
    "    \"Each dot = first eligible hour of an encounter block. \"\n",
    "    \"Vertical position/colour show the fraction of satisfied criteria that were \"\n",
    "    \"GREEN (physiologically safer) versus YELLOW (less conservative).\" \n",
    "    \"Horizontal spread is tiny random jitter to avoid over plotting; x -axis has no meaning \"\n",
    ")\n",
    "fig.text(0.01, -0.10, caption, ha=\"left\", va=\"top\", wrap=True, fontsize=9)\n",
    "fig.savefig(f'../output/final/graphs/yellow_eligibility_colour_spectrum_{pyCLIF.helper[\"site_name\"]}.png') \n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f0b759",
   "metadata": {
    "papermill": {
     "duration": 0.3196,
     "end_time": "2025-05-05T20:37:23.438518",
     "exception": false,
     "start_time": "2025-05-05T20:37:23.118918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sensitivity analysis: Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29985222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekday Sensitivity Analysis Comparison\n",
    "print(\"=== WEEKDAY SENSITIVITY ANALYSIS ===\")\n",
    "\n",
    "BUSINESS_FLAGS = {\n",
    "    'Patel': 'patel_flag',\n",
    "    'TEAM': 'team_flag', \n",
    "    'Yellow': 'any_yellow_or_green_no_red',\n",
    "    'Green': 'all_green'\n",
    "}\n",
    "\n",
    "WEEKDAY_FLAGS = {\n",
    "    'Patel': 'patel_flag_weekday',\n",
    "    'TEAM': 'team_flag_weekday', \n",
    "    'Yellow': 'any_yellow_or_green_no_red_weekday',\n",
    "    'Green': 'all_green_weekday'\n",
    "}\n",
    "\n",
    "# Compare eligible hours: All-day vs Weekday-only\n",
    "comparison_rows = []\n",
    "for crit in BUSINESS_FLAGS.keys():\n",
    "    all_day_flag = BUSINESS_FLAGS[crit]\n",
    "    weekday_flag = WEEKDAY_FLAGS[crit]\n",
    "    \n",
    "    # All-day business hours\n",
    "    all_day_eligible = len(final_df[\n",
    "        (final_df[all_day_flag] == 1) & \n",
    "        (final_df['recorded_hour'].isin(range(8, 17)))\n",
    "    ])\n",
    "    \n",
    "    # Weekday-only business hours  \n",
    "    weekday_eligible = len(final_df[\n",
    "        (final_df[weekday_flag] == 1) & \n",
    "        (final_df['recorded_hour'].isin(range(8, 17))) &\n",
    "        (final_df['is_weekday'] == True)\n",
    "    ])\n",
    "    \n",
    "    comparison_rows.append({\n",
    "        'Criteria': crit,\n",
    "        'AllDay_Eligible_Hours': all_day_eligible,\n",
    "        'Weekday_Eligible_Hours': weekday_eligible,\n",
    "        'Difference': weekday_eligible - all_day_eligible,\n",
    "        'Percent_Change': (weekday_eligible - all_day_eligible) / all_day_eligible * 100\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows)\n",
    "comparison_df.to_csv(f'../output/final/weekday_sensitivity_hours_{pyCLIF.helper[\"site_name\"]}.csv', index=False)\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3920e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mobilization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 91.02745,
   "end_time": "2025-05-05T20:37:24.850528",
   "environment_variables": {},
   "exception": null,
   "input_path": "02_mobilization_analysis.ipynb",
   "output_path": "02_mobilization_analysis.ipynb",
   "parameters": {},
   "start_time": "2025-05-05T20:35:53.823078",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
