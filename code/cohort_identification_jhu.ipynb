{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eligibility for mobilization: Cohort ID and Discretizing script\n",
    "\n",
    "This script identifies the cohort using CLIF 2.0 tables and discretizes the dataset at an hourly level\n",
    "\n",
    " \n",
    "                        ðŸš¨Code will break if the following requirements are not satisfiedðŸš¨  \n",
    "#### Requirements:\n",
    "* Required table filenames should be `clif_patient`, `clif_hospitalization`, `clif_adt`, `clif_vitals`, `clif_labs`, `clif_medication_admin_continuous`, `clif_respiratory_support`\n",
    "* Within each table, the following variables and categories are required.\n",
    "\n",
    "| Table Name | Required Variables | Required Categories |\n",
    "| --- | --- | --- |\n",
    "| **patient** | `patient_id`, `race_category`, `ethnicity_category`, `sex_category` | - |\n",
    "| **hospitalization** | `patient_id`, `hospitalization_id`, `admission_dttm`, `discharge_dttm`, `age_at_admission` | - |\n",
    "| **vitals** | `hospitalization_id`, `recorded_dttm`, `vital_category`, `vital_value` | 'heart_rate', 'resp_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm' |\n",
    "| **labs** | `hospitalization_id`, `lab_result_dttm`, `lab_category`, `lab_value` | 'lactate' |\n",
    "| **medication_admin_continuous** | `hospitalization_id`, `admin_dttm`, `med_name`, `med_category`, `med_dose`, `med_dose_unit` | \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \"vasopressin\", \"dopamine\", \"angiotensin\"(optional), \"nicardipine\", \"nitroprusside\", \"clevidipine\", \"cisatracurium\", \"vecuronium\" |\n",
    "| **respiratory_support** | `hospitalization_id`, `recorded_dttm`, `device_category`, `mode_category`, `tracheostomy`, `fio2_set`, `lpm_set`, `resp_rate_set`, `peep_set`, `resp_rate_obs` | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idies/miniconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from config.json\n",
      "{'site_name': 'Hopkins', 'tables_path': '/home/idies/workspace/Storage/chochbe1/JH_CCRD/CLIF/rclif/', 'file_type': 'parquet'}\n"
     ]
    }
   ],
   "source": [
    "#! pip install pandas numpy duckdb seaborn matplotlib plotly\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyCLIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required columns and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'device_name',\n",
    "    'device_category',\n",
    "    'mode_name', \n",
    "    'mode_category',\n",
    "    'tracheostomy',\n",
    "    'fio2_set',\n",
    "    'lpm_set',\n",
    "    'resp_rate_set',\n",
    "    'peep_set',\n",
    "    'resp_rate_obs',\n",
    "    'tidal_volume_set'\n",
    "]\n",
    "\n",
    "vitals_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'vital_category',\n",
    "    'vital_value'\n",
    "]\n",
    "vitals_of_interest = ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
    "\n",
    "labs_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'lab_result_dttm',\n",
    "    'lab_category',\n",
    "    'lab_value',\n",
    "    'lab_value_numeric'\n",
    "]\n",
    "labs_of_interest = ['lactate']\n",
    "\n",
    "meds_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'admin_dttm',\n",
    "    'med_name',\n",
    "    'med_category',\n",
    "    'med_dose',\n",
    "    'med_dose_unit'\n",
    "]\n",
    "meds_of_interest = [\n",
    "    'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin', 'nicardipine', 'nitroprusside',\n",
    "    'clevidipine', 'cisatracurium', 'vecuronium'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from /home/idies/workspace/Storage/chochbe1/JH_CCRD/CLIF/rclif/clif_patient.parquet\n",
      "Data loaded successfully from /home/idies/workspace/Storage/chochbe1/JH_CCRD/CLIF/rclif/clif_hospitalization.parquet\n"
     ]
    }
   ],
   "source": [
    "patient = pyCLIF.load_data('clif_patient')\n",
    "hospitalization = pyCLIF.load_data('clif_hospitalization')\n",
    "hospitalization['hospitalization_id']= hospitalization['hospitalization_id'].astype(str)\n",
    "patient['patient_id']= patient['patient_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize all _dttm variables to the same format\n",
    "patient = pyCLIF.standardize_datetime(patient)\n",
    "hospitalization = pyCLIF.standardize_datetime(hospitalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: patient\n",
      "Found 2 duplicate rows based on columns: ['patient_id']\n",
      "Dropped 1 duplicate rows. New DataFrame has 135170 rows.\n",
      "Processing DataFrame: hospitalization\n",
      "No duplicates found based on columns: ['hospitalization_id'].\n"
     ]
    }
   ],
   "source": [
    "patient = pyCLIF.remove_duplicates(patient, ['patient_id'], 'patient')\n",
    "hospitalization = pyCLIF.remove_duplicates(hospitalization, ['hospitalization_id'], 'hospitalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of unique encounters in the data: 454476\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Number of unique encounters in the data: {pyCLIF.count_unique_encounters(hospitalization, 'hospitalization_id')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inclusion Criteria:\n",
    "\n",
    "* Filter Admissions for 2020-03-01 to 2022-03-31\n",
    "* Encounters receiving invasive mechanical ventilation during this period\n",
    "* A cool off period of 4 hours after first intubation for analysis\n",
    "\n",
    "### Exclusion criteria:\n",
    "\n",
    "1. Encounters that were on vent for less than 2 hours\n",
    "2. Encounters that were on trach in the first 72 hours \n",
    "3. Encounters that received Cisatracurium or Vecuronium for 4 hours or more within the first 72 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique encounters after filtering by date and age: 126598\n"
     ]
    }
   ],
   "source": [
    "cohort = hospitalization[\n",
    "    (hospitalization['admission_dttm'] >= '2020-03-01') &\n",
    "    (hospitalization['admission_dttm'] <= '2022-03-31') &\n",
    "    (hospitalization['age_at_admission'] >= 18)\n",
    "].reset_index(drop=True)[['hospitalization_id']].drop_duplicates()\n",
    "\n",
    "cohort_ids = cohort['hospitalization_id'].unique().tolist()\n",
    "print(f\"Number of unique encounters after filtering by date and age:\", cohort['hospitalization_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bb2494551840c98f1532cacb13f626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from /home/idies/workspace/Storage/chochbe1/JH_CCRD/CLIF/rclif/clif_respiratory_support.parquet\n"
     ]
    }
   ],
   "source": [
    "# Import clif respiratory table for this cohort\n",
    "rst_filters = {\n",
    "    'hospitalization_id': cohort_ids\n",
    "}\n",
    "resp_support_raw = pyCLIF.load_data('clif_respiratory_support', columns=rst_required_columns, filters=rst_filters)\n",
    "resp_support_raw['hospitalization_id']= resp_support_raw['hospitalization_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_support = resp_support_raw.copy()\n",
    "resp_support['recorded_dttm'] = pd.to_datetime(resp_support['recorded_dttm'])\n",
    "resp_support['device_category'] = resp_support['device_category'].str.lower()\n",
    "resp_support['mode_category'] = resp_support['mode_category'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating waterfall processing...\n",
      "Creating recorded_date and recorded_hour...\n",
      "Sorting data by 'hospitalization_id' and 'recorded_dttm'...\n",
      "Fixing missing 'device_category' and 'device_name' based on 'mode_category'...\n",
      "Fixing 'device_category' and 'device_name' based on neighboring records...\n",
      "Handling duplicates and removing rows with all key variables missing...\n",
      "Filling forward 'device_category' within each hospitalization...\n",
      "Creating 'device_cat_id' to track changes in 'device_category'...\n",
      "Filling 'device_name' within each 'device_cat_id'...\n",
      "Creating 'device_id' to track changes in 'device_name'...\n",
      "Filling 'mode_category' within each 'device_id'...\n",
      "Creating 'mode_cat_id' to track changes in 'mode_category'...\n",
      "Filling 'mode_name' within each 'mode_cat_id'...\n",
      "Creating 'mode_name_id' to track changes in 'mode_name'...\n",
      "Adjusting 'fio2_set' for 'room air' device_category...\n",
      "Adjusting 'mode_category' for 't-piece' devices...\n",
      "Filling remaining variables within each 'mode_name_id'...\n",
      "Filling 'tracheostomy' forward within each hospitalization...\n",
      "Removing duplicates...\n",
      "Waterfall processing completed.\n"
     ]
    }
   ],
   "source": [
    "# Apply Nick's Waterfall fill logic for respiratory support table\n",
    "# This can take time: 2 - 12 mins depending on data size\n",
    "processed_resp_support = pyCLIF.process_resp_support(resp_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIO2_SET MEAN 0.4654953087272558\n",
      "Number of unique encounters after filtering for ventilator usage: 10403\n"
     ]
    }
   ],
   "source": [
    "# Identify the cohort on invasive mechanical ventilation \n",
    "columns_to_keep = [\n",
    "    'hospitalization_id', 'recorded_dttm', 'device_name','device_category',\n",
    "    'mode_name', 'mode_category' , 'tracheostomy',\n",
    "    'fio2_set', 'lpm_set', 'peep_set', \n",
    "    'resp_rate_obs', 'resp_rate_set'\n",
    "]\n",
    "\n",
    "ventilator_usage = processed_resp_support[processed_resp_support['device_category'].str.contains(\"imv\", case=False, na=False)]\n",
    "cohort_on_vent = ventilator_usage.merge(cohort, on='hospitalization_id', how='left')\n",
    "cohort_on_vent = cohort_on_vent[columns_to_keep]\n",
    "cohort_on_vent['on_vent'] = cohort_on_vent['device_category'].str.contains(\"imv\", case=False, na=False).astype(int)\n",
    "cohort_on_vent.loc[:, 'recorded_dttm'] = pd.to_datetime(cohort_on_vent['recorded_dttm'])\n",
    "cohort_on_vent = cohort_on_vent.sort_values(by=['hospitalization_id', 'recorded_dttm'])\n",
    "cohort_on_vent = cohort_on_vent[cohort_on_vent['on_vent'] == 1]\n",
    "\n",
    "\n",
    "# Apply thresholds and replace values outside these with NaN using .loc[]\n",
    "# UPDATE THIS TO USE CSV / JSON FROM OUTLIER DIRECTORY\n",
    "# cohort_on_vent.loc[:, 'fio2_set'] = cohort_on_vent['fio2_set'].where(cohort_on_vent['fio2_set'].between(0.21, 1, inclusive='both'), np.nan)\n",
    "# Calculate the mean of 'fio2_set', excluding NaN values\n",
    "fio2_mean = cohort_on_vent['fio2_set'].mean(skipna=True)\n",
    "print(\"FIO2_SET MEAN\", fio2_mean)\n",
    "# If the mean is greater than 1, divide 'fio2_set' by 100\n",
    "if fio2_mean > 1:\n",
    "    # Only divide values greater than 1 to avoid re-dividing already correct values\n",
    "    print(\"Updated fio2_set to be between 0.21 and 1\")\n",
    "    cohort_on_vent.loc[cohort_on_vent['fio2_set'] > 1, 'fio2_set'] = \\\n",
    "        cohort_on_vent.loc[cohort_on_vent['fio2_set'] > 1, 'fio2_set'] / 100\n",
    "\n",
    "cohort_on_vent.loc[:, 'fio2_set'] = cohort_on_vent['fio2_set'].where(cohort_on_vent['fio2_set'].between(0.21, 1, inclusive='both'), np.nan)\n",
    "cohort_on_vent.loc[:, 'resp_rate_set'] = cohort_on_vent['resp_rate_set'].where(cohort_on_vent['resp_rate_set'].between(0, 60, inclusive='both'), np.nan)\n",
    "cohort_on_vent.loc[:, 'peep_set'] = cohort_on_vent['peep_set'].where(cohort_on_vent['peep_set'].between(0, 50, inclusive='both'), np.nan)\n",
    "cohort_on_vent.loc[:, 'resp_rate_obs'] = cohort_on_vent['resp_rate_obs'].where(cohort_on_vent['resp_rate_obs'].between(0, 100, inclusive='both'), np.nan)\n",
    "cohort_on_vent.loc[:, 'lpm_set'] = cohort_on_vent['lpm_set'].where(cohort_on_vent['lpm_set'].between(0, 60, inclusive='both'), np.nan)\n",
    "\n",
    "cohort_on_vent['recorded_date'] = cohort_on_vent['recorded_dttm'].dt.date\n",
    "cohort_on_vent['recorded_hour'] = cohort_on_vent['recorded_dttm'].dt.hour\n",
    "\n",
    "print(f\"Number of unique encounters after filtering for ventilator usage: {cohort_on_vent['hospitalization_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique encounters after filtering for ventilator usage: 10088\n"
     ]
    }
   ],
   "source": [
    "vent_start_end = cohort_on_vent.groupby('hospitalization_id').agg(\n",
    "    vent_start_time=('recorded_dttm', 'min'),\n",
    "    vent_end_time=('recorded_dttm', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# Exclude encounters where vent start time and end time is the same \n",
    "vent_start_end = vent_start_end[vent_start_end['vent_start_time'] != vent_start_end['vent_end_time']]\n",
    "print(f\"Number of unique encounters after filtering for ventilator usage: {vent_start_end['hospitalization_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f412c81b8c5d4b398717b44e7ca155ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from /home/idies/workspace/Storage/chochbe1/JH_CCRD/CLIF/rclif/clif_vitals.parquet\n"
     ]
    }
   ],
   "source": [
    "# import required vitals\n",
    "vitals_filters = {\n",
    "    'hospitalization_id': cohort_ids,\n",
    "    'vital_category': vitals_of_interest\n",
    "}\n",
    "vitals = pyCLIF.load_data('clif_vitals', columns=vitals_required_columns, filters=vitals_filters)\n",
    "vitals['hospitalization_id']= vitals['hospitalization_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vital_category\n",
       "sbp                 9597959\n",
       "dbp                 9597867\n",
       "heart_rate          9476001\n",
       "map                 9265964\n",
       "spo2                9186579\n",
       "respiratory_rate    8307875\n",
       "weight_kg            471225\n",
       "height_cm            157121\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vitals.value_counts('vital_category')\n",
    "## if you don't have MAP, we can calculate here- TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique encounters in vitals 126296\n"
     ]
    }
   ],
   "source": [
    "# Get first_vital_dttm and last_vital_dttm for each hospitalization_id \n",
    "# We use this as proxy for admission and discharge dttm to construct hourly sequence for each hospitalization\n",
    "vital_dttm_bounds = vitals.groupby('hospitalization_id')['recorded_dttm'].agg(['min', 'max']).reset_index()\n",
    "vital_dttm_bounds.columns = ['hospitalization_id', 'first_vital_dttm', 'last_vital_dttm']\n",
    "print(\"unique encounters in vitals\", pyCLIF.count_unique_encounters(vital_dttm_bounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get height , weight and bmi for \n",
    "# Filter vitals to include only height and weight\n",
    "vitals_bmi = vitals[vitals['vital_category'].isin(['weight_kg', 'height_cm'])].copy()\n",
    "\n",
    "# Remove outliers\n",
    "vitals_bmi = vitals_bmi[\n",
    "    ((vitals_bmi['vital_category'] == 'height_cm') & (vitals_bmi['vital_value'] >= 76.2) & (vitals_bmi['vital_value'] <= 244)) |\n",
    "    ((vitals_bmi['vital_category'] == 'weight_kg') & (vitals_bmi['vital_value'] >= 20) & (vitals_bmi['vital_value'] <= 1100))\n",
    "]\n",
    "\n",
    "# Merge with vent_start_end to get ventilation start time\n",
    "vitals_bmi = vitals_bmi.merge(vent_start_end[['hospitalization_id', 'vent_start_time']], on='hospitalization_id', how='left')\n",
    "\n",
    "# Calculate time difference between recorded_dttm and vent_start_time\n",
    "vitals_bmi['recorded_dttm'] = pd.to_datetime(vitals_bmi['recorded_dttm'])\n",
    "vitals_bmi['vent_start_time'] = pd.to_datetime(vitals_bmi['vent_start_time'])\n",
    "vitals_bmi['time_diff'] = (vitals_bmi['recorded_dttm'] - vitals_bmi['vent_start_time']).dt.total_seconds() / 3600  # in hours\n",
    "\n",
    "# Define whether measurement is before or after vent_start_time\n",
    "vitals_bmi['before_vent_start'] = (vitals_bmi['time_diff'] <= 0).astype(int)\n",
    "\n",
    "# Calculate absolute time difference\n",
    "vitals_bmi['abs_time_diff'] = vitals_bmi['time_diff'].abs()\n",
    "\n",
    "# Sort data to prioritize measurements before vent start and closest in time\n",
    "vitals_bmi = vitals_bmi.sort_values(['hospitalization_id', 'vital_category', 'before_vent_start', 'abs_time_diff'], ascending=[True, True, False, True])\n",
    "\n",
    "# Drop duplicates to keep the closest measurement for each vital_category per hospitalization_id\n",
    "vitals_bmi = vitals_bmi.drop_duplicates(subset=['hospitalization_id', 'vital_category'], keep='first')\n",
    "\n",
    "# Pivot to get height and weight in columns\n",
    "vitals_bmi_pivot = vitals_bmi.pivot(index='hospitalization_id', columns='vital_category', values='vital_value').reset_index()\n",
    "\n",
    "# Calculate BMI\n",
    "vitals_bmi_pivot['bmi'] = vitals_bmi_pivot['weight_kg'] / ((vitals_bmi_pivot['height_cm'] / 100) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly sequence for the cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique encounters in resp filtered 10088\n"
     ]
    }
   ],
   "source": [
    "final_cohort = vent_start_end.merge(vital_dttm_bounds, on='hospitalization_id', how='inner')\n",
    "print(\"unique encounters in resp filtered\", pyCLIF.count_unique_encounters(final_cohort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases where last vital dttm is before vent_start time: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hospitalization_id</th>\n",
       "      <th>vent_start_time</th>\n",
       "      <th>vent_end_time</th>\n",
       "      <th>first_vital_dttm</th>\n",
       "      <th>last_vital_dttm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>1567544868278289.0</td>\n",
       "      <td>2021-01-03 10:41:00-05:00</td>\n",
       "      <td>2021-01-03 12:50:00-05:00</td>\n",
       "      <td>2021-01-03 09:10:00-05:00</td>\n",
       "      <td>2021-01-03 10:40:55-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>3205441582599331.0</td>\n",
       "      <td>2020-11-16 14:29:00-05:00</td>\n",
       "      <td>2020-11-16 17:14:00-05:00</td>\n",
       "      <td>2020-11-15 22:48:00-05:00</td>\n",
       "      <td>2020-11-16 14:06:00-05:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      hospitalization_id           vent_start_time             vent_end_time  \\\n",
       "1374  1567544868278289.0 2021-01-03 10:41:00-05:00 2021-01-03 12:50:00-05:00   \n",
       "5440  3205441582599331.0 2020-11-16 14:29:00-05:00 2020-11-16 17:14:00-05:00   \n",
       "\n",
       "              first_vital_dttm           last_vital_dttm  \n",
       "1374 2021-01-03 09:10:00-05:00 2021-01-03 10:40:55-05:00  \n",
       "5440 2020-11-15 22:48:00-05:00 2020-11-16 14:06:00-05:00  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check - last recorded vital shouldn't be less than vent start time\n",
    "# if this happens, check your CLIF tables bro\n",
    "cases_before_vent_start = final_cohort[final_cohort['last_vital_dttm'] < final_cohort['vent_start_time']]\n",
    "print(\"Cases where last vital dttm is before vent_start time:\", len(cases_before_vent_start))\n",
    "cases_before_vent_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique hospitalizations in the identified cohort 10088\n"
     ]
    }
   ],
   "source": [
    "## save IDs in this cohort to filter other tables\n",
    "cohort_ids = final_cohort['hospitalization_id'].unique().tolist()\n",
    "print(\"total number of unique hospitalizations in the identified cohort\", len(cohort_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105/1508617681.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  hour_sequence = final_cohort.groupby('hospitalization_id')\\\n"
     ]
    }
   ],
   "source": [
    "# Function to generate hourly sequence for each group (hospitalization_id)\n",
    "def generate_hourly_sequence(group):\n",
    "    # Get the vent start time and discharge time\n",
    "    start_time = group['vent_start_time'].iloc[0]\n",
    "    end_time = group['last_vital_dttm'].iloc[0]\n",
    "    \n",
    "    # Generate the sequence of hourly timestamps\n",
    "    hourly_timestamps = pd.date_range(start=start_time, end=end_time, freq='h')\n",
    "    \n",
    "    # Create a new DataFrame for this sequence\n",
    "    return pd.DataFrame({\n",
    "        'hospitalization_id': group['hospitalization_id'].iloc[0],\n",
    "        'recorded_dttm': hourly_timestamps\n",
    "    })\n",
    "\n",
    "# Apply the function to each group and concatenate the results\n",
    "hour_sequence = final_cohort.groupby('hospitalization_id')\\\n",
    "    .apply(generate_hourly_sequence)\\\n",
    "    .reset_index(drop=True)\n",
    "\n",
    "# Add `recorded_date` and `recorded_hour` columns\n",
    "# Convert recorded_dttm to datetime sanity check\n",
    "hour_sequence['recorded_dttm'] = pd.to_datetime(hour_sequence['recorded_dttm'])\n",
    "hour_sequence['recorded_date'] = hour_sequence['recorded_dttm'].dt.date\n",
    "hour_sequence['recorded_hour'] = hour_sequence['recorded_dttm'].dt.hour\n",
    "hour_sequence = hour_sequence.drop('recorded_dttm', axis=1)\n",
    "hour_sequence = hour_sequence.drop_duplicates()\n",
    "hour_sequence['time_from_vent'] = hour_sequence.groupby('hospitalization_id').cumcount()\n",
    "## add a cool off period of 4 hours after first intubation\n",
    "hour_sequence['time_from_vent_adjusted'] = hour_sequence['time_from_vent'].apply(lambda x: x - 4 if x >= 4 else -1).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: hour_sequence_check\n",
      "No duplicates found based on columns: ['hospitalization_id', 'recorded_date', 'recorded_hour'].\n"
     ]
    }
   ],
   "source": [
    "## SHOULDN'T HAVE ANY DUPLICATES\n",
    "hour_sequence_check = pyCLIF.remove_duplicates(hour_sequence, ['hospitalization_id', 'recorded_date', 'recorded_hour'], 'hour_sequence_check')\n",
    "del hour_sequence_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Respiratory support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_vent_df = cohort_on_vent.groupby(['hospitalization_id', 'recorded_date', 'recorded_hour']).agg(\n",
    "    min_resp_rate_obs=pd.NamedAgg(column='resp_rate_obs', aggfunc='min'),\n",
    "    min_lpm_set=pd.NamedAgg(column='lpm_set', aggfunc='min'),\n",
    "    min_fio2_set=pd.NamedAgg(column='fio2_set', aggfunc='min'),\n",
    "    min_peep_set=pd.NamedAgg(column='peep_set', aggfunc='min'),\n",
    "    max_resp_rate_obs=pd.NamedAgg(column='resp_rate_obs', aggfunc='max'),\n",
    "    max_lpm_set=pd.NamedAgg(column='lpm_set', aggfunc='max'),\n",
    "    max_fio2_set=pd.NamedAgg(column='fio2_set', aggfunc='max'),\n",
    "    max_peep_set=pd.NamedAgg(column='peep_set', aggfunc='max'),\n",
    "    hourly_trach=pd.NamedAgg(column='tracheostomy', aggfunc=lambda x: 1 if x.max() == 1 else 0),\n",
    "    hourly_on_vent=pd.NamedAgg(column='on_vent', aggfunc=lambda x: 1 if x.max() == 1 else 0)\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique encounters who were ever on vent (before applying exclusion criteria) 10086\n"
     ]
    }
   ],
   "source": [
    "# Merge hourly_vent_df with hour_sequence on hospitalization_id, recorded_date, and recorded_hour\n",
    "final_df = pd.merge(hour_sequence, hourly_vent_df, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], \n",
    "                     how='left')\n",
    "print(\"unique encounters who were ever on vent (before applying exclusion criteria)\", pyCLIF.count_unique_encounters(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total hours on vent for each encounter within the first 72 hours\n",
    "first_72_hours = hour_sequence[(hour_sequence['time_from_vent_adjusted'] >= 0) & (hour_sequence['time_from_vent_adjusted'] < 72)]\n",
    "final_df_72 = pd.merge(first_72_hours, hourly_vent_df, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], \n",
    "                     how='left')\n",
    "vent_hours_per_encounter = final_df_72.groupby('hospitalization_id')['hourly_on_vent'].sum()\n",
    "# Identify encounters with less than 2 hours on vent\n",
    "encounters_less_than_2_hours = vent_hours_per_encounter[vent_hours_per_encounter <= 2].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " encounters that were on the vent for less than 2 hours in the first 72 hours 1731\n",
      "\n",
      " unique encounters after excluding encounters on vent for 2 hrs or less 8355\n"
     ]
    }
   ],
   "source": [
    "# exclude those encounters that were on the vent for less than 2 hours in the first 72 hours\n",
    "final_df = final_df[~final_df['hospitalization_id'].isin(encounters_less_than_2_hours)]\n",
    "print(\"\\n encounters that were on the vent for less than 2 hours in the first 72 hours\", len(encounters_less_than_2_hours))\n",
    "print(\"\\n unique encounters after excluding encounters on vent for 2 hrs or less\", pyCLIF.count_unique_encounters(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude encounters with tracheostomy in the first 72 hours\n",
    "# Identify encounters with trach in the first 72 hours\n",
    "encounters_with_trach = final_df_72.groupby('hospitalization_id')['hourly_trach'].max()\n",
    "# Identify encounters where trach is present\n",
    "encounters_with_trach = encounters_with_trach[encounters_with_trach == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " encounters with trach in the first 72 hours 479\n",
      "\n",
      " unique encounters after excluding encounters on trach during the first 72 hours 7896\n"
     ]
    }
   ],
   "source": [
    "# Exclude encounters with trach in the first 72 hours\n",
    "final_df = final_df[~final_df['hospitalization_id'].isin(encounters_with_trach)]\n",
    "print(\"\\n encounters with trach in the first 72 hours\", len(encounters_with_trach))\n",
    "print(\"\\n unique encounters after excluding encounters on trach during the first 72 hours\", pyCLIF.count_unique_encounters(final_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Meds\n",
    "\n",
    "* Exclude encounters that are on cisatracurium or vecuronium for more than 4 hours within the first 72 hours\n",
    "* Calculate NE equivalent levels using \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \"vasopressin\", \"dopamine\",  \"angiotensin\"\n",
    "* Create flags for \"nicardipine\", \"nitroprusside\", \"clevidipine\" for the red criteria under consensus criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from /home/idies/workspace/Storage/chochbe1/JH_CCRD/CLIF/rclif/clif_medication_admin_continuous.parquet\n",
      "unique encounters in meds 4711\n"
     ]
    }
   ],
   "source": [
    "# Import clif continuous meds for the cohort on vent during the required time period\n",
    "meds_filters = {\n",
    "    'hospitalization_id': cohort_ids,\n",
    "    'med_category': meds_of_interest\n",
    "}\n",
    "meds = pyCLIF.load_data('clif_medication_admin_continuous', columns=meds_required_columns, filters=meds_filters)\n",
    "print(\"unique encounters in meds\", pyCLIF.count_unique_encounters(meds))\n",
    "meds['hospitalization_id']= meds['hospitalization_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "meds['admin_dttm'] = pd.to_datetime(meds['admin_dttm'], format='%Y-%m-%d %H:%M:%S')\n",
    "meds['med_dose'] = pd.to_numeric(meds['med_dose'], errors='coerce')\n",
    "# Create 'date' and 'hour_of_day' columns\n",
    "meds['recorded_date'] = meds['admin_dttm'].dt.date\n",
    "meds['recorded_hour'] = meds['admin_dttm'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude encounters that are on cisatracurium or vecuronium for more than 4 hours within the first 72 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105/3152522711.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cis_periods = cisatracurium_filtered.groupby('hospitalization_id').apply(identify_continuous_periods).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'admin_dttm' is in datetime format\n",
    "cisatracurium_filtered = meds[meds['med_category'].str.contains(\"cisatracurium\", case=False, na=False)].drop_duplicates()\n",
    "# Sort by 'hospitalization_id' and 'admin_dttm'\n",
    "cisatracurium_filtered = cisatracurium_filtered.sort_values(['hospitalization_id', 'recorded_date', 'recorded_hour'])\n",
    "\n",
    "# Merge with vent_start_end to get vent_start_time\n",
    "cisatracurium_filtered = cisatracurium_filtered.merge(\n",
    "    final_df_72[['hospitalization_id', 'recorded_date', 'recorded_hour']], \n",
    "    on=['hospitalization_id', 'recorded_date', 'recorded_hour'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Define the maximum allowed gap between doses (e.g., 1 hour)\n",
    "max_gap = pd.Timedelta(hours=1)\n",
    "\n",
    "# Function to identify continuous periods\n",
    "def identify_continuous_periods(group):\n",
    "    group = group.copy()\n",
    "    group['time_diff'] = group['admin_dttm'].diff()\n",
    "    group['new_period'] = (group['time_diff'] > max_gap) | (group['time_diff'].isna())\n",
    "    group['period_id'] = group['new_period'].cumsum()\n",
    "    return group\n",
    "\n",
    "# Apply the function to each 'hospitalization_id'\n",
    "cis_periods = cisatracurium_filtered.groupby('hospitalization_id').apply(identify_continuous_periods).reset_index(drop=True)\n",
    "\n",
    "# Calculate the duration of each continuous period\n",
    "period_durations = cis_periods.groupby(['hospitalization_id', 'period_id']).agg(\n",
    "    period_start=('admin_dttm', 'min'),\n",
    "    period_end=('admin_dttm', 'max')\n",
    ").reset_index()\n",
    "\n",
    "period_durations['period_duration'] = (\n",
    "    period_durations['period_end'] - period_durations['period_start']\n",
    ").dt.total_seconds() / 3600  # Convert to hours\n",
    "\n",
    "# Identify patients with any continuous period >= 4 hours\n",
    "cis_flag_df = period_durations.groupby('hospitalization_id').agg(\n",
    "    max_period_duration=('period_duration', 'max')\n",
    ").reset_index()\n",
    "\n",
    "cis_flag_df['cis_flag'] = (cis_flag_df['max_period_duration'] >= 4).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " encounters with cis for more than 4 hours  in the first 72 hours 63\n",
      "\n",
      " unique encounters after excluding encounters on trach during the first 72 hours 7834\n"
     ]
    }
   ],
   "source": [
    "encounters_with_cis = cis_flag_df[cis_flag_df['cis_flag'] == 1]['hospitalization_id'].drop_duplicates()\n",
    "final_df = final_df[~final_df['hospitalization_id'].isin(encounters_with_cis)]\n",
    "print(\"\\n encounters with cis for more than 4 hours  in the first 72 hours\", len(encounters_with_cis))\n",
    "print(\"\\n unique encounters after excluding encounters on trach during the first 72 hours\", pyCLIF.count_unique_encounters(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105/2696787705.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  vec_periods = vecuronium_filtered.groupby('hospitalization_id').apply(identify_continuous_periods).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'admin_dttm' is in datetime format\n",
    "vecuronium_filtered = meds[meds['med_category'].str.contains(\"vecuronium\", case=False, na=False)].drop_duplicates()\n",
    "# Sort by 'hospitalization_id' and 'admin_dttm'\n",
    "vecuronium_filtered = vecuronium_filtered.sort_values(['hospitalization_id', 'recorded_date', 'recorded_hour'])\n",
    "\n",
    "# Merge with vent_start_end to get vent_start_time\n",
    "vecuronium_filtered = vecuronium_filtered.merge(\n",
    "    final_df_72[['hospitalization_id', 'recorded_date', 'recorded_hour']], \n",
    "    on=['hospitalization_id', 'recorded_date', 'recorded_hour'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Define the maximum allowed gap between doses (e.g., 1 hour)\n",
    "max_gap = pd.Timedelta(hours=1)\n",
    "\n",
    "# Function to identify continuous periods\n",
    "def identify_continuous_periods(group):\n",
    "    group = group.copy()\n",
    "    group['time_diff'] = group['admin_dttm'].diff()\n",
    "    group['new_period'] = (group['time_diff'] > max_gap) | (group['time_diff'].isna())\n",
    "    group['period_id'] = group['new_period'].cumsum()\n",
    "    return group\n",
    "\n",
    "# Apply the function to each 'hospitalization_id'\n",
    "vec_periods = vecuronium_filtered.groupby('hospitalization_id').apply(identify_continuous_periods).reset_index(drop=True)\n",
    "\n",
    "# Calculate the duration of each continuous period\n",
    "period_durations = vec_periods.groupby(['hospitalization_id', 'period_id']).agg(\n",
    "    period_start=('admin_dttm', 'min'),\n",
    "    period_end=('admin_dttm', 'max')\n",
    ").reset_index()\n",
    "\n",
    "period_durations['period_duration'] = (\n",
    "    period_durations['period_end'] - period_durations['period_start']\n",
    ").dt.total_seconds() / 3600  # Convert to hours\n",
    "\n",
    "# Identify patients with any continuous period >= 4 hours\n",
    "vec_flag_df = period_durations.groupby('hospitalization_id').agg(\n",
    "    max_period_duration=('period_duration', 'max')\n",
    ").reset_index()\n",
    "\n",
    "vec_flag_df['vec_flag'] = (vec_flag_df['max_period_duration'] >= 4).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " encounters with cis for more than 4 hours  in the first 72 hours 63\n",
      "\n",
      " unique encounters after excluding encounters on trach during the first 72 hours 7676\n"
     ]
    }
   ],
   "source": [
    "encounters_with_vec = vec_flag_df[vec_flag_df['vec_flag'] == 1]['hospitalization_id'].drop_duplicates()\n",
    "final_df = final_df[~final_df['hospitalization_id'].isin(encounters_with_vec)]\n",
    "print(\"\\n encounters with vec for more than 4 hours  in the first 72 hours\", len(encounters_with_cis))\n",
    "print(\"\\n unique encounters after excluding encounters on trach during the first 72 hours\", pyCLIF.count_unique_encounters(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Norepinephrine equivalent calculation\n",
    "# Goradia S, Sardaneh AA, Narayan SW, Penm J, Patanwala AE. Vasopressor dose equivalence: \n",
    "# A scoping review and suggested formula. J Crit Care. 2021 Feb;61:233-240. doi: 10.1016/j.jcrc.2020.11.002. Epub 2020 Nov 14. PMID: 33220576.\n",
    "\n",
    "meds_list = [\n",
    "    \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \n",
    "    \"vasopressin\", \"dopamine\",  \n",
    "    \"angiotensin\"\n",
    "]\n",
    "\n",
    "# Function to check if 'med_dose_unit' contains '/hr' or '/min'\n",
    "def has_per_hour_or_min(unit):\n",
    "    if pd.isnull(unit):\n",
    "        return False\n",
    "    unit = unit.lower()\n",
    "    return '/hr' in unit or '/min' in unit\n",
    "\n",
    "# Filter meds to include only rows with '/hr' or '/min' in 'med_dose_unit'\n",
    "meds_filtered = meds[meds['med_dose_unit'].apply(has_per_hour_or_min)].copy()\n",
    "\n",
    "ne_df = meds_filtered[meds_filtered['med_category'].isin(meds_list)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>med_category</th>\n",
       "      <th>total_N</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>first_quantile</th>\n",
       "      <th>second_quantile</th>\n",
       "      <th>third_quantile</th>\n",
       "      <th>missing_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dopamine</td>\n",
       "      <td>2565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>epinephrine</td>\n",
       "      <td>9074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>norepinephrine</td>\n",
       "      <td>117777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phenylephrine</td>\n",
       "      <td>6301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.95</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vasopressin</td>\n",
       "      <td>28872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     med_category  total_N  min     max  first_quantile  second_quantile  \\\n",
       "0        dopamine     2565  0.0    30.0            2.00             4.00   \n",
       "1     epinephrine     9074  0.0   100.0            0.03             0.06   \n",
       "2  norepinephrine   117777  0.0   600.0            0.05             0.15   \n",
       "3   phenylephrine     6301  0.0   300.0            0.30             0.95   \n",
       "4     vasopressin    28872  0.0  1000.0            0.03             0.04   \n",
       "\n",
       "   third_quantile  missing_values  \n",
       "0            6.00               0  \n",
       "1            0.15               0  \n",
       "2            0.54               0  \n",
       "3           10.00               0  \n",
       "4            0.04               0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a summary table for each med_category\n",
    "summary_table = ne_df.groupby('med_category').agg(\n",
    "    total_N=('med_category', 'size'),\n",
    "    min=('med_dose', 'min'),\n",
    "    max=('med_dose', 'max'),\n",
    "    first_quantile=('med_dose', lambda x: x.quantile(0.25)),\n",
    "    second_quantile=('med_dose', lambda x: x.quantile(0.5)),\n",
    "    third_quantile=('med_dose', lambda x: x.quantile(0.75)),\n",
    "    missing_values=('med_dose', lambda x: x.isna().sum())\n",
    ").reset_index()\n",
    "\n",
    "## check the distrbituon of required continuous meds\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>med_category</th>\n",
       "      <th>med_dose_unit</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cisatracurium</td>\n",
       "      <td>mcg/kg/min</td>\n",
       "      <td>6514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cisatracurium</td>\n",
       "      <td>mg</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cisatracurium</td>\n",
       "      <td>mg/kg/hr</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dopamine</td>\n",
       "      <td>mcg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dopamine</td>\n",
       "      <td>mcg/kg/min</td>\n",
       "      <td>2565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>epinephrine</td>\n",
       "      <td>mcg/kg/min</td>\n",
       "      <td>8362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>epinephrine</td>\n",
       "      <td>mcg/min</td>\n",
       "      <td>712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>epinephrine</td>\n",
       "      <td>mg</td>\n",
       "      <td>2898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nicardipine</td>\n",
       "      <td>mcg/kg/min</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nicardipine</td>\n",
       "      <td>mg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nicardipine</td>\n",
       "      <td>mg/hr</td>\n",
       "      <td>6710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nitroprusside</td>\n",
       "      <td>mcg/kg/min</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>norepinephrine</td>\n",
       "      <td>mcg/kg/min</td>\n",
       "      <td>108721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>norepinephrine</td>\n",
       "      <td>mcg/min</td>\n",
       "      <td>9055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>norepinephrine</td>\n",
       "      <td>mg</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>norepinephrine</td>\n",
       "      <td>mg/min</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phenylephrine</td>\n",
       "      <td>mcg</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>phenylephrine</td>\n",
       "      <td>mcg/kg/min</td>\n",
       "      <td>4564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>phenylephrine</td>\n",
       "      <td>mcg/min</td>\n",
       "      <td>1737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>phenylephrine</td>\n",
       "      <td>mg</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>vasopressin</td>\n",
       "      <td>Units</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>vasopressin</td>\n",
       "      <td>Units/mL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>vasopressin</td>\n",
       "      <td>Units/min</td>\n",
       "      <td>28005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>vasopressin</td>\n",
       "      <td>mL/hr</td>\n",
       "      <td>821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>vasopressin</td>\n",
       "      <td>milli-units/kg/min</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>vecuronium</td>\n",
       "      <td>mcg/kg/min</td>\n",
       "      <td>19759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>vecuronium</td>\n",
       "      <td>mg</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>vecuronium</td>\n",
       "      <td>mg/kg/hr</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      med_category       med_dose_unit   count\n",
       "0    cisatracurium          mcg/kg/min    6514\n",
       "1    cisatracurium                  mg       7\n",
       "2    cisatracurium            mg/kg/hr      76\n",
       "3         dopamine                 mcg       4\n",
       "4         dopamine          mcg/kg/min    2565\n",
       "5      epinephrine          mcg/kg/min    8362\n",
       "6      epinephrine             mcg/min     712\n",
       "7      epinephrine                  mg    2898\n",
       "8      nicardipine          mcg/kg/min     414\n",
       "9      nicardipine                  mg       6\n",
       "10     nicardipine               mg/hr    6710\n",
       "11   nitroprusside          mcg/kg/min     543\n",
       "12  norepinephrine          mcg/kg/min  108721\n",
       "13  norepinephrine             mcg/min    9055\n",
       "14  norepinephrine                  mg      45\n",
       "15  norepinephrine              mg/min       1\n",
       "16   phenylephrine                 mcg     995\n",
       "17   phenylephrine          mcg/kg/min    4564\n",
       "18   phenylephrine             mcg/min    1737\n",
       "19   phenylephrine                  mg       9\n",
       "20     vasopressin               Units      17\n",
       "21     vasopressin            Units/mL       1\n",
       "22     vasopressin           Units/min   28005\n",
       "23     vasopressin               mL/hr     821\n",
       "24     vasopressin  milli-units/kg/min      46\n",
       "25      vecuronium          mcg/kg/min   19759\n",
       "26      vecuronium                  mg      79\n",
       "27      vecuronium            mg/kg/hr     165"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the med_dose_unit for each med_category in the meds table\n",
    "med_dose_unit_check = meds.groupby(['med_category', 'med_dose_unit']).size().reset_index(name='count')\n",
    "# Display the results\n",
    "med_dose_unit_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angiotensin is not in the dataset.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# **2. Convert Medication Doses to Required Units**\n",
    "\n",
    "# Define medications and their unit conversion information\n",
    "meds_list = [\n",
    "    \"norepinephrine\", \"epinephrine\", \"phenylephrine\",\n",
    "    \"vasopressin\", \"dopamine\", \"angiotensin\"\n",
    "]\n",
    "\n",
    "med_unit_info = {\n",
    "    'norepinephrine': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mcg/kg/min', 'mcg/kg/hr', 'mg/kg/hr', 'mcg/min', 'mg/hr'],\n",
    "    },\n",
    "    'epinephrine': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mcg/kg/min', 'mcg/kg/hr', 'mg/kg/hr', 'mcg/min', 'mg/hr'],\n",
    "    },\n",
    "    'phenylephrine': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mcg/kg/min', 'mcg/kg/hr', 'mg/kg/hr', 'mcg/min', 'mg/hr'],\n",
    "    },\n",
    "    'dopamine': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mcg/kg/min', 'mcg/kg/hr', 'mg/kg/hr', 'mcg/min', 'mg/hr'],\n",
    "    },\n",
    "    'metaraminol': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mg/hr', 'mcg/min'],\n",
    "    },\n",
    "    'angiotensin': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['ng/kg/min', 'ng/kg/hr'],\n",
    "    },\n",
    "    'vasopressin': {\n",
    "        'required_unit': 'units/min',\n",
    "        'acceptable_units': ['units/min', 'units/hr', 'milliunits/min', 'milliunits/hr'],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Function to get conversion factor for each medication\n",
    "def get_conversion_factor(med_category, med_dose_unit, weight_kg):\n",
    "    med_info = med_unit_info.get(med_category, None)\n",
    "    if not med_info:\n",
    "        # Medication not in the list\n",
    "        return None\n",
    "    required_unit = med_info['required_unit']\n",
    "    acceptable_units = med_info['acceptable_units']\n",
    "    med_dose_unit = med_dose_unit.lower()\n",
    "    if med_category in ['norepinephrine', 'epinephrine', 'phenylephrine', 'dopamine', 'metaraminol']:\n",
    "        # Required unit: mcg/kg/min\n",
    "        if med_dose_unit == 'mcg/kg/min':\n",
    "            factor = 1.0\n",
    "        elif med_dose_unit == 'mcg/kg/hr':\n",
    "            factor = 1 / 60\n",
    "        elif med_dose_unit == 'mg/kg/hr':\n",
    "            factor = 1000 / 60\n",
    "        elif med_dose_unit == 'mcg/min':\n",
    "            factor = 1 / weight_kg\n",
    "        elif med_dose_unit == 'mg/hr':\n",
    "            factor = 1000 / 60 / weight_kg\n",
    "        else:\n",
    "            return None\n",
    "    elif med_category == 'angiotensin':\n",
    "        # Required unit: mcg/kg/min\n",
    "        if med_dose_unit == 'ng/kg/min':\n",
    "            factor = 1 / 1000\n",
    "        elif med_dose_unit == 'ng/kg/hr':\n",
    "            factor = 1 / 1000 / 60\n",
    "        else:\n",
    "            return None\n",
    "    elif med_category == 'vasopressin':\n",
    "        # Required unit: units/min\n",
    "        if med_dose_unit == 'units/min':\n",
    "            factor = 1.0\n",
    "        elif med_dose_unit == 'units/hr':\n",
    "            factor = 1 / 60\n",
    "        elif med_dose_unit == 'milliunits/min':\n",
    "            factor = 1 / 1000\n",
    "        elif med_dose_unit == 'milliunits/hr':\n",
    "            factor = 1 / 1000 / 60\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "    return factor\n",
    "\n",
    "# Merge weight_kg into meds_filtered (assuming 'vitals_bmi_pivot' is available)\n",
    "meds_filtered = meds_filtered.merge(vitals_bmi_pivot[['hospitalization_id', 'weight_kg']], on='hospitalization_id', how='left')\n",
    "\n",
    "# Remove rows with missing weight_kg\n",
    "meds_filtered = meds_filtered[~meds_filtered['weight_kg'].isnull()].copy()\n",
    "\n",
    "# Function to convert doses\n",
    "def convert_dose(row):\n",
    "    med_category = row['med_category']\n",
    "    med_dose = row['med_dose']\n",
    "    med_dose_unit = row['med_dose_unit']\n",
    "    weight_kg = row['weight_kg']\n",
    "    factor = get_conversion_factor(med_category, med_dose_unit, weight_kg)\n",
    "    if factor is None:\n",
    "        return np.nan\n",
    "    return med_dose * factor\n",
    "\n",
    "# Apply the conversion to get 'med_dose_converted'\n",
    "meds_filtered['med_dose_converted'] = meds_filtered.apply(convert_dose, axis=1)\n",
    "\n",
    "# Drop rows with NaN in 'med_dose_converted' (unrecognized units)\n",
    "meds_filtered = meds_filtered[~meds_filtered['med_dose_converted'].isnull()].copy()\n",
    "\n",
    "# Define acceptable dose ranges\n",
    "med_dose_ranges = {\n",
    "    'norepinephrine': (0.01, 3),\n",
    "    'epinephrine': (0.01, 0.1),\n",
    "    'phenylephrine': (0.1, 5),\n",
    "    'dopamine': (2, 20),\n",
    "    'metaraminol': (0.5, 10),  # Hypothetical range\n",
    "    'angiotensin': (0.02, 0.2),  # After conversion to mcg/kg/min\n",
    "    'vasopressin': (0.01, 0.1),  # Units/min acceptable range\n",
    "}\n",
    "\n",
    "# Function to check if dose is within range\n",
    "def is_dose_within_range(row):\n",
    "    med_category = row['med_category']\n",
    "    med_dose_converted = row['med_dose_converted']\n",
    "    dose_range = med_dose_ranges.get(med_category, None)\n",
    "    if dose_range is None:\n",
    "        return False\n",
    "    min_dose, max_dose = dose_range\n",
    "    return min_dose <= med_dose_converted <= max_dose\n",
    "\n",
    "# Filter doses within acceptable ranges\n",
    "meds_filtered = meds_filtered[meds_filtered.apply(is_dose_within_range, axis=1)].copy()\n",
    "\n",
    "# **4. Flag Medications Not in the Dataset**\n",
    "\n",
    "for med in meds_list:\n",
    "    if med not in meds_filtered['med_category'].unique():\n",
    "        print(f\"{med} is not in the dataset.\")\n",
    "\n",
    "# Pivot and Aggregate the Data**\n",
    "\n",
    "# Create 'recorded_date' and 'recorded_hour' columns\n",
    "meds_filtered['admin_dttm'] = pd.to_datetime(meds_filtered['admin_dttm'])\n",
    "meds_filtered['recorded_date'] = meds_filtered['admin_dttm'].dt.date\n",
    "meds_filtered['recorded_hour'] = meds_filtered['admin_dttm'].dt.hour\n",
    "\n",
    "# Group and aggregate doses\n",
    "group_cols = ['hospitalization_id', 'recorded_date', 'recorded_hour', 'med_category']\n",
    "dose_agg = meds_filtered.groupby(group_cols)['med_dose_converted'].agg(['min', 'max']).reset_index()\n",
    "\n",
    "# Pivot to have medications as columns\n",
    "dose_pivot_min = dose_agg.pivot_table(index=['hospitalization_id', 'recorded_date', 'recorded_hour'], columns='med_category', values='min').reset_index()\n",
    "dose_pivot_max = dose_agg.pivot_table(index=['hospitalization_id', 'recorded_date', 'recorded_hour'], columns='med_category', values='max').reset_index()\n",
    "\n",
    "# Rename columns to indicate min and max\n",
    "dose_pivot_min.columns = ['hospitalization_id', 'recorded_date', 'recorded_hour'] + ['min_' + col for col in dose_pivot_min.columns if col not in ['hospitalization_id', 'recorded_date', 'recorded_hour']]\n",
    "dose_pivot_max.columns = ['hospitalization_id', 'recorded_date', 'recorded_hour'] + ['max_' + col for col in dose_pivot_max.columns if col not in ['hospitalization_id', 'recorded_date', 'recorded_hour']]\n",
    "\n",
    "# Merge min and max DataFrames\n",
    "dose_pivot = pd.merge(dose_pivot_min, dose_pivot_max, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], how='outer')\n",
    "\n",
    "# **6. Calculate Norepinephrine Equivalents**\n",
    "\n",
    "# Replace NaN with 0 for calculations\n",
    "dose_pivot.fillna(0, inplace=True)\n",
    "\n",
    "# Calculate NE min\n",
    "dose_pivot['ne_calc_min'] = (\n",
    "    dose_pivot.get('min_norepinephrine', 0) +\n",
    "    dose_pivot.get('min_epinephrine', 0) +\n",
    "    dose_pivot.get('min_phenylephrine', 0) / 10 +\n",
    "    dose_pivot.get('min_dopamine', 0) / 100 +\n",
    "    dose_pivot.get('min_metaraminol', 0) / 8 +\n",
    "    dose_pivot.get('min_vasopressin', 0) * 2.5 +\n",
    "    dose_pivot.get('min_angiotensin', 0) * 10\n",
    ")\n",
    "\n",
    "# Calculate NE max\n",
    "dose_pivot['ne_calc_max'] = (\n",
    "    dose_pivot.get('max_norepinephrine', 0) +\n",
    "    dose_pivot.get('max_epinephrine', 0) +\n",
    "    dose_pivot.get('max_phenylephrine', 0) / 10 +\n",
    "    dose_pivot.get('max_dopamine', 0) / 100 +\n",
    "    dose_pivot.get('max_metaraminol', 0) / 8 +\n",
    "    dose_pivot.get('max_vasopressin', 0) * 2.5 +\n",
    "    dose_pivot.get('max_angiotensin', 0) * 10\n",
    ")\n",
    "\n",
    "# **7. Prepare the Final Dataset**\n",
    "\n",
    "# Keep only the required columns\n",
    "ne_calc_df = dose_pivot[['hospitalization_id', 'recorded_date', \n",
    "                         'recorded_hour', \n",
    "                         'ne_calc_min', 'ne_calc_max']].drop_duplicates(subset=['hospitalization_id', 'recorded_date', 'recorded_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, ne_calc_df, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_meds_list = [\n",
    "    \"nicardipine\", \"nitroprusside\", \"clevidipine\"\n",
    "]\n",
    "\n",
    "# Filter meds_filtered for the medications in red_meds_list\n",
    "red_meds_df = meds[meds['med_category'].isin(red_meds_list)].copy()\n",
    "\n",
    "# Create a flag for each medication in red_meds_list\n",
    "for med in red_meds_list:\n",
    "    # Create a flag that is 1 if the medication was administered in that hour, 0 otherwise\n",
    "    red_meds_df[med + '_flag'] = np.where(red_meds_df['med_category'] == med, 1, 0).astype(int)\n",
    "\n",
    "# Aggregate to get the maximum value for each flag (per hospitalization_id, recorded_date, recorded_hour)\n",
    "# This ensures that if the medication was administered even once in the hour, the flag is 1\n",
    "red_meds_flags = red_meds_df.groupby(['hospitalization_id', 'recorded_date', 'recorded_hour']).agg(\n",
    "    {med + '_flag': 'max' for med in red_meds_list}\n",
    ").reset_index()\n",
    "\n",
    "#  combine all flags into a single 'red_meds_flag', you can do so like this:\n",
    "red_meds_flags['red_meds_flag'] = red_meds_flags[[med + '_flag' for med in red_meds_list]].max(axis=1)\n",
    "\n",
    "# Select the relevant columns\n",
    "red_meds_flags_final = red_meds_flags[[\n",
    "    'hospitalization_id', 'recorded_date', 'recorded_hour',\n",
    "    'nicardipine_flag', 'nitroprusside_flag',\n",
    "    'clevidipine_flag', 'red_meds_flag'\n",
    "]].drop_duplicates(subset=['hospitalization_id', 'recorded_date', 'recorded_hour'])\n",
    "\n",
    "red_meds_flags_final['nicardipine_flag'] = red_meds_flags_final['nicardipine_flag'].astype(int)\n",
    "red_meds_flags_final['nitroprusside_flag'] = red_meds_flags_final['nitroprusside_flag'].astype(int)\n",
    "red_meds_flags_final['clevidipine_flag'] = red_meds_flags_final['clevidipine_flag'].astype(int)\n",
    "red_meds_flags_final['red_meds_flag'] = red_meds_flags_final['red_meds_flag'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, red_meds_flags_final, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hospitalization_id', 'recorded_dttm', 'vital_category', 'vital_value'], dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vitals.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'recorded_dttm' is datetime\n",
    "vitals['recorded_dttm'] = pd.to_datetime(vitals['recorded_dttm'])\n",
    "# Extract 'recorded_date' and 'recorded_hour'\n",
    "vitals['recorded_hour'] = vitals['recorded_dttm'].dt.hour\n",
    "vitals['recorded_date'] = vitals['recorded_dttm'].dt.date\n",
    "\n",
    "# Check if 'map' exists in 'vital_category'\n",
    "if 'map' not in vitals['vital_category'].unique():\n",
    "    print(\"map is not present, so we'll calculate it\")\n",
    "\n",
    "    # Filter vitals to include only 'sbp' and 'dbp'\n",
    "    sbp_dbp_vitals = vitals[vitals['vital_category'].isin(['sbp', 'dbp'])].copy()\n",
    "\n",
    "    # Pivot to have 'sbp' and 'dbp' as columns\n",
    "    sbp_dbp_pivot = sbp_dbp_vitals.pivot_table(\n",
    "        index=['hospitalization_id', 'recorded_dttm'],\n",
    "        columns='vital_category',\n",
    "        values='vital_value'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Drop rows where either 'sbp' or 'dbp' is missing\n",
    "    sbp_dbp_pivot = sbp_dbp_pivot.dropna(subset=['sbp', 'dbp'])\n",
    "\n",
    "    # Calculate 'map' using the formula\n",
    "    sbp_dbp_pivot['map'] = (sbp_dbp_pivot['sbp'] + 2 * sbp_dbp_pivot['dbp']) / 3\n",
    "\n",
    "    # Create a DataFrame for 'map' vitals\n",
    "    map_vitals = sbp_dbp_pivot[['hospitalization_id', 'recorded_dttm', 'map']].copy()\n",
    "\n",
    "    # Add 'vital_category' and 'vital_value' columns\n",
    "    map_vitals['vital_category'] = 'map'\n",
    "    map_vitals['vital_value'] = map_vitals['map']\n",
    "    # Extract 'recorded_date' and 'recorded_hour' for map_vitals\n",
    "    map_vitals['recorded_date'] = map_vitals['recorded_dttm'].dt.date\n",
    "    map_vitals['recorded_hour'] = map_vitals['recorded_dttm'].dt.hour\n",
    "\n",
    "    # Keep only the necessary columns\n",
    "    map_vitals = map_vitals[['hospitalization_id', 'recorded_dttm', 'recorded_date', \n",
    "                             'recorded_hour', 'vital_category', 'vital_value']]\n",
    "\n",
    "\n",
    "    # Append 'map' vitals back to the original 'vitals' DataFrame\n",
    "    vitals = pd.concat([vitals, map_vitals], ignore_index=True)\n",
    "\n",
    "# Proceed with grouping and pivoting\n",
    "vitals_min_max = vitals.groupby(\n",
    "    ['hospitalization_id', 'recorded_date', 'recorded_hour', 'vital_category']\n",
    ").agg(\n",
    "    min=pd.NamedAgg(column='vital_value', aggfunc='min'),\n",
    "    max=pd.NamedAgg(column='vital_value', aggfunc='max')\n",
    ").reset_index()\n",
    "\n",
    "# Pivot the table to reshape it\n",
    "vitals_pivot = vitals_min_max.pivot_table(\n",
    "    index=['hospitalization_id', 'recorded_date', 'recorded_hour'],\n",
    "    columns='vital_category',\n",
    "    values=['min', 'max']\n",
    ").reset_index()\n",
    "\n",
    "# Flatten the column multi-index after pivot\n",
    "vitals_pivot.columns = [\n",
    "    '_'.join(col).strip() if isinstance(col, tuple) else col for col in vitals_pivot.columns\n",
    "]\n",
    "\n",
    "# Remove trailing underscores\n",
    "vitals_pivot.columns = [col.rstrip('_') for col in vitals_pivot.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge vitals with final_df\n",
    "final_df = pd.merge(final_df, vitals_pivot, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], \n",
    "                   how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: final_df\n",
      "No duplicates found based on columns: ['hospitalization_id', 'recorded_date', 'recorded_hour'].\n"
     ]
    }
   ],
   "source": [
    "## confirm duplicates don't exist\n",
    "checkpoint_vitals = pyCLIF.remove_duplicates(final_df, [\n",
    "    'hospitalization_id','recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_vitals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Lab\n",
    "\n",
    "Get most recent lactate defined as closest lab result time to the start of first intubation event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from /home/idies/workspace/Storage/chochbe1/JH_CCRD/CLIF/rclif/clif_labs.parquet\n",
      "unique encounters in labs 8256\n"
     ]
    }
   ],
   "source": [
    "# Import clif continuous meds and clif labs table for the cohort on vent during the required time period\n",
    "labs_filters = {\n",
    "    'hospitalization_id': cohort_ids,\n",
    "    'lab_category': labs_of_interest\n",
    "}\n",
    "labs = pyCLIF.load_data('clif_labs', columns=labs_required_columns, filters=labs_filters)\n",
    "print(\"unique encounters in labs\", pyCLIF.count_unique_encounters(labs))\n",
    "labs['hospitalization_id']= labs['hospitalization_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs['lab_result_dttm'] = pd.to_datetime(labs['lab_result_dttm'])\n",
    "labs['recorded_hour'] = labs['lab_result_dttm'].dt.hour\n",
    "labs['recorded_date'] = labs['lab_result_dttm'].dt.date\n",
    "\n",
    "lactate_df = pd.merge(labs, vent_start_end, on='hospitalization_id', how='left')\n",
    "lactate_df['time_since_vent_start_hours'] = (\n",
    "    (lactate_df['lab_result_dttm'] - lactate_df['vent_start_time']).dt.total_seconds() / 3600\n",
    ")\n",
    "\n",
    "# Calculate the absolute time difference between lab_result_dttm and vent_start_time in hours\n",
    "lactate_df['time_diff_hours'] = abs((lactate_df['lab_result_dttm'] - lactate_df['vent_start_time']).dt.total_seconds() / 3600)\n",
    "\n",
    "# Filter for observations within the first 72 hours since vent_start_time\n",
    "lactate_df = lactate_df[(lactate_df['time_since_vent_start_hours'] >= 0) & \n",
    "                        (lactate_df['time_since_vent_start_hours'] <= 72)]\n",
    "\n",
    "# Sort by hospitalization_id, recorded_hour, and time_diff_hours to find the closest measurement to vent_start_time\n",
    "lactate_df = lactate_df.sort_values(by=['hospitalization_id', 'recorded_date', 'recorded_hour', 'time_diff_hours'])\n",
    "\n",
    "# Group by hospitalization_id and recorded_hour, and get the first row in each group (which is the closest measurement)\n",
    "# closest lactate measurement is defined as closest to the vent_start_time in that hour. \n",
    "closest_lactate_df = lactate_df.groupby(['hospitalization_id', 'recorded_date','recorded_hour']).first().reset_index()\n",
    "\n",
    "labs_final = closest_lactate_df[['hospitalization_id', 'recorded_date', 'recorded_hour', 'lab_value_numeric']].copy()\n",
    "\n",
    "# Rename the 'lab_value_numeric' column to 'lactate'\n",
    "labs_final = labs_final.rename(columns={'lab_value_numeric': 'lactate'})\n",
    "\n",
    "final_df = pd.merge(final_df, labs_final, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], \n",
    "                   how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: final_df\n",
      "No duplicates found based on columns: ['hospitalization_id', 'recorded_date', 'recorded_hour'].\n"
     ]
    }
   ],
   "source": [
    "checkpoint_labs= pyCLIF.remove_duplicates(final_df, [\n",
    "    'hospitalization_id', 'recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hospitalization_id', 'recorded_date', 'recorded_hour',\n",
       "       'time_from_vent', 'time_from_vent_adjusted', 'min_resp_rate_obs',\n",
       "       'min_lpm_set', 'min_fio2_set', 'min_peep_set', 'max_resp_rate_obs',\n",
       "       'max_lpm_set', 'max_fio2_set', 'max_peep_set', 'hourly_trach',\n",
       "       'hourly_on_vent', 'ne_calc_min', 'ne_calc_max', 'nicardipine_flag',\n",
       "       'nitroprusside_flag', 'clevidipine_flag', 'red_meds_flag', 'max_dbp',\n",
       "       'max_heart_rate', 'max_height_cm', 'max_map', 'max_respiratory_rate',\n",
       "       'max_sbp', 'max_spo2', 'max_weight_kg', 'min_dbp', 'min_heart_rate',\n",
       "       'min_height_cm', 'min_map', 'min_respiratory_rate', 'min_sbp',\n",
       "       'min_spo2', 'min_weight_kg', 'lactate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write analysis dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_parquet('../output/intermediate/final_df.parquet')\n",
    "vent_start_end.to_parquet('../output/intermediate/vent_start_end.parquet')\n",
    "final_df['hospitalization_id'].to_csv('../output/intermediate/cohort_ids.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mobilization)",
   "language": "python",
   "name": ".mobilization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
