{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eligibility for mobilization: Cohort ID and Discretizing script\n",
    "\n",
    "Author: Kaveri Chhikara\n",
    "v1: October 30, 2024\n",
    "v2: February 10, 2025\n",
    "\n",
    "This script identifies the cohort using CLIF 2.0 tables and discretizes the dataset at an hourly level\n",
    "\n",
    " \n",
    "                        ðŸš¨Code will break if the following requirements are not satisfiedðŸš¨  \n",
    "#### Requirements:\n",
    "* Required table filenames should be `clif_patient`, `clif_hospitalization`, `clif_adt`, `clif_vitals`, `clif_labs`, `clif_medication_admin_continuous`, `clif_respiratory_support`\n",
    "* Within each table, the following variables and categories are required.\n",
    "\n",
    "| Table Name | Required Variables | Required Categories |\n",
    "| --- | --- | --- |\n",
    "| **patient** | `patient_id`, `race_category`, `ethnicity_category`, `sex_category`, `death_dttm` | - |\n",
    "| **hospitalization** | `patient_id`, `hospitalization_id`, `admission_dttm`, `discharge_dttm`, `age_at_admission` | - |\n",
    "| **adt** |  `hospitalization_id`, `hospital_id`,`in_dttm`, `out_dttm`, `location_category` | - |\n",
    "| **vitals** | `hospitalization_id`, `recorded_dttm`, `vital_category`, `vital_value` | heart_rate, resp_rate, sbp, dbp, map, spo2, weight_kg, height_cm |\n",
    "| **labs** | `hospitalization_id`, `lab_result_dttm`, `lab_category`, `lab_value` | lactate |\n",
    "| **medication_admin_continuous** | `hospitalization_id`, `admin_dttm`, `med_name`, `med_category`, `med_dose`, `med_dose_unit` | norepinephrine, epinephrine, phenylephrine, vasopressin, dopamine, angiotensin(optional), nicardipine, nitroprusside, clevidipine, cisatracurium, vecuronium, rocuronium |\n",
    "| **respiratory_support** | `hospitalization_id`, `recorded_dttm`, `device_category`, `mode_category`, `tracheostomy`, `fio2_set`, `lpm_set`, `resp_rate_set`, `peep_set`, `resp_rate_obs` | - |\n",
    "\n",
    "\n",
    "Updates 2/10:\n",
    "* Get discharge_dttm and death_dttm. Everyone in the cohort must have one of these. If not dead, assume discharged alive.\n",
    "* Include all paralytics in the mCIDE. Instead of excluding anyone who ever received a paralytics, create flags for paralytics. While creating flags for eligibility, exclude hours when the patient was on a paralytic.\n",
    "* Update exclusion criteria - exclude all patients intubated for < 4 hrs instead of 2 hrs to be consistent with the cool-off period.\n",
    "* Add code to stitch encounters when there are multiple hospitals at a site\n",
    "* Extend the analysis to competing risk. Events - 1 - eligible, 2- died, 3 discharged-alive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from config.json\n",
      "{'site_name': 'UCMC', 'tables_path': '/Users/kavenchhikara/Desktop/CLIF/CLIF-UCMC/rclif/c19', 'file_type': 'parquet'}\n"
     ]
    }
   ],
   "source": [
    "#! pip install pandas numpy duckdb seaborn matplotlib plotly\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pyCLIF\n",
    "\n",
    "## import outlier json\n",
    "with open('../config/outlier_config.json', 'r') as f:\n",
    "    outlier_cfg = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required columns and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'device_name',\n",
    "    'device_category',\n",
    "    'mode_name', \n",
    "    'mode_category',\n",
    "    'tracheostomy',\n",
    "    'fio2_set',\n",
    "    'lpm_set',\n",
    "    'resp_rate_set',\n",
    "    'peep_set',\n",
    "    'resp_rate_obs'\n",
    "]\n",
    "\n",
    "vitals_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'vital_category',\n",
    "    'vital_value'\n",
    "]\n",
    "vitals_of_interest = ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
    "\n",
    "labs_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'lab_result_dttm',\n",
    "    'lab_category',\n",
    "    'lab_value',\n",
    "    'lab_value_numeric'\n",
    "]\n",
    "labs_of_interest = ['lactate']\n",
    "\n",
    "meds_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'admin_dttm',\n",
    "    'med_name',\n",
    "    'med_category',\n",
    "    'med_dose',\n",
    "    'med_dose_unit'\n",
    "]\n",
    "meds_of_interest = [\n",
    "    'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin', 'nicardipine', 'nitroprusside',\n",
    "    'clevidipine', 'cisatracurium', 'vecuronium', 'rocuronium '\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from /Users/kavenchhikara/Desktop/CLIF/CLIF-UCMC/rclif/c19/clif_patient.parquet\n",
      "Data loaded successfully from /Users/kavenchhikara/Desktop/CLIF/CLIF-UCMC/rclif/c19/clif_hospitalization.parquet\n",
      "Data loaded successfully from /Users/kavenchhikara/Desktop/CLIF/CLIF-UCMC/rclif/c19/clif_adt.parquet\n"
     ]
    }
   ],
   "source": [
    "patient = pyCLIF.load_data('clif_patient')\n",
    "hospitalization = pyCLIF.load_data('clif_hospitalization')\n",
    "adt = pyCLIF.load_data('clif_adt')\n",
    "\n",
    "# ensure id variable is of dtype character\n",
    "hospitalization['hospitalization_id']= hospitalization['hospitalization_id'].astype(str)\n",
    "patient['patient_id']= patient['patient_id'].astype(str)\n",
    "adt['hospitalization_id']= adt['hospitalization_id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate check\n",
    "\n",
    "If duplicates exist, only the first row is preserved after arranging the data by time. Please check your CLIF tables if there are duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: patient\n",
      "No duplicates found based on columns: ['patient_id'].\n",
      "Processing DataFrame: hospitalization\n",
      "No duplicates found based on columns: ['hospitalization_id'].\n",
      "Processing DataFrame: adt\n",
      "No duplicates found based on columns: ['hospitalization_id', 'hospital_id', 'in_dttm'].\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates\n",
    "# patient table should be unique by patient id\n",
    "patient = pyCLIF.remove_duplicates(patient, ['patient_id'], 'patient')\n",
    "# hospitalization table should be unique by hospitalization id\n",
    "hospitalization = pyCLIF.remove_duplicates(hospitalization, ['hospitalization_id'], 'hospitalization')\n",
    "# adt table should be unique by hospitalization id and in dttm\n",
    "adt = pyCLIF.remove_duplicates(adt, ['hospitalization_id', 'hospital_id', 'in_dttm'], 'adt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of unique encounters in the hospitalization table: 448402\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Number of unique encounters in the hospitalization table: {pyCLIF.count_unique_encounters(hospitalization, 'hospitalization_id')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize all _dttm variables to the same format\n",
    "patient = pyCLIF.standardize_datetime_utc(patient, 'death_dttm')\n",
    "hospitalization = pyCLIF.standardize_datetime_utc(hospitalization, ['admission_dttm', 'discharge_dttm'])\n",
    "adt = pyCLIF.standardize_datetime_utc(adt, ['in_dttm', 'out_dttm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort Identification\n",
    "\n",
    "**Inclusion Criteria:**\n",
    "\n",
    "* Adult admissions between 2020-03-01 and 2022-03-31\n",
    "* Encounters receiving invasive mechanical ventilation during this period\n",
    "\n",
    "**Exclusion criteria:**\n",
    "\n",
    "1. Encounters that were on vent for less than 4 hours in the first 72 hours of first intubation\n",
    "2. Encounters that were on trach in the first 72 hours of first intubation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up a dictionary to keep track of STROBE counts\n",
    "strobe_counts = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (A) Date and Age Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP A: Filter by date range & age ===\n",
      "\n",
      "Number of unique hospitalizations after date & age filter: 164613\n"
     ]
    }
   ],
   "source": [
    "# STEP A: Basic Data Cleaning + Date/Age Filter\n",
    "#   - Filter hospitalization for date range & adult patients\n",
    "#   - Then reduce ADT to those hospitalization_ids\n",
    "\n",
    "\n",
    "print(\"\\n=== STEP A: Filter by date range & age ===\\n\")\n",
    "date_mask = (hospitalization['admission_dttm'] >= '2020-03-01') & \\\n",
    "            (hospitalization['admission_dttm'] <= '2022-03-31')\n",
    "age_mask = (hospitalization['age_at_admission'] >= 18)\n",
    "\n",
    "hospitalization_cohort = hospitalization[date_mask & age_mask].copy()\n",
    "\n",
    "strobe_counts['A_after_date_age_filter'] = hospitalization_cohort['hospitalization_id'].nunique()\n",
    "print(f\"Number of unique hospitalizations after date & age filter: {strobe_counts['A_after_date_age_filter']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (B) Stitch hospitalizations\n",
    "\n",
    "Combine multiple `hospitalization_ids` into a single `encounter_block` for patients who transfer between hospital campuses or return soon after discharge. Hospitalizations that have a gap of **6 hours or less** between the discharge dttm and admission dttm are put in one encounter block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter ADT to only those in the cohort set\n",
    "cohort_ids = hospitalization_cohort['hospitalization_id'].unique().tolist()\n",
    "adt_cohort = adt[adt['hospitalization_id'].isin(cohort_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP B: Stitch encounters ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kavenchhikara/Desktop/CLIF/CLIF-eligibility-for-mobilization/code/pyCLIF.py:503: FutureWarning: The 'downcast' keyword in bfill is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "  hospital_block['encounter_block'] = hospital_block['encounter_block'].bfill(downcast='int')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique hospitalizations before stitching: 164613\n",
      "Number of unique encounter blocks after stitching: 164315\n",
      "Number of linked hospitalization ids: 298\n"
     ]
    }
   ],
   "source": [
    "# STEP B: Stitch Encounters => 'encounter_block'\n",
    "# Use stitch_encounters from pyCLIF with time_interval=6\n",
    "\n",
    "print(\"\\n=== STEP B: Stitch encounters ===\\n\")\n",
    "stitched_cohort = pyCLIF.stitch_encounters(hospitalization_cohort, adt_cohort, time_interval=6)\n",
    "# stitched_cohort now has: 'patient_id','hospitalization_id','encounter_block' etc. This will have duplicate rows because of location category\n",
    "# We only want 1 row per unique encounter_block for the next steps.\n",
    "stitched_unique = stitched_cohort[['patient_id', 'encounter_block']].drop_duplicates()\n",
    "\n",
    "strobe_counts['B_before_stitching'] = stitched_cohort['hospitalization_id'].nunique()\n",
    "strobe_counts['B_after_stitching'] = stitched_unique['encounter_block'].nunique()\n",
    "strobe_counts['B_stitched_hosp_ids'] = strobe_counts['B_before_stitching']-strobe_counts['B_after_stitching']\n",
    "print(f\"Number of unique hospitalizations before stitching: {stitched_cohort['hospitalization_id'].nunique()}\")\n",
    "print(f\"Number of unique encounter blocks after stitching: {strobe_counts['B_after_stitching']}\")\n",
    "print(f\"Number of linked hospitalization ids: {strobe_counts['B_before_stitching']-strobe_counts['B_after_stitching']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (C) Outcome dttm for each encounter block\n",
    "\n",
    "Calulcate final outcome dttm for each encounter block using discharge or death times.   \n",
    "We drop hospitalizations where `discharge_dttm` and `death_dttm` are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP C: Identify final outcome times (discharge vs death) ===\n",
      "\n",
      "Encounter blocks after ensuring each has discharge or death: 164286\n",
      "Hospitalizations without death or discharge dttm: 29. \n",
      " Check output/intermediate/hospitalizations_without_outcomes.csv\n"
     ]
    }
   ],
   "source": [
    "# STEP C: Merge with patient to define discharge/death\n",
    "#    each encounter_blockâ€™s final outcome time\n",
    "\n",
    "print(\"\\n=== STEP C: Identify final outcome times (discharge vs death) ===\\n\")\n",
    "\n",
    "# 1) Merge hospital & patient on hospitalization_id\n",
    "#    (Though weâ€™ll eventually group on encounter_block)\n",
    "tmp_hosp_patient = (\n",
    "    hospitalization_cohort[['hospitalization_id','patient_id','discharge_dttm']]\n",
    "    .merge(patient[['patient_id','death_dttm']], on='patient_id', how='left')\n",
    ")\n",
    "\n",
    "# 2) Drop any row where both discharge_dttm and death_dttm are missing\n",
    "mask_no_discharge_no_death = tmp_hosp_patient['discharge_dttm'].isna() & tmp_hosp_patient['death_dttm'].isna()\n",
    "hosp_no_outcome = tmp_hosp_patient[mask_no_discharge_no_death].copy()\n",
    "tmp_hosp_patient = tmp_hosp_patient[~mask_no_discharge_no_death].copy()\n",
    "\n",
    "# 3) Define final_outcome_dttm and a died_flag\n",
    "tmp_hosp_patient['final_outcome_dttm'] = np.where(\n",
    "    tmp_hosp_patient['death_dttm'].notna(),\n",
    "    tmp_hosp_patient['death_dttm'],\n",
    "    tmp_hosp_patient['discharge_dttm']\n",
    ")\n",
    "tmp_hosp_patient['died_flag'] = np.where(tmp_hosp_patient['death_dttm'].notna(), 1, 0)\n",
    "\n",
    "stitched_cohort_filtered = stitched_cohort[['patient_id', 'hospitalization_id', 'encounter_block',\n",
    "                                            'location_category', 'in_dttm', 'out_dttm', \n",
    "                                            'admission_dttm','discharge_dttm', \n",
    "                                            'hospital_id',]].drop_duplicates()\n",
    "\n",
    "# 4) Merge that into stitched_cohort, so each row knows final_outcome_dttm\n",
    "stitched_merge = stitched_cohort_filtered.merge(\n",
    "    tmp_hosp_patient[['hospitalization_id','final_outcome_dttm','died_flag']],\n",
    "    on='hospitalization_id', how='inner'\n",
    ").drop_duplicates()\n",
    "\n",
    "# 5) Group by encounter_block to find final_outcome at encounter block-level:\n",
    "agg_dict = {\n",
    "    'final_outcome_dttm': 'max',  # the latest if multiple hospitalizations\n",
    "    'died_flag': 'max'            # if any hospitalization was a death\n",
    "}\n",
    "encounter_block_outcome = stitched_merge.groupby('encounter_block').agg(agg_dict).reset_index()\n",
    "encounter_block_outcome.rename(columns={'final_outcome_dttm':'encounter_block_outcome_dttm'}, inplace=True)\n",
    "\n",
    "# 6) Now we have final outcome time at block level\n",
    "strobe_counts['C_final_encounter_blocks_with_outcome'] = encounter_block_outcome['encounter_block'].nunique()\n",
    "strobe_counts['C_hosp_without_outcome'] = hosp_no_outcome['hospitalization_id'].nunique()\n",
    "print(f\"Encounter blocks after ensuring each has discharge or death: {strobe_counts['C_final_encounter_blocks_with_outcome']}\")\n",
    "if len(hosp_no_outcome) > 0:\n",
    "    hosp_no_outcome.to_csv('../output/intermediate/hospitalizations_without_outcomes.csv', index=False)\n",
    "    print(f\"Hospitalizations without death or discharge dttm: {len(hosp_no_outcome)}. \\n Check output/intermediate/hospitalizations_without_outcomes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164584, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping of patient id, hospitalization id and encounter blocks\n",
    "all_ids = stitched_cohort[\n",
    "    stitched_cohort['encounter_block'].isin(encounter_block_outcome['encounter_block'])\n",
    "][['patient_id', 'hospitalization_id', 'encounter_block']].drop_duplicates()\n",
    "all_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A_after_date_age_filter': 164613,\n",
       " 'B_before_stitching': 164613,\n",
       " 'B_after_stitching': 164315,\n",
       " 'B_stitched_hosp_ids': 298,\n",
       " 'C_final_encounter_blocks_with_outcome': 164286,\n",
       " 'C_hosp_without_outcome': 29}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (D) Identify ventilator usage\n",
    "\n",
    "Filter down to encounters that received invasive mechanical ventilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP D: Load & process respiratory support => Identify IMV usage ===\n",
      "\n",
      "Data loaded successfully from /Users/kavenchhikara/Desktop/CLIF/CLIF-UCMC/rclif/c19/clif_respiratory_support.parquet\n"
     ]
    }
   ],
   "source": [
    "# STEP D: Identify Ventilator Usage\n",
    "# Load respiratory support only for the relevant â€œhospitalization_idâ€ set\n",
    "# These hospitalizations map to an encounter_block for final grouping.\n",
    "\n",
    "print(\"\\n=== STEP D: Load & process respiratory support => Identify IMV usage ===\\n\")\n",
    "\n",
    "# 1) Load respiratory support\n",
    "resp_support_raw = pyCLIF.load_data(\n",
    "    'clif_respiratory_support',\n",
    "    columns=rst_required_columns,\n",
    "    filters={'hospitalization_id': all_ids['hospitalization_id'].unique().tolist()}\n",
    ")\n",
    "\n",
    "resp_support = resp_support_raw.copy()\n",
    "resp_support['recorded_dttm'] = pd.to_datetime(resp_support['recorded_dttm'])\n",
    "resp_support = pyCLIF.standardize_datetime_utc(resp_support, 'recorded_dttm') #standardize to utc tz naive\n",
    "resp_support['device_category'] = resp_support['device_category'].str.lower()\n",
    "resp_support['mode_category'] = resp_support['mode_category'].str.lower()\n",
    "resp_support['fio2_set'] = pd.to_numeric(resp_support['fio2_set'], errors='coerce')\n",
    "resp_support['lpm_set'] = pd.to_numeric(resp_support['lpm_set'], errors='coerce')\n",
    "resp_support['resp_rate_set'] = pd.to_numeric(resp_support['resp_rate_set'], errors='coerce')\n",
    "resp_support['peep_set'] = pd.to_numeric(resp_support['peep_set'], errors='coerce')\n",
    "resp_support['resp_rate_obs'] = pd.to_numeric(resp_support['resp_rate_obs'], errors='coerce')\n",
    "\n",
    "# del resp_support_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respiratory Support Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "group_cols = 'device_category'  # or a list like ['device_category','mode_category']\n",
    "numeric_cols = ['fio2_set','peep_set','lpm_set', 'resp_rate_set', 'resp_rate_obs']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    tmp = pyCLIF.create_summary_table(\n",
    "        df=resp_support,\n",
    "        numeric_col=col,\n",
    "        group_by_cols=group_cols\n",
    "    )\n",
    "    # tmp might have columns:\n",
    "    #   ['device_category','N','missing','min','q25','median','q75','mean','max']\n",
    "    # Insert a \"variable\" column next to the group-by columns:\n",
    "    tmp['variable'] = col\n",
    "    # We want \"device_category\" (the group col), then \"variable\", then the rest\n",
    "    if isinstance(group_cols, str):\n",
    "        group_cols_list = [group_cols]  # unify into list\n",
    "    else:\n",
    "        group_cols_list = group_cols  # already a list\n",
    "    # Reorder so that group-by columns come first, then 'variable', then the rest\n",
    "    front_cols = group_cols_list + ['variable']\n",
    "    # Build the list of remaining columns\n",
    "    rest_cols = [c for c in tmp.columns if c not in front_cols]\n",
    "    new_cols = front_cols + rest_cols\n",
    "    tmp = tmp[new_cols]\n",
    "    results_list.append(tmp)\n",
    "\n",
    "# Finally, concatenate all results\n",
    "final_summary_resp_support = pd.concat(results_list, ignore_index=True)\n",
    "final_summary_resp_support.to_csv('../output/final/summary_respiratory_support_by_device.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "group_cols = ['device_category','mode_category']\n",
    "numeric_cols = ['fio2_set','peep_set','lpm_set', 'resp_rate_set', 'resp_rate_obs']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    tmp = pyCLIF.create_summary_table(\n",
    "        df=resp_support,\n",
    "        numeric_col=col,\n",
    "        group_by_cols=group_cols\n",
    "    )\n",
    "    # tmp might have columns:\n",
    "    #   ['device_category','N','missing','min','q25','median','q75','mean','max']\n",
    "    # Insert a \"variable\" column next to the group-by columns:\n",
    "    tmp['variable'] = col\n",
    "    # We want \"device_category\" (the group col), then \"variable\", then the rest\n",
    "    if isinstance(group_cols, str):\n",
    "        group_cols_list = [group_cols]  # unify into list\n",
    "    else:\n",
    "        group_cols_list = group_cols  # already a list\n",
    "    # Reorder so that group-by columns come first, then 'variable', then the rest\n",
    "    front_cols = group_cols_list + ['variable']\n",
    "    # Build the list of remaining columns\n",
    "    rest_cols = [c for c in tmp.columns if c not in front_cols]\n",
    "    new_cols = front_cols + rest_cols\n",
    "    tmp = tmp[new_cols]\n",
    "    results_list.append(tmp)\n",
    "\n",
    "# Finally, concatenate all results\n",
    "final_summary_resp_support = pd.concat(results_list, ignore_index=True)\n",
    "final_summary_resp_support.to_csv('../output/final/summary_respiratory_support_by_device_mode.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating waterfall processing...\n",
      "Creating recorded_date and recorded_hour...\n",
      "Sorting data by 'hospitalization_id' and 'recorded_dttm'...\n",
      "Fixing missing 'device_category' and 'device_name' based on 'mode_category'...\n",
      "Fixing 'device_category' and 'device_name' based on neighboring records...\n",
      "Handling duplicates and removing rows with all key variables missing...\n",
      "Filling forward 'device_category' within each hospitalization...\n",
      "Creating 'device_cat_id' to track changes in 'device_category'...\n",
      "Filling 'device_name' within each 'device_cat_id'...\n",
      "Creating 'device_id' to track changes in 'device_name'...\n",
      "Filling 'mode_category' within each 'device_id'...\n",
      "Creating 'mode_cat_id' to track changes in 'mode_category'...\n",
      "Filling 'mode_name' within each 'mode_cat_id'...\n",
      "Creating 'mode_name_id' to track changes in 'mode_name'...\n",
      "Adjusting 'fio2_set' for 'room air' device_category...\n",
      "Adjusting 'mode_category' for 't-piece' devices...\n",
      "Filling remaining variables within each 'mode_name_id'...\n",
      "Filling 'tracheostomy' forward within each hospitalization...\n",
      "Removing duplicates...\n",
      "Waterfall processing completed.\n"
     ]
    }
   ],
   "source": [
    "# 2) Process with waterfall logic\n",
    "processed_resp_support = pyCLIF.process_resp_support(resp_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Merge to get encounter_block for the cohort identified so far\n",
    "resp_stitched = processed_resp_support.merge(\n",
    "    all_ids[['hospitalization_id','encounter_block']],\n",
    "    on='hospitalization_id', how='right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Apply outlier thresholds ===\n",
      "\n",
      "FIO2_SET mean= 0.4798112288188503 is within the required range\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Apply outlier thresholds ===\\n\")\n",
    "\n",
    "# (Optional) If FiO2 is >1 on average => scale by /100\n",
    "fio2_mean = resp_stitched['fio2_set'].mean(skipna=True)\n",
    "# If the mean is greater than 1, divide 'fio2_set' by 100\n",
    "if fio2_mean and fio2_mean > 1.0:\n",
    "    # Only divide values greater than 1 to avoid re-dividing already correct values\n",
    "    resp_stitched.loc[resp_stitched['fio2_set'] > 1, 'fio2_set'] = \\\n",
    "        resp_stitched.loc[resp_stitched['fio2_set'] > 1, 'fio2_set'] / 100\n",
    "    print(\"Updated fio2_set to be between 0.21 and 1\")\n",
    "else:\n",
    "    print(\"FIO2_SET mean=\", fio2_mean, \"is within the required range\")\n",
    "\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'fio2_set', *outlier_cfg['fio2_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'peep_set', *outlier_cfg['peep_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'lpm_set',  *outlier_cfg['lpm_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'resp_rate_set', *outlier_cfg['resp_rate_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'resp_rate_obs', *outlier_cfg['resp_rate_obs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total IMV respiratory support hospitalizations: 5690\n",
      "Total IMV respiratory support encounter blocks: 5690\n"
     ]
    }
   ],
   "source": [
    "# 4) Identify IMV\n",
    "imv_mask = resp_stitched['device_category'].str.contains(\"imv\", case=False, na=False)\n",
    "resp_stitched_imv = resp_stitched[imv_mask].copy()\n",
    "# this creates a on vent field for everytime the patient is on a vent\n",
    "resp_stitched_imv['on_vent'] = 1\n",
    "\n",
    "\n",
    "strobe_counts['D_imv_hospitalizations'] = resp_stitched_imv['hospitalization_id'].nunique()\n",
    "strobe_counts['D_imv_encounter_blocks'] = resp_stitched_imv['encounter_block'].nunique()\n",
    "\n",
    "print(f\"Total IMV respiratory support hospitalizations: {strobe_counts['D_imv_hospitalizations']}\")\n",
    "print(f\"Total IMV respiratory support encounter blocks: {strobe_counts['D_imv_encounter_blocks']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (E) Vent start and end times \n",
    "\n",
    "Calculate vent start times for the first episode of invasive mechanical intubation.   \n",
    "Limitation: the vent end time might not be associated with the same intubation episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP E: Determine ventilation times (start/end) at hospitalization level and encounter block level ===\n",
      "\n",
      "Unique hospitalizations with valid IMV start/end: 5452\n",
      "Unique encounter blocks with valid IMV start/end: 5452\n"
     ]
    }
   ],
   "source": [
    "# STEP E: Determine Vent Start/End for Each Hospitalization and Encounter block\n",
    "\n",
    "print(\"\\n=== STEP E: Determine ventilation times (start/end) at hospitalization level and encounter block level ===\\n\")\n",
    "\n",
    "# at the hospitalization id level\n",
    "vent_start_end = resp_stitched_imv.groupby('hospitalization_id').agg(\n",
    "    vent_start_time=('recorded_dttm','min'),\n",
    "    vent_end_time=('recorded_dttm','max')\n",
    ").reset_index()\n",
    "\n",
    "# Exclude edge case: if start_time == end_time \n",
    "# these would otherwise have been excluded when we remove encounters on vent for less than 4 hours\n",
    "check_same_vent_start_end = vent_start_end[vent_start_end['vent_start_time'] == vent_start_end['vent_end_time']].copy()\n",
    "vent_start_end= vent_start_end[vent_start_end['vent_start_time'] != vent_start_end['vent_end_time']].copy()\n",
    "\n",
    "strobe_counts['E_hospitalizations_with_valid_vent'] = vent_start_end['hospitalization_id'].nunique()\n",
    "strobe_counts['E_hospitalizations_with_same_vent_start_end'] = check_same_vent_start_end['hospitalization_id'].nunique()\n",
    "print(f\"Unique hospitalizations with valid IMV start/end: {strobe_counts['E_hospitalizations_with_valid_vent']}\")\n",
    "\n",
    "# at the block level\n",
    "block_vent_times = resp_stitched_imv.groupby('encounter_block', dropna=True).agg(\n",
    "    vent_start_time=('recorded_dttm','min'),\n",
    "    vent_end_time=('recorded_dttm','max')\n",
    ").reset_index()\n",
    "\n",
    "# If start==end, no real vent- there was just ONE vent entry, this exclusion can count under \n",
    "block_same_vent = block_vent_times[block_vent_times['vent_start_time']==block_vent_times['vent_end_time']].copy()\n",
    "block_vent_times = block_vent_times[block_vent_times['vent_start_time']!=block_vent_times['vent_end_time']].copy()\n",
    "\n",
    "strobe_counts['E_blocks_with_valid_vent'] = block_vent_times['encounter_block'].nunique()\n",
    "strobe_counts['E_blocks_with_same_vent_start_end'] = block_same_vent['encounter_block'].nunique()\n",
    "print(f\"Unique encounter blocks with valid IMV start/end: {strobe_counts['E_blocks_with_valid_vent']}\")\n",
    "\n",
    "valid_blocks_vent = block_vent_times['encounter_block'].unique()\n",
    "\n",
    "# Filter all_ids to only keep rows where encounter_block is in valid_blocks_vent\n",
    "all_ids = all_ids[all_ids['encounter_block'].isin(valid_blocks_vent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A_after_date_age_filter': 164613,\n",
       " 'B_before_stitching': 164613,\n",
       " 'B_after_stitching': 164315,\n",
       " 'B_stitched_hosp_ids': 298,\n",
       " 'C_final_encounter_blocks_with_outcome': 164286,\n",
       " 'C_hosp_without_outcome': 29,\n",
       " 'D_imv_hospitalizations': 5690,\n",
       " 'D_imv_encounter_blocks': 5690,\n",
       " 'F_hospitalizations_with_valid_vent': 5452,\n",
       " 'F_hospitalizations_with_same_vent_start_end': 238,\n",
       " 'E_blocks_with_valid_vent': 5452,\n",
       " 'E_blocks_with_same_vent_start_end': 238,\n",
       " 'E_hospitalizations_with_valid_vent': 5452,\n",
       " 'E_hospitalizations_with_same_vent_start_end': 238}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (F) Hourly Sequence \n",
    "\n",
    "This section achieves the following important steps:  \n",
    "* Identifies the first and last recorded times for vitals for each encounter block\n",
    "* These times are used to generate an hourly sequence of patients hospitalization journey\n",
    "* Combines with hourly vent usage data from the respiratory support table\n",
    "* Excludes encounters on vent for less than 4 hours in the first 72 hours\n",
    "* Excludes encounters on trach in the first 72 hours \n",
    "* Creates a final dataframe with the identified cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP G: Hourly sequence generation & < 4 hour vent exclusion BLOCK level===\n",
      "\n",
      "Data loaded successfully from /Users/kavenchhikara/Desktop/CLIF/CLIF-UCMC/rclif/c19/clif_vitals.parquet\n"
     ]
    }
   ],
   "source": [
    "# STEP F: Generate Hourly Sequence & Exclude encounter blocks with <4 Vent Hours\n",
    "#  Create an hourly timeline from vent_start to last vital or outcome time for each encounter block\n",
    "# We stop operating at hospitalization id level \n",
    "\n",
    "print(\"\\n=== STEP G: Hourly sequence generation & < 4 hour vent exclusion BLOCK level===\\n\")\n",
    "\n",
    "# 1) define the 'end_time' for the sequence from vitals or outcome.\n",
    "vitals_cohort = pyCLIF.load_data('clif_vitals',\n",
    "    columns=vitals_required_columns,\n",
    "    filters={'hospitalization_id': all_ids['hospitalization_id'].unique().tolist(), \n",
    "             'vital_category': vitals_of_interest}\n",
    ")\n",
    "vitals_cohort['recorded_dttm'] = pd.to_datetime(vitals_cohort['recorded_dttm'])\n",
    "vitals_cohort = pyCLIF.standardize_datetime_utc(vitals_cohort, 'recorded_dttm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_vitals = pyCLIF.create_summary_table(\n",
    "        df=vitals_cohort,\n",
    "        numeric_col='vital_value',\n",
    "        group_by_cols='vital_category'\n",
    "    )\n",
    "summary_vitals.to_csv('../output/final/summary_vitals_by_category.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to get encounter_block on each vital\n",
    "vitals_stitched = vitals_cohort.merge(all_ids, on='hospitalization_id', how='left')\n",
    "# Group by block => find earliest & latest vital for that block\n",
    "vital_bounds_block = vitals_stitched.groupby('encounter_block', dropna=True)['recorded_dttm'].agg(['min','max']).reset_index()\n",
    "vital_bounds_block.columns = ['encounter_block','block_first_vital_dttm','block_last_vital_dttm']\n",
    "\n",
    "# 2) Merge block_vent_times with vital_bounds_block\n",
    "final_blocks = block_vent_times.merge(vital_bounds_block, on='encounter_block', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) If block_last_vital_dttm < vent_start_time => weird edge case. Ideally shouldn't happen. \n",
    "# If such bad blocks exist, check your CLIF tables bro\n",
    "bad_block = final_blocks[final_blocks['block_last_vital_dttm'] < final_blocks['vent_start_time']]\n",
    "if len(bad_block) > 0:\n",
    "    print(\"Warning: Some blocks have last vital < vent start:\\n\", bad_block)\n",
    "else:\n",
    "    print(\"There are no bad blocks! Good job CLIF-ing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zj/jxkvz04s3q55f82vwjbj_5w80000gq/T/ipykernel_92784/2900972920.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  hourly_seq_block = final_blocks.groupby('encounter_block', as_index=False).apply(generate_hourly_sequence_block)\n"
     ]
    }
   ],
   "source": [
    "# 4) Generate the hourly sequence at block level\n",
    "def generate_hourly_sequence_block(row):\n",
    "    blk  = row['encounter_block'].iloc[0]\n",
    "    start_time = row['vent_start_time'].iloc[0]\n",
    "    end_time   = row['block_last_vital_dttm'].iloc[0]\n",
    "    hourly_timestamps = pd.date_range(start=start_time, end=end_time, freq='h')\n",
    "    return pd.DataFrame({\n",
    "        'encounter_block': blk,\n",
    "        'recorded_dttm': hourly_timestamps\n",
    "    })\n",
    "\n",
    "hourly_seq_block = final_blocks.groupby('encounter_block', as_index=False).apply(generate_hourly_sequence_block)\n",
    "hourly_seq_block = hourly_seq_block.reset_index(drop=True)\n",
    "\n",
    "# hourly_seq_block['recorded_dttm'] = hourly_seq_block['recorded_dttm'].dt.tz_convert('UTC')\n",
    "hourly_seq_block['recorded_date'] = hourly_seq_block['recorded_dttm'].dt.date\n",
    "hourly_seq_block['recorded_hour'] = hourly_seq_block['recorded_dttm'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: hour_sequence_check\n",
      "No duplicates found based on columns: ['encounter_block', 'recorded_date', 'recorded_hour'].\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in hourly_seq_block- could be because of DST, so we have converted all tz to UTC\n",
    "# if duplicates exist, check why\n",
    "hourly_seq_block_check = pyCLIF.remove_duplicates(hourly_seq_block, ['encounter_block', 'recorded_date', 'recorded_hour'], \n",
    "                                                  'hour_sequence_check')\n",
    "del hourly_seq_block_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Add time_from_vent & 4-hr â€œcool-offâ€\n",
    "hourly_seq_block['time_from_vent'] = hourly_seq_block.groupby('encounter_block').cumcount()\n",
    "hourly_seq_block['time_from_vent_adjusted'] = np.where(\n",
    "    hourly_seq_block['time_from_vent'] < 4, -1, hourly_seq_block['time_from_vent'] - 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Combine with actual vent usage by hour\n",
    "resp_stitched_imv = resp_stitched_imv[resp_stitched_imv['encounter_block'].isin(all_ids['encounter_block'])]\n",
    "hourly_vent_block = resp_stitched_imv.groupby(['encounter_block','recorded_date','recorded_hour']).agg(\n",
    "    min_fio2_set=('fio2_set','min'),\n",
    "    max_fio2_set=('fio2_set','max'),\n",
    "    min_peep_set=('peep_set','min'),\n",
    "    max_peep_set=('peep_set','max'),\n",
    "    min_lpm_set=('lpm_set', 'min'),\n",
    "    max_lpm_set=('lpm_set', 'max'),\n",
    "    min_resp_rate_obs=('resp_rate_obs', 'min'),\n",
    "    max_resp_rate_obs=('resp_rate_obs', 'max'),\n",
    "    hourly_trach=('tracheostomy', lambda x: 1 if x.max()==1 else 0), # 1 if the any value within that hour is 1\n",
    "    hourly_on_vent=('on_vent','max'),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocks in hourly_seq_block but not in hourly_vent_block: 0\n",
      "\n",
      "Blocks in hourly_vent_block but not in hourly_seq_block: 1\n",
      "[43820]\n"
     ]
    }
   ],
   "source": [
    "# Find encounter_blocks that are in hourly_seq_block but not in hourly_vent_block and vice versa\n",
    "seq_blocks = set(hourly_seq_block['encounter_block'].unique())\n",
    "vent_blocks = set(hourly_vent_block['encounter_block'].unique())\n",
    "\n",
    "blocks_in_seq_not_vent = seq_blocks - vent_blocks\n",
    "blocks_in_vent_not_seq = vent_blocks - seq_blocks\n",
    "\n",
    "print(\"Blocks in hourly_seq_block but not in hourly_vent_block:\", len(blocks_in_seq_not_vent))\n",
    "if len(blocks_in_seq_not_vent) > 0:\n",
    "    print(sorted(list(blocks_in_seq_not_vent)))\n",
    "\n",
    "print(\"\\nBlocks in hourly_vent_block but not in hourly_seq_block:\", len(blocks_in_vent_not_seq))\n",
    "if len(blocks_in_vent_not_seq) > 0:\n",
    "    print(sorted(list(blocks_in_vent_not_seq)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hospitalization_id</th>\n",
       "      <th>recorded_dttm</th>\n",
       "      <th>device_name</th>\n",
       "      <th>device_category</th>\n",
       "      <th>mode_name</th>\n",
       "      <th>mode_category</th>\n",
       "      <th>tracheostomy</th>\n",
       "      <th>fio2_set</th>\n",
       "      <th>lpm_set</th>\n",
       "      <th>resp_rate_set</th>\n",
       "      <th>peep_set</th>\n",
       "      <th>resp_rate_obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>547092</th>\n",
       "      <td>14309874</td>\n",
       "      <td>2021-03-13 22:02:00-06:00</td>\n",
       "      <td>Bag Valve Mask</td>\n",
       "      <td>IMV</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547093</th>\n",
       "      <td>14309874</td>\n",
       "      <td>2021-03-13 22:10:00-06:00</td>\n",
       "      <td>Bag Valve Mask</td>\n",
       "      <td>IMV</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       hospitalization_id             recorded_dttm     device_name  \\\n",
       "547092           14309874 2021-03-13 22:02:00-06:00  Bag Valve Mask   \n",
       "547093           14309874 2021-03-13 22:10:00-06:00  Bag Valve Mask   \n",
       "\n",
       "       device_category mode_name mode_category  tracheostomy  fio2_set  \\\n",
       "547092             IMV      None          None           0.0       NaN   \n",
       "547093             IMV      None          None           0.0       NaN   \n",
       "\n",
       "        lpm_set  resp_rate_set  peep_set  resp_rate_obs  \n",
       "547092      NaN            NaN       NaN            NaN  \n",
       "547093      NaN            NaN       NaN            NaN  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encounter block 43820  hosp= 14309874 not in vitals stitched\n",
    "c = resp_support_raw[resp_support_raw['hospitalization_id']== '14309874']\n",
    "c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>hospitalization_id</th>\n",
       "      <th>admission_dttm</th>\n",
       "      <th>discharge_dttm</th>\n",
       "      <th>age_at_admission</th>\n",
       "      <th>discharge_name</th>\n",
       "      <th>discharge_category</th>\n",
       "      <th>admission_type_name</th>\n",
       "      <th>admission_type_category</th>\n",
       "      <th>zipcode_five_digit</th>\n",
       "      <th>zipcode_nine_digit</th>\n",
       "      <th>census_block_group</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117857</th>\n",
       "      <td>2178952</td>\n",
       "      <td>14309874</td>\n",
       "      <td>2021-03-14 04:02:00</td>\n",
       "      <td>2021-03-14 10:00:00</td>\n",
       "      <td>48.0</td>\n",
       "      <td>Expired</td>\n",
       "      <td>Expired</td>\n",
       "      <td>Non-Health Care Facility Point of Origin</td>\n",
       "      <td>Long Term Care Hospital (LTACH)</td>\n",
       "      <td>60619</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       patient_id hospitalization_id      admission_dttm      discharge_dttm  \\\n",
       "117857    2178952           14309874 2021-03-14 04:02:00 2021-03-14 10:00:00   \n",
       "\n",
       "        age_at_admission discharge_name discharge_category  \\\n",
       "117857              48.0        Expired            Expired   \n",
       "\n",
       "                             admission_type_name  \\\n",
       "117857  Non-Health Care Facility Point of Origin   \n",
       "\n",
       "                admission_type_category zipcode_five_digit zipcode_nine_digit  \\\n",
       "117857  Long Term Care Hospital (LTACH)              60619               None   \n",
       "\n",
       "       census_block_group latitude longitude  \n",
       "117857               None     None      None  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospitalization_cohort[hospitalization_cohort['hospitalization_id']== '14309874']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_block = pd.merge(\n",
    "    hourly_seq_block, \n",
    "    hourly_vent_block,\n",
    "    on=['encounter_block','recorded_date','recorded_hour'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Count how many vent hours per block in the first 72 hours after first intubation,\n",
    "#  Exclude <4 hours on vent in first 72 hours at block level- They cannot meaningfully be studied for early mobilization if theyâ€™re barely intubated.. including them could bias results\n",
    "first_72_hours = final_df_block[(final_df_block['time_from_vent'] >= 0) & (final_df_block['time_from_vent'] < 72)]\n",
    "vent_hours_per_block = first_72_hours.groupby('encounter_block')['hourly_on_vent'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_under_4 = vent_hours_per_block[vent_hours_per_block < 4].index\n",
    "blocks_under_4_df = final_df_block[final_df_block['encounter_block'].isin(blocks_under_4)]\n",
    "final_df_block = final_df_block[~final_df_block['encounter_block'].isin(blocks_under_4)]\n",
    "\n",
    "strobe_counts['G_blocks_with_vent_4_or_more'] = final_df_block['encounter_block'].nunique()\n",
    "strobe_counts['G_blocks_with_vent_less_than_4'] = len(blocks_under_4)\n",
    "print(f\"Unique encounter blocks with valid IMV start/end: {strobe_counts['G_blocks_with_vent_4_or_more']}\")\n",
    "print(f\"Excluded {len(blocks_under_4)} encounter blocks with <4 vent hours in first 72 hours of intubation.\\n\")\n",
    "\n",
    "# 8) Exclude blocks with early trach in first 72\n",
    "trach_flag_block = first_72_hours.groupby('encounter_block')['hourly_trach'].max()\n",
    "blocks_with_trach = trach_flag_block[trach_flag_block==1].index\n",
    "\n",
    "final_df_block = final_df_block[~final_df_block['encounter_block'].isin(blocks_with_trach)]\n",
    "print(f\"Excluded {len(blocks_with_trach)} encounter blocks with trach in first 72 hours of intubation.\\n\")\n",
    "\n",
    "strobe_counts['G_final_blocks_without_trach'] = final_df_block['encounter_block'].nunique()\n",
    "strobe_counts['G_final_blocks_with_trach'] = len(blocks_with_trach)\n",
    "print(f\"Final cohort size (unique blocks) after all exclusions: {strobe_counts['G_final_blocks_without_trach']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(\n",
    "    final_df_block,\n",
    "    all_ids,\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ").reindex(columns=[\n",
    "    'patient_id', 'hospitalization_id', 'encounter_block', \n",
    "    'recorded_dttm', 'recorded_date', 'recorded_hour',\n",
    "    'time_from_vent', 'time_from_vent_adjusted',\n",
    "    'min_fio2_set', 'max_fio2_set', 'min_peep_set', 'max_peep_set',\n",
    "    'min_lpm_set', 'max_lpm_set', 'min_resp_rate_obs', 'max_resp_rate_obs',\n",
    "    'hourly_trach', 'hourly_on_vent'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "key_cols = ['encounter_block', 'recorded_date', 'recorded_hour']\n",
    "duplicates = final_df.duplicated(subset=key_cols).sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = all_ids[all_ids['encounter_block'].isin(final_df['encounter_block'])]\n",
    "all_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get height , weight to calculate bmi\n",
    "# Filter vitals to include only height and weight\n",
    "vitals_bmi = vitals_stitched[vitals_stitched['vital_category'].isin(['weight_kg', 'height_cm'])].copy()\n",
    "\n",
    "# Remove outliers\n",
    "# Extract the min/max from the config\n",
    "min_height, max_height = outlier_cfg['height_cm']\n",
    "min_weight, max_weight = outlier_cfg['weight_kg']\n",
    "\n",
    "# For height rows: set out-of-range to NaN\n",
    "is_height = vitals_bmi['vital_category'] == 'height_cm'\n",
    "height_mask_low  = is_height & (vitals_bmi['vital_value'] < min_height)\n",
    "height_mask_high = is_height & (vitals_bmi['vital_value'] > max_height)\n",
    "vitals_bmi.loc[height_mask_low | height_mask_high, 'vital_value'] = np.nan\n",
    "\n",
    "# For weight rows: set out-of-range to NaN\n",
    "is_weight = vitals_bmi['vital_category'] == 'weight_kg'\n",
    "weight_mask_low  = is_weight & (vitals_bmi['vital_value'] < min_weight)\n",
    "weight_mask_high = is_weight & (vitals_bmi['vital_value'] > max_weight)\n",
    "vitals_bmi.loc[weight_mask_low | weight_mask_high, 'vital_value'] = np.nan\n",
    "\n",
    "# Merge with vent_start_end to get ventilation start time\n",
    "vitals_bmi = vitals_bmi.merge(\n",
    "    block_vent_times[['encounter_block','vent_start_time']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate time difference between recorded_dttm and vent_start_time\n",
    "vitals_bmi['recorded_dttm'] = pd.to_datetime(vitals_bmi['recorded_dttm'])\n",
    "vitals_bmi['vent_start_time'] = pd.to_datetime(vitals_bmi['vent_start_time'])\n",
    "vitals_bmi['time_diff'] = (vitals_bmi['recorded_dttm'] - vitals_bmi['vent_start_time']).dt.total_seconds() / 3600  # in hours\n",
    "\n",
    "# Define whether measurement is before or after vent_start_time\n",
    "vitals_bmi['before_vent_start'] = (vitals_bmi['time_diff'] <= 0).astype(int)\n",
    "\n",
    "# Calculate absolute time difference\n",
    "vitals_bmi['abs_time_diff'] = vitals_bmi['time_diff'].abs()\n",
    "\n",
    "# Sort data to prioritize measurements before vent start and closest in time\n",
    "vitals_bmi = vitals_bmi.sort_values(['encounter_block', 'vital_category', 'before_vent_start', 'abs_time_diff'], \n",
    "                                    ascending=[True, True, False, True])\n",
    "\n",
    "# Drop duplicates to keep the closest measurement for each vital_category per encounter block\n",
    "vitals_bmi = vitals_bmi.drop_duplicates(subset=['encounter_block', 'vital_category'], keep='first')\n",
    "\n",
    "# Pivot to get height and weight per encounter block\n",
    "vitals_bmi_pivot = vitals_bmi.pivot(index='encounter_block', \n",
    "                                    columns='vital_category', \n",
    "                                    values='vital_value'\n",
    "                                    ).reset_index()\n",
    "\n",
    "# Calculate BMI\n",
    "vitals_bmi_pivot['bmi'] = vitals_bmi_pivot['weight_kg'] / ((vitals_bmi_pivot['height_cm'] / 100) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'recorded_dttm' is datetime\n",
    "vitals_stitched['recorded_dttm'] = pd.to_datetime(vitals_stitched['recorded_dttm'])\n",
    "\n",
    "# Extract 'recorded_date' and 'recorded_hour' from recorded_dttm\n",
    "vitals_stitched['recorded_date'] = vitals_stitched['recorded_dttm'].dt.date\n",
    "vitals_stitched['recorded_hour'] = vitals_stitched['recorded_dttm'].dt.hour\n",
    "\n",
    "# Check if 'map' exists\n",
    "if 'map' not in vitals_stitched['vital_category'].unique():\n",
    "    print(\"map is not present, so we'll calculate it...\")\n",
    "    # 1) Filter for sbp & dbp\n",
    "    sbp_dbp = vitals_stitched[vitals_stitched['vital_category'].isin(['sbp','dbp'])].copy()\n",
    "    \n",
    "    # 2) Pivot at the encounter_block + recorded_dttm level\n",
    "    sbp_dbp_pivot = sbp_dbp.pivot_table(\n",
    "        index=['encounter_block','recorded_dttm'],\n",
    "        columns='vital_category',\n",
    "        values='vital_value'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # 3) Drop any row missing sbp or dbp\n",
    "    sbp_dbp_pivot = sbp_dbp_pivot.dropna(subset=['sbp','dbp'])\n",
    "    \n",
    "    # 4) Calculate MAP\n",
    "    sbp_dbp_pivot['map'] = (sbp_dbp_pivot['sbp'] + 2*sbp_dbp_pivot['dbp']) / 3\n",
    "    \n",
    "    # 5) Build a DataFrame for map\n",
    "    map_vitals = sbp_dbp_pivot[['encounter_block','recorded_dttm','map']].copy()\n",
    "    map_vitals['vital_category'] = 'map'\n",
    "    map_vitals['vital_value'] = map_vitals['map']\n",
    "    \n",
    "    # Also add recorded_date/hour\n",
    "    map_vitals['recorded_date'] = map_vitals['recorded_dttm'].dt.date\n",
    "    map_vitals['recorded_hour'] = map_vitals['recorded_dttm'].dt.hour\n",
    "    \n",
    "    # Keep only the needed columns\n",
    "    map_vitals = map_vitals[[\n",
    "        'encounter_block','recorded_dttm','recorded_date','recorded_hour','vital_category','vital_value'\n",
    "    ]]\n",
    "    \n",
    "    # 6) Append 'map' to the main vitals_stitched DataFrame\n",
    "    vitals_stitched = pd.concat([vitals_stitched, map_vitals], ignore_index=True)\n",
    "    print(\"...map was calculated and appended to vitals_stitched.\")\n",
    "\n",
    "\n",
    "#Compute min/max vitals  at the BLOCK level\n",
    "# group by encounter_block + recorded_date + recorded_hour + vital_category\n",
    "vitals_min_max = vitals_stitched.groupby(\n",
    "    ['encounter_block','recorded_date','recorded_hour','vital_category']\n",
    ").agg(\n",
    "    min_val=('vital_value','min'),\n",
    "    max_val=('vital_value','max')\n",
    ").reset_index()\n",
    "\n",
    "# 3) Pivot so each row is unique by (encounter_block, recorded_date, recorded_hour),\n",
    "#    with columns like min_sbp, max_sbp, min_map, max_map, etc.\n",
    "vitals_pivot = vitals_min_max.pivot_table(\n",
    "    index=['encounter_block','recorded_date','recorded_hour'],\n",
    "    columns='vital_category',\n",
    "    values=['min_val','max_val']\n",
    ").reset_index()\n",
    "\n",
    "# Flatten the multi-level columns\n",
    "vitals_pivot.columns = [\n",
    "    '_'.join(col).rstrip('_') if isinstance(col, tuple) else col \n",
    "    for col in vitals_pivot.columns\n",
    "]\n",
    "\n",
    "#  Rename columns for clarity\n",
    "rename_dict = {}\n",
    "for c in vitals_pivot.columns:\n",
    "    if c.startswith('min_val_'):\n",
    "        rename_dict[c] = c.replace('min_val_','min_')\n",
    "    elif c.startswith('max_val_'):\n",
    "        rename_dict[c] = c.replace('max_val_','max_')\n",
    "\n",
    "vitals_pivot = vitals_pivot.rename(columns=rename_dict)\n",
    "\n",
    "# The resulting columns might look like:\n",
    "# ['encounter_block','recorded_date','recorded_hour',\n",
    "#  'min_sbp','max_sbp','min_map','max_map','min_resp_rate','max_resp_rate', etc.]\n",
    "\n",
    "print(\"Finished creating block-level min/max vitals pivot:\")\n",
    "vitals_pivot.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge vitals with final_df\n",
    "final_df = pd.merge(final_df, vitals_pivot, on=['encounter_block', 'recorded_date', 'recorded_hour'], \n",
    "                   how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm duplicates don't exist\n",
    "checkpoint_vitals = pyCLIF.remove_duplicates(final_df, [\n",
    "    'encounter_block','recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_vitals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Meds\n",
    "\n",
    "* Handle med dose unit conversion for all vasoactives\n",
    "* Calculate NE equivalent levels using \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \"vasopressin\", \"dopamine\",  \"angiotensin\"\n",
    "* Create flags for \"nicardipine\", \"nitroprusside\", \"clevidipine\" for the red criteria under consensus criteria\n",
    "* Identify encounters on paralytics - cisatracurium, vecuronium, rocuronium- and create flags for each of these paralytic meds. These patients will not be considered eligible for mobilization during the hour they were receiving paralytic medication. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import clif continuous meds for the cohort on vent during the required time period\n",
    "meds_filters = {\n",
    "    'hospitalization_id': all_ids['hospitalization_id'].unique().tolist(),\n",
    "    'med_category': meds_of_interest\n",
    "}\n",
    "meds = pyCLIF.load_data('clif_medication_admin_continuous', columns=meds_required_columns, filters=meds_filters)\n",
    "print(\"unique encounters in meds\", pyCLIF.count_unique_encounters(meds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure correct format\n",
    "meds['hospitalization_id']= meds['hospitalization_id'].astype(str)\n",
    "meds['med_dose_unit'] = meds['med_dose_unit'].str.lower()\n",
    "meds['admin_dttm'] = pd.to_datetime(meds['admin_dttm'], format='%Y-%m-%d %H:%M:%S')\n",
    "meds['med_dose'] = pd.to_numeric(meds['med_dose'], errors='coerce')\n",
    "# Create 'date' and 'hour_of_day' columns\n",
    "meds['recorded_date'] = meds['admin_dttm'].dt.date\n",
    "meds['recorded_hour'] = meds['admin_dttm'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table for each med_category\n",
    "summary_table = meds.groupby('med_category').agg(\n",
    "    total_N=('med_category', 'size'),\n",
    "    min=('med_dose', 'min'),\n",
    "    max=('med_dose', 'max'),\n",
    "    first_quantile=('med_dose', lambda x: x.quantile(0.25)),\n",
    "    second_quantile=('med_dose', lambda x: x.quantile(0.5)),\n",
    "    third_quantile=('med_dose', lambda x: x.quantile(0.75)),\n",
    "    missing_values=('med_dose', lambda x: x.isna().sum())\n",
    ").reset_index()\n",
    "\n",
    "## check the distrbituon of required continuous meds\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table for each med_category and med_dose_unit combination\n",
    "summary_table = meds.groupby(['med_category', 'med_dose_unit']).agg(\n",
    "    total_N=('med_category', 'size'),\n",
    "    min=('med_dose', 'min'),\n",
    "    max=('med_dose', 'max'),\n",
    "    first_quantile=('med_dose', lambda x: x.quantile(0.25)),\n",
    "    second_quantile=('med_dose', lambda x: x.quantile(0.5)),\n",
    "    third_quantile=('med_dose', lambda x: x.quantile(0.75)),\n",
    "    missing_values=('med_dose', lambda x: x.isna().sum())\n",
    ").reset_index()\n",
    "\n",
    "## check the distrbituon of required continuous meds\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by med_category and med_dose_unit\n",
    "grouped_data = meds.groupby(['med_category', 'med_dose_unit'])\n",
    "\n",
    "# Dynamically determine the number of required subplots\n",
    "n_plots = len(grouped_data.groups.keys())\n",
    "n_cols = 4\n",
    "n_rows = (n_plots + n_cols - 1) // n_cols  # Round up to determine rows\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5))\n",
    "\n",
    "# Flatten the axs array for easier indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Loop through each group and plot the histogram\n",
    "for i, ((med_category, med_dose_unit), group) in enumerate(grouped_data):\n",
    "    ax = axs[i]\n",
    "    ax.hist(group['med_dose'], bins=20, alpha=0.7, label=f\"N = {len(group)}\")\n",
    "    ax.set_title(f\"{med_category} - {med_dose_unit}\")\n",
    "    ax.set_xlabel('Med Dose')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "# Hide any unused axes\n",
    "for j in range(i + 1, len(axs)):\n",
    "    axs[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define medications and their unit conversion information\n",
    "meds_list = [\n",
    "    \"norepinephrine\", \"epinephrine\", \"phenylephrine\",\n",
    "    \"vasopressin\", \"dopamine\", \"angiotensin\", \"metaraminol\"\n",
    "]\n",
    "\n",
    "med_unit_info = {\n",
    "    'norepinephrine': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mcg/kg/min', 'mcg/kg/hr', 'mg/kg/hr', 'mcg/min', 'mg/hr'],\n",
    "    },\n",
    "    'epinephrine': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mcg/kg/min', 'mcg/kg/hr', 'mg/kg/hr', 'mcg/min', 'mg/hr'],\n",
    "    },\n",
    "    'phenylephrine': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mcg/kg/min', 'mcg/kg/hr', 'mg/kg/hr', 'mcg/min', 'mg/hr'],\n",
    "    },\n",
    "    'dopamine': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mcg/kg/min', 'mcg/kg/hr', 'mg/kg/hr', 'mcg/min', 'mg/hr'],\n",
    "    },\n",
    "    'metaraminol': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mg/hr', 'mcg/min'],\n",
    "    },\n",
    "    'angiotensin': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['ng/kg/min', 'ng/kg/hr'],\n",
    "    },\n",
    "    'vasopressin': {\n",
    "        'required_unit': 'units/min',\n",
    "        'acceptable_units': ['units/min', 'units/hr', 'milliunits/min', 'milliunits/hr'],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the med_dose_unit for each med_category in the meds table\n",
    "med_dose_unit_check = meds.groupby(['med_category', 'med_dose_unit']).size().reset_index(name='count')\n",
    "\n",
    "# Create a new column to flag invalid dose units\n",
    "def check_dose_unit(row):\n",
    "    med_category = row['med_category']\n",
    "    med_dose_unit = row['med_dose_unit']\n",
    "    # Check if med_category exists in med_unit_info\n",
    "    if med_category in med_unit_info:\n",
    "        # Check if med_dose_unit is in the acceptable units\n",
    "        if med_dose_unit in med_unit_info[med_category]['acceptable_units']:\n",
    "            return \"Valid\"\n",
    "        else:\n",
    "            return \"Not an acceptable unit\"\n",
    "    else:\n",
    "        return \"Not a vasoactive\"\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "med_dose_unit_check['unit_validity'] = med_dose_unit_check.apply(check_dose_unit, axis=1)\n",
    "\n",
    "# # Optional: Filter for invalid units\n",
    "invalid_units = med_dose_unit_check[med_dose_unit_check['unit_validity'] == 'Not an acceptable unit']\n",
    "print(\"Invalid units. These will be dropped:\\n\")\n",
    "print(invalid_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Norepinephrine equivalent calculation\n",
    "# Goradia S, Sardaneh AA, Narayan SW, Penm J, Patanwala AE. Vasopressor dose equivalence: \n",
    "# A scoping review and suggested formula. J Crit Care. 2021 Feb;61:233-240. doi: 10.1016/j.jcrc.2020.11.002. Epub 2020 Nov 14. PMID: 33220576.\n",
    "\n",
    "meds_list = [\n",
    "    \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \n",
    "    \"vasopressin\", \"dopamine\",  \n",
    "    \"angiotensin\"\n",
    "]\n",
    "\n",
    "# Function to check if 'med_dose_unit' contains '/hr' or '/min'\n",
    "def has_per_hour_or_min(unit):\n",
    "    if pd.isnull(unit):\n",
    "        return False\n",
    "    unit = unit.lower()\n",
    "    return '/hr' in unit or '/min' in unit\n",
    "\n",
    "# Filter meds to include only rows with '/hr' or '/min' in 'med_dose_unit'\n",
    "meds_filtered = meds[meds['med_dose_unit'].apply(has_per_hour_or_min)].copy()\n",
    "\n",
    "ne_df = meds_filtered[meds_filtered['med_category'].isin(meds_list)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **2. Convert Medication Doses to Required Units**\n",
    "### Function to get conversion factor for each medication\n",
    "def get_conversion_factor(med_category, med_dose_unit, weight_kg):\n",
    "    med_info = med_unit_info.get(med_category, None)\n",
    "    if not med_info:\n",
    "        # Medication not in the list\n",
    "        return None\n",
    "    required_unit = med_info['required_unit']\n",
    "    acceptable_units = med_info['acceptable_units']\n",
    "    med_dose_unit = med_dose_unit.lower()\n",
    "    if med_category in ['norepinephrine', 'epinephrine', 'phenylephrine', 'dopamine', 'metaraminol']:\n",
    "        # Required unit: mcg/kg/min\n",
    "        if med_dose_unit == 'mcg/kg/min':\n",
    "            factor = 1.0\n",
    "        elif med_dose_unit == 'mcg/kg/hr':\n",
    "            factor = 1 / 60\n",
    "        elif med_dose_unit == 'mg/kg/hr':\n",
    "            factor = 1000 / 60\n",
    "        elif med_dose_unit == 'mcg/min':\n",
    "            factor = 1 / weight_kg\n",
    "        elif med_dose_unit == 'mg/hr':\n",
    "            factor = 1000 / 60 / weight_kg\n",
    "        else:\n",
    "            return None\n",
    "    elif med_category == 'angiotensin':\n",
    "        # Required unit: mcg/kg/min\n",
    "        if med_dose_unit == 'ng/kg/min':\n",
    "            factor = 1 / 1000\n",
    "        elif med_dose_unit == 'ng/kg/hr':\n",
    "            factor = 1 / 1000 / 60\n",
    "        else:\n",
    "            return None\n",
    "    elif med_category == 'vasopressin':\n",
    "        # Required unit: units/min\n",
    "        if med_dose_unit == 'units/min':\n",
    "            factor = 1.0\n",
    "        elif med_dose_unit == 'units/hr':\n",
    "            factor = 1 / 60\n",
    "        elif med_dose_unit == 'milliunits/min':\n",
    "            factor = 1 / 1000\n",
    "        elif med_dose_unit == 'milliunits/hr':\n",
    "            factor = 1 / 1000 / 60\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "    return factor\n",
    "\n",
    "# Merge weight_kg into meds_filtered (assuming 'vitals_bmi_pivot' is available)\n",
    "meds_filtered = meds_filtered.merge(vitals_bmi_pivot[['hospitalization_id', 'weight_kg']], on='hospitalization_id', how='left')\n",
    "\n",
    "# Remove rows with missing weight_kg\n",
    "meds_filtered = meds_filtered[~meds_filtered['weight_kg'].isnull()].copy()\n",
    "\n",
    "# Function to convert doses\n",
    "def convert_dose(row):\n",
    "    med_category = row['med_category']\n",
    "    med_dose = row['med_dose']\n",
    "    med_dose_unit = row['med_dose_unit']\n",
    "    weight_kg = row['weight_kg']\n",
    "    factor = get_conversion_factor(med_category, med_dose_unit, weight_kg)\n",
    "    if factor is None:\n",
    "        return np.nan\n",
    "    return med_dose * factor\n",
    "\n",
    "# Apply the conversion to get 'med_dose_converted'\n",
    "meds_filtered['med_dose_converted'] = meds_filtered.apply(convert_dose, axis=1)\n",
    "\n",
    "# Drop rows with NaN in 'med_dose_converted' (unrecognized units)\n",
    "meds_filtered = meds_filtered[~meds_filtered['med_dose_converted'].isnull()].copy()\n",
    "\n",
    "# Define acceptable dose ranges\n",
    "med_dose_ranges = {\n",
    "    'norepinephrine': (0.01, 3),\n",
    "    'epinephrine': (0.01, 0.1),\n",
    "    'phenylephrine': (0.1, 5),\n",
    "    'dopamine': (2, 20),\n",
    "    'metaraminol': (0.5, 10),  \n",
    "    'angiotensin': (0.02, 0.2), \n",
    "    'vasopressin': (0.01, 0.1),  \n",
    "}\n",
    "\n",
    "# Function to check if dose is within range\n",
    "def is_dose_within_range(row, outlier_dict):\n",
    "    '''\n",
    "    Check if med_dose_converted is within the outlier-configured range for this med_category.\n",
    "    Parameters:\n",
    "        row (pd.Series): A row from a DataFrame, must include 'med_category' and 'med_dose_converted'.\n",
    "        outlier_dict (dict): Dictionary of min/max pairs from outlier_config.json.\n",
    "    Returns:\n",
    "        bool: True if the dose is within range or if med_category is not found, False otherwise.\n",
    "    '''\n",
    "    med_category = row['med_category']\n",
    "    med_dose_converted = row['med_dose_converted']\n",
    "    dose_range = outlier_dict.get(med_category, None)\n",
    "    if dose_range is None:\n",
    "        return False\n",
    "    min_dose, max_dose = dose_range\n",
    "    return min_dose <= med_dose_converted <= max_dose\n",
    "\n",
    "# Filter doses within acceptable ranges\n",
    "meds_filtered = meds_filtered[meds_filtered.apply(is_dose_within_range, axis=1, args=(outlier_cfg,))].copy()\n",
    "\n",
    "# **4. Flag Medications Not in the Dataset**\n",
    "\n",
    "for med in meds_list:\n",
    "    if med not in meds_filtered['med_category'].unique():\n",
    "        print(f\"{med} is not in the dataset.\")\n",
    "\n",
    "# Pivot and Aggregate the Data**\n",
    "\n",
    "# Create 'recorded_date' and 'recorded_hour' columns\n",
    "meds_filtered['admin_dttm'] = pd.to_datetime(meds_filtered['admin_dttm'])\n",
    "meds_filtered['recorded_date'] = meds_filtered['admin_dttm'].dt.date\n",
    "meds_filtered['recorded_hour'] = meds_filtered['admin_dttm'].dt.hour\n",
    "\n",
    "# Group and aggregate doses\n",
    "group_cols = ['hospitalization_id', 'recorded_date', 'recorded_hour', 'med_category']\n",
    "dose_agg = meds_filtered.groupby(group_cols)['med_dose_converted'].agg(['min', 'max']).reset_index()\n",
    "\n",
    "# Pivot to have medications as columns\n",
    "dose_pivot_min = dose_agg.pivot_table(index=['hospitalization_id', 'recorded_date', 'recorded_hour'], columns='med_category', values='min').reset_index()\n",
    "dose_pivot_max = dose_agg.pivot_table(index=['hospitalization_id', 'recorded_date', 'recorded_hour'], columns='med_category', values='max').reset_index()\n",
    "\n",
    "# Rename columns to indicate min and max\n",
    "dose_pivot_min.columns = ['hospitalization_id', 'recorded_date', 'recorded_hour'] + ['min_' + col for col in dose_pivot_min.columns if col not in ['hospitalization_id', 'recorded_date', 'recorded_hour']]\n",
    "dose_pivot_max.columns = ['hospitalization_id', 'recorded_date', 'recorded_hour'] + ['max_' + col for col in dose_pivot_max.columns if col not in ['hospitalization_id', 'recorded_date', 'recorded_hour']]\n",
    "\n",
    "# Merge min and max DataFrames\n",
    "dose_pivot = pd.merge(dose_pivot_min, dose_pivot_max, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], how='outer')\n",
    "\n",
    "# **6. Calculate Norepinephrine Equivalents**\n",
    "\n",
    "# Replace NaN with 0 for calculations\n",
    "dose_pivot.fillna(0, inplace=True)\n",
    "\n",
    "# Calculate NE min\n",
    "dose_pivot['ne_calc_min'] = (\n",
    "    dose_pivot.get('min_norepinephrine', 0) +\n",
    "    dose_pivot.get('min_epinephrine', 0) +\n",
    "    dose_pivot.get('min_phenylephrine', 0) / 10 +\n",
    "    dose_pivot.get('min_dopamine', 0) / 100 +\n",
    "    dose_pivot.get('min_metaraminol', 0) / 8 +\n",
    "    dose_pivot.get('min_vasopressin', 0) * 2.5 +\n",
    "    dose_pivot.get('min_angiotensin', 0) * 10\n",
    ")\n",
    "\n",
    "# Calculate NE max\n",
    "dose_pivot['ne_calc_max'] = (\n",
    "    dose_pivot.get('max_norepinephrine', 0) +\n",
    "    dose_pivot.get('max_epinephrine', 0) +\n",
    "    dose_pivot.get('max_phenylephrine', 0) / 10 +\n",
    "    dose_pivot.get('max_dopamine', 0) / 100 +\n",
    "    dose_pivot.get('max_metaraminol', 0) / 8 +\n",
    "    dose_pivot.get('max_vasopressin', 0) * 2.5 +\n",
    "    dose_pivot.get('max_angiotensin', 0) * 10\n",
    ")\n",
    "\n",
    "# **7. Prepare the Final Dataset**\n",
    "# Keep only the required columns\n",
    "ne_calc_df = dose_pivot[['hospitalization_id', 'recorded_date', \n",
    "                         'recorded_hour', \n",
    "                         'ne_calc_min', 'ne_calc_max']].drop_duplicates(subset=['hospitalization_id', 'recorded_date', 'recorded_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, ne_calc_df, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_meds_list = [\n",
    "    \"nicardipine\", \"nitroprusside\", \"clevidipine\"\n",
    "]\n",
    "\n",
    "# Filter meds_filtered for the medications in red_meds_list\n",
    "red_meds_df = meds[meds['med_category'].isin(red_meds_list)].copy()\n",
    "\n",
    "# Create a flag for each medication in red_meds_list\n",
    "for med in red_meds_list:\n",
    "    # Create a flag that is 1 if the medication was administered in that hour, 0 otherwise\n",
    "    red_meds_df[med + '_flag'] = np.where(red_meds_df['med_category'] == med, 1, 0).astype(int)\n",
    "\n",
    "# Aggregate to get the maximum value for each flag (per hospitalization_id, recorded_date, recorded_hour)\n",
    "# This ensures that if the medication was administered even once in the hour, the flag is 1\n",
    "red_meds_flags = red_meds_df.groupby(['hospitalization_id', 'recorded_date', 'recorded_hour']).agg(\n",
    "    {med + '_flag': 'max' for med in red_meds_list}\n",
    ").reset_index()\n",
    "\n",
    "#  combine all flags into a single 'red_meds_flag', you can do so like this:\n",
    "red_meds_flags['red_meds_flag'] = red_meds_flags[[med + '_flag' for med in red_meds_list]].max(axis=1)\n",
    "\n",
    "# Select the relevant columns\n",
    "red_meds_flags_final = red_meds_flags[[\n",
    "    'hospitalization_id', 'recorded_date', 'recorded_hour',\n",
    "    'nicardipine_flag', 'nitroprusside_flag',\n",
    "    'clevidipine_flag', 'red_meds_flag'\n",
    "]].drop_duplicates(subset=['hospitalization_id', 'recorded_date', 'recorded_hour'])\n",
    "\n",
    "red_meds_flags_final['nicardipine_flag'] = red_meds_flags_final['nicardipine_flag'].astype(int)\n",
    "red_meds_flags_final['nitroprusside_flag'] = red_meds_flags_final['nitroprusside_flag'].astype(int)\n",
    "red_meds_flags_final['clevidipine_flag'] = red_meds_flags_final['clevidipine_flag'].astype(int)\n",
    "red_meds_flags_final['red_meds_flag'] = red_meds_flags_final['red_meds_flag'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, red_meds_flags_final, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralytics_list = [\n",
    "    \"cisatracurium\", \"vecuronium\", \"rocuronium\" \n",
    "]\n",
    "\n",
    "# Filter meds_filtered for the medications in paralytics_list\n",
    "paralytics_df = meds[meds['med_category'].isin(paralytics_list)].copy()\n",
    "\n",
    "# Create a flag for each medication in paralytics_list\n",
    "for med in paralytics_list:\n",
    "    # Create a flag that is 1 if the medication was administered in that hour, 0 otherwise\n",
    "    paralytics_df[med + '_flag'] = np.where(paralytics_df['med_category'] == med, 1, 0).astype(int)\n",
    "\n",
    "# Aggregate to get the maximum value for each flag (per hospitalization_id, recorded_date, recorded_hour)\n",
    "# This ensures that if the medication was administered even once in the hour, the flag is 1\n",
    "paralytics_flags = paralytics_df.groupby(['hospitalization_id', 'recorded_date', 'recorded_hour']).agg(\n",
    "    {med + '_flag': 'max' for med in paralytics_list}\n",
    ").reset_index()\n",
    "\n",
    "#  combine all flags into a single 'paralytics_flag', you can do so like this:\n",
    "paralytics_flags['paralytics_flag'] = paralytics_flags[[med + '_flag' for med in paralytics_list]].max(axis=1)\n",
    "\n",
    "# Select the relevant columns\n",
    "paralytics_flags_final = paralytics_flags[[\n",
    "    'hospitalization_id', 'recorded_date', 'recorded_hour',\n",
    "    'cisatracurium_flag', 'vecuronium_flag',\n",
    "    'rocuronium_flag', 'paralytics_flag'\n",
    "]].drop_duplicates(subset=['hospitalization_id', 'recorded_date', 'recorded_hour'])\n",
    "\n",
    "paralytics_flags_final['cisatracurium_flag'] = paralytics_flags_final['cisatracurium_flag'].astype(int)\n",
    "paralytics_flags_final['vecuronium_flag'] = paralytics_flags_final['vecuronium_flag'].astype(int)\n",
    "paralytics_flags_final['rocuronium_flag'] = paralytics_flags_final['rocuronium_flag'].astype(int)\n",
    "paralytics_flags_final['paralytics_flag'] = paralytics_flags_final['paralytics_flag'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, paralytics_flags_final, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Lab\n",
    "\n",
    "Get most recent lactate defined as closest lab result time to the start of first intubation event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import clif continuous meds and clif labs table for the cohort on vent during the required time period\n",
    "labs_filters = {\n",
    "    'hospitalization_id': cohort_ids,\n",
    "    'lab_category': labs_of_interest\n",
    "}\n",
    "labs = pyCLIF.load_data('clif_labs', columns=labs_required_columns, filters=labs_filters)\n",
    "print(\"unique encounters in labs\", pyCLIF.count_unique_encounters(labs))\n",
    "labs['hospitalization_id']= labs['hospitalization_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs['lab_result_dttm'] = pd.to_datetime(labs['lab_result_dttm'])\n",
    "labs['recorded_hour'] = labs['lab_result_dttm'].dt.hour\n",
    "labs['recorded_date'] = labs['lab_result_dttm'].dt.date\n",
    "\n",
    "lactate_df = pd.merge(labs, vent_start_end, on='hospitalization_id', how='left')\n",
    "lactate_df['time_since_vent_start_hours'] = (\n",
    "    (lactate_df['lab_result_dttm'] - lactate_df['vent_start_time']).dt.total_seconds() / 3600\n",
    ")\n",
    "\n",
    "# Calculate the absolute time difference between lab_result_dttm and vent_start_time in hours\n",
    "lactate_df['time_diff_hours'] = abs((lactate_df['lab_result_dttm'] - lactate_df['vent_start_time']).dt.total_seconds() / 3600)\n",
    "\n",
    "# Filter for observations within the first 72 hours since vent_start_time\n",
    "lactate_df = lactate_df[(lactate_df['time_since_vent_start_hours'] >= 0) & \n",
    "                        (lactate_df['time_since_vent_start_hours'] <= 72)]\n",
    "\n",
    "# Sort by hospitalization_id, recorded_hour, and time_diff_hours to find the closest measurement to vent_start_time\n",
    "lactate_df = lactate_df.sort_values(by=['hospitalization_id', 'recorded_date', 'recorded_hour', 'time_diff_hours'])\n",
    "\n",
    "# Group by hospitalization_id and recorded_hour, and get the first row in each group (which is the closest measurement)\n",
    "# closest lactate measurement is defined as closest to the vent_start_time in that hour. \n",
    "closest_lactate_df = lactate_df.groupby(['hospitalization_id', 'recorded_date','recorded_hour']).first().reset_index()\n",
    "\n",
    "labs_final = closest_lactate_df[['hospitalization_id', 'recorded_date', 'recorded_hour', 'lab_value_numeric']].copy()\n",
    "\n",
    "# Rename the 'lab_value_numeric' column to 'lactate'\n",
    "labs_final = labs_final.rename(columns={'lab_value_numeric': 'lactate'})\n",
    "\n",
    "final_df = pd.merge(final_df, labs_final, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], \n",
    "                   how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_labs= pyCLIF.remove_duplicates(final_df, [\n",
    "    'hospitalization_id', 'recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write analysis dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_parquet('../output/intermediate/final_df.parquet')\n",
    "vent_start_end.to_parquet('../output/intermediate/vent_start_end.parquet')\n",
    "final_df['hospitalization_id'].to_csv('../output/intermediate/cohort_ids.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mobilization)",
   "language": "python",
   "name": ".mobilization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
