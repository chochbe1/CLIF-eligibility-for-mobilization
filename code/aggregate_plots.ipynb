{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main directory path\n",
    "main_directory = \"/Users/kavenchhikara/Library/CloudStorage/Box-Box/CLIF/projects/CLIF-eligibility-for-mobilization\"  \n",
    "\n",
    "# Verify the directory exists\n",
    "if not os.path.isdir(main_directory):\n",
    "    print(f\"Error: The directory '{main_directory}' does not exist.\")\n",
    "else:\n",
    "    print(f\"Main directory found: {main_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributing sites details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from directory names to full university names\n",
    "DIR_TO_UNIVERSITY = {\n",
    "    'hopkins': \"Johns Hopkins University\",\n",
    "    'nu': \"Northwestern University\",\n",
    "    'ohsu': \"Oregon Health & Science University\",\n",
    "    'ucmc': \"University of Chicago\",\n",
    "    'Michigan': \"University of Michigan\",\n",
    "    'rush': \"Rush University\",\n",
    "    'umn': \"University of Minnesota\",\n",
    "    'penn': \"University of Pennsylvania\",\n",
    "    'MIMIC': \"MIMIC IV\"\n",
    "}\n",
    "\n",
    "# Mapping from directory names to site names used in filenames\n",
    "DIR_TO_SITE_NAME = {\n",
    "    'hopkins': \"Hopkins\",\n",
    "    'nu': \"NU\",\n",
    "    'ohsu': \"ohsu\",\n",
    "    'ucmc': \"UCMC\",\n",
    "    'Michigan': \"U Michigan\",\n",
    "    'rush': \"RUSH\",\n",
    "    'umn': \"UMN\",\n",
    "    'penn': \"Penn\",\n",
    "    'MIMIC': \"MIMIC\"\n",
    "}\n",
    "\n",
    "# Mapping from full university names to official colors (Hex codes)\n",
    "UNIVERSITY_COLORS = {\n",
    "    \"Johns Hopkins University\": \"#000000\",          \n",
    "    \"Northwestern University\": \"#4E2A84\",           \n",
    "    \"Oregon Health & Science University\": \"#5D98CA\", \n",
    "    \"University of Chicago\": \"#800000\",              \n",
    "    \"University of Michigan\": \"#0000F5\",            \n",
    "    \"Rush University\": \"#006747\",                     \n",
    "    \"University of Minnesota\": \"#F9D849\",           \n",
    "    \"University of Pennsylvania\": \"#FF0000\",\n",
    "    \"MIMIC IV\": \"#AA336A\",           \n",
    "}\n",
    "\n",
    "# Criteria types and corresponding keywords in filenames\n",
    "CRITERIA_TYPES = {\n",
    "    'patel': 'Patel Criteria',\n",
    "    'team': 'TEAM Criteria',\n",
    "    'yellow': 'Consensus Guidelines Criteria'\n",
    "}\n",
    "\n",
    "# Mapping from site_name to university names\n",
    "site_to_university = {v: DIR_TO_UNIVERSITY[k] for k, v in DIR_TO_SITE_NAME.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate cumulative incidence plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_file_path(site_dir, criteria, site_name, main_dir):\n",
    "    \"\"\"\n",
    "    Constructs the file path for a given site and criteria.\n",
    "    \n",
    "    Parameters:\n",
    "        site_dir (str): Subdirectory name (e.g., 'Michigan').\n",
    "        criteria (str): Criteria type ('patel', 'team', 'consensus guidelines ').\n",
    "        site_name (str): Site name used in the filename (e.g., 'U Michigan').\n",
    "        main_dir (str): Path to the main directory.\n",
    "    \n",
    "    Returns:\n",
    "        str or None: Full path to the CSV file or None if not found.\n",
    "    \"\"\"\n",
    "    # Define the expected filename pattern with wildcard for timestamp\n",
    "    pattern = f'cif_b_hours_{criteria}_{site_name}_*.csv'\n",
    "    search_path = os.path.join(main_dir, site_dir, pattern)\n",
    "    matched_files = glob.glob(search_path)\n",
    "    \n",
    "    if not matched_files:\n",
    "        print(f\"    Warning: No file found for criteria '{criteria}' in site '{site_dir}'.\")\n",
    "        return None\n",
    "    elif len(matched_files) > 1:\n",
    "        print(f\"    Warning: Multiple files found for criteria '{criteria}' in site '{site_dir}'. Using the first match.\")\n",
    "    \n",
    "    # Return the first matched file\n",
    "    return matched_files[0]\n",
    "\n",
    "def read_and_process_file(file_path, criterion_type, university_name):\n",
    "    \"\"\"\n",
    "    Reads a CSV file and processes it for aggregation.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        criterion_type (str): Human-readable criterion type (e.g., 'Patel Criteria').\n",
    "        university_name (str): Full name of the university.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with standardized column names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"      Reading file: {file_path}\")\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if the file has exactly four columns\n",
    "        if df.shape[1] != 4:\n",
    "            print(f\"      Error: Expected 4 columns in {file_path}, but found {df.shape[1]}. Skipping this file.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Assign standard column names regardless of original names\n",
    "        df.columns = ['timeline', 'value', 'criteria', 'site']\n",
    "        \n",
    "        # Optional: Display the first few rows to verify\n",
    "        # display(df.head())\n",
    "        \n",
    "        # Add additional columns for aggregation\n",
    "        df['criterion_type'] = criterion_type\n",
    "        df['university'] = university_name\n",
    "        \n",
    "        # Select relevant columns for aggregation\n",
    "        df = df[['timeline', 'value', 'criterion_type', 'university']]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"      Error reading {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def aggregate_all_sites(main_dir):\n",
    "    \"\"\"\n",
    "    Aggregates data from all site directories.\n",
    "    \n",
    "    Parameters:\n",
    "        main_dir (str): Path to the main directory containing site subdirectories.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Aggregated DataFrame containing data from all sites and criteria.\n",
    "    \"\"\"\n",
    "    aggregated_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through each site directory\n",
    "    for site_dir, university_name in DIR_TO_UNIVERSITY.items():\n",
    "        site_name_in_file = DIR_TO_SITE_NAME.get(site_dir, \"\")\n",
    "        \n",
    "        if not site_name_in_file:\n",
    "            print(f\"\\n  Warning: No site name mapping found for directory '{site_dir}'. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing site: {university_name} ({site_dir})\")\n",
    "        \n",
    "        for criteria_key, criterion_type in CRITERIA_TYPES.items():\n",
    "            print(f\"  Processing criteria: {criterion_type}\")\n",
    "            file_path = construct_file_path(site_dir, criteria_key, site_name_in_file, main_dir)\n",
    "            \n",
    "            if file_path:\n",
    "                df = read_and_process_file(file_path, criterion_type, university_name)\n",
    "                \n",
    "                if not df.empty:\n",
    "                    aggregated_df = pd.concat([aggregated_df, df], ignore_index=True)\n",
    "                else:\n",
    "                    print(f\"  Warning: DataFrame is empty for file '{file_path}'.\")\n",
    "            else:\n",
    "                print(f\"  Warning: File for criteria '{criteria_key}' not found in site '{site_dir}'.\")\n",
    "    \n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data from all sites\n",
    "aggregated_business_hours = aggregate_all_sites(main_directory)\n",
    "\n",
    "# Check if data was aggregated\n",
    "if aggregated_business_hours.empty:\n",
    "    print(\"No data was aggregated. Please check the directory structure and file naming conventions.\")\n",
    "else:\n",
    "    print(f\"\\nTotal aggregated records: {len(aggregated_business_hours)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAggregated Data Preview:\")\n",
    "display(aggregated_business_hours.head())\n",
    "\n",
    "# Display information about the DataFrame\n",
    "print(\"\\nAggregated Data Info:\")\n",
    "aggregated_business_hours.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aggregated_data(aggregated_df):\n",
    "    \"\"\"\n",
    "    Plots the aggregated data in three panels, one for each criterion,\n",
    "    with y-axis fixed from 0 to 1 and a legend positioned below the plots.\n",
    "    \n",
    "    Parameters:\n",
    "        aggregated_df (pd.DataFrame): Aggregated DataFrame containing all sites and criteria.\n",
    "    \"\"\"\n",
    "    # Ensure 'timeline' is numeric\n",
    "    aggregated_df['timeline'] = pd.to_numeric(aggregated_df['timeline'], errors='coerce')\n",
    "    \n",
    "    # Drop rows with NaN 'timeline' or 'value'\n",
    "    aggregated_df = aggregated_df.dropna(subset=['timeline', 'value'])\n",
    "    \n",
    "    # Sort by 'timeline'\n",
    "    aggregated_df = aggregated_df.sort_values('timeline')\n",
    "    \n",
    "    # Initialize a FacetGrid with three columns for each criterion\n",
    "    g = sns.FacetGrid(\n",
    "        aggregated_df, \n",
    "        col='criterion_type', \n",
    "        sharey=True,  # Share y-axis to ensure consistent scaling\n",
    "        height=6, \n",
    "        aspect=1.2\n",
    "    )\n",
    "    \n",
    "    # Define a palette based on university colors\n",
    "    palette = UNIVERSITY_COLORS\n",
    "    \n",
    "    # Plot each facet without an internal legend\n",
    "    g.map_dataframe(\n",
    "        sns.lineplot, \n",
    "        x='timeline', \n",
    "        y='value', \n",
    "        hue='university', \n",
    "        palette=palette,\n",
    "        legend=False  # Disable internal legends\n",
    "    )\n",
    "    \n",
    "    # Create custom legend handles\n",
    "    custom_handles = [\n",
    "        Line2D([0], [0], color=color, lw=2) for color in palette.values()\n",
    "    ]\n",
    "    custom_labels = list(palette.keys())\n",
    "    \n",
    "    # Add a single legend below the plots\n",
    "    g.fig.legend(\n",
    "        handles=custom_handles, \n",
    "        labels=custom_labels, \n",
    "        # title='Hospitals', \n",
    "        loc='lower center', \n",
    "        ncol=5, \n",
    "        bbox_to_anchor=(0.5, 0)  # Position the legend below the plots\n",
    "    )\n",
    "    \n",
    "    # Adjust the layout to make space for the legend\n",
    "    plt.subplots_adjust(top=0.85, bottom=0.25, right=0.9)  # Increase the bottom margin for the legend\n",
    "    \n",
    "    # Set titles and labels\n",
    "    g.set_titles(\"{col_name}\")\n",
    "    g.set_axis_labels(\"Timeline (Business Hours)\", \"Cumulative Incidence Probability\")\n",
    "    \n",
    "    # Set y-axis limits from 0 to 1 for all plots\n",
    "    g.set(ylim=(0, 1))\n",
    "    \n",
    "    # Set a super title for the entire figure\n",
    "    # plt.suptitle('Cumulative Incidence Function for Time to First Eligibility Across CLIF Sites', fontsize=16)\n",
    "    \n",
    "    # Increase font size in the graph to 20\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure seaborn aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "plot_aggregated_data(aggregated_business_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_business_hours.value_counts('university')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of universities to their number of subjects\n",
    "n_mapping = {\n",
    "    'Johns Hopkins University': 7676,\n",
    "    'University of Chicago': 4200,\n",
    "    'University of Michigan': 3826,\n",
    "    'University of Pennsylvania': 6747,\n",
    "    'Rush University': 2778,\n",
    "    'University of Minnesota': 6021,\n",
    "    'Northwestern University': 24170,\n",
    "    'MIMIC IV': 23372,\n",
    "    'Oregon Health & Science University': 3197\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'n' column by mapping the 'university' column\n",
    "aggregated_business_hours['n'] = aggregated_business_hours['university'].map(n_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate median time\n",
    "def calculate_median_time(df):\n",
    "    median_times = {}\n",
    "    criteria = df['criterion_type'].unique()\n",
    "    universities = df['university'].unique()\n",
    "    \n",
    "    for criterion in criteria:\n",
    "        for uni in universities:\n",
    "            subset = df[(df['criterion_type'] == criterion) & (df['university'] == uni)]\n",
    "            eligible = subset[subset['value'] >= 0.5]\n",
    "            if not eligible.empty:\n",
    "                median_time = eligible['timeline'].min()\n",
    "                median_times[(criterion, uni)] = f\"{median_time} hours\"\n",
    "            else:\n",
    "                median_times[(criterion, uni)] = \"Not reached within 72 hours\"\n",
    "    return median_times\n",
    "\n",
    "median_times = calculate_median_time(aggregated_business_hours)\n",
    "\n",
    "# Convert 'Not reached within 72 hours' to 72\n",
    "processed_median_times = {}\n",
    "for (criterion, uni), time in median_times.items():\n",
    "    if isinstance(time, str) and 'Not reached' in time:\n",
    "        processed_median_times[(criterion, uni)] = np.inf\n",
    "    else:\n",
    "        processed_median_times[(criterion, uni)] = float(time.split()[0])\n",
    "\n",
    "median_df = pd.DataFrame(list(processed_median_times.items()), columns=['Criterion_University', 'Median_Time'])\n",
    "median_df[['Criterion', 'university']] = pd.DataFrame(median_df['Criterion_University'].tolist(), index=median_df.index)\n",
    "median_df = median_df.drop('Criterion_University', axis=1)\n",
    "\n",
    "site_n_df = aggregated_business_hours[['university', 'n']].drop_duplicates()\n",
    "median_df = median_df.merge(site_n_df, on='university', how='left')\n",
    "\n",
    "median_df['Weighted_Median'] = median_df['Median_Time'] * median_df['n']\n",
    "\n",
    "# Calculate overall weighted mean\n",
    "overall_weighted_mean = median_df.groupby('Criterion')['Weighted_Median'].sum() / median_df.groupby('Criterion')['n'].sum()\n",
    "overall_weighted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for timeline = 1.0 hour\n",
    "df_at_1_hour = aggregated_business_hours[aggregated_business_hours['timeline'] == 1.0]\n",
    "# Group by 'criterion_type' and calculate the weighted CIF at 1 hour using sum of 'n' per group\n",
    "overall_cif_at_1_hour = df_at_1_hour.groupby('criterion_type').apply(\n",
    "    lambda x: (x['value'] * x['n']).sum() / x['n'].sum()\n",
    ")\n",
    "\n",
    "# Convert to percentage\n",
    "overall_cif_at_1_hour_percent = overall_cif_at_1_hour * 100\n",
    "\n",
    "# Display the results\n",
    "print(overall_cif_at_1_hour_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for timeline = 3.0 hour\n",
    "df_at_1_hour = aggregated_business_hours[aggregated_business_hours['timeline'] == 3.0]\n",
    "# Group by 'criterion_type' and calculate the weighted CIF at 1 hour using sum of 'n' per group\n",
    "overall_cif_at_1_hour = df_at_1_hour.groupby('criterion_type').apply(\n",
    "    lambda x: (x['value'] * x['n']).sum() / x['n'].sum()\n",
    ")\n",
    "\n",
    "# Convert to percentage\n",
    "overall_cif_at_1_hour_percent = overall_cif_at_1_hour * 100\n",
    "\n",
    "# Display the results\n",
    "print(overall_cif_at_1_hour_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_file_path(site_dir, criteria, site_name, main_dir):\n",
    "    \"\"\"\n",
    "    Constructs the file path for a given site and criteria.\n",
    "    \n",
    "    Parameters:\n",
    "        site_dir (str): Subdirectory name (e.g., 'Michigan').\n",
    "        criteria (str): Criteria type ('patel', 'team', 'consensus guidelines ').\n",
    "        site_name (str): Site name used in the filename (e.g., 'U Michigan').\n",
    "        main_dir (str): Path to the main directory.\n",
    "    \n",
    "    Returns:\n",
    "        str or None: Full path to the CSV file or None if not found.\n",
    "    \"\"\"\n",
    "    # Define the expected filename pattern with wildcard for timestamp\n",
    "    pattern = f'cif_{criteria}_{site_name}_*.csv'\n",
    "    search_path = os.path.join(main_dir, site_dir, pattern)\n",
    "    matched_files = glob.glob(search_path)\n",
    "    \n",
    "    if not matched_files:\n",
    "        print(f\"    Warning: No file found for criteria '{criteria}' in site '{site_dir}'.\")\n",
    "        return None\n",
    "    elif len(matched_files) > 1:\n",
    "        print(f\"    Warning: Multiple files found for criteria '{criteria}' in site '{site_dir}'. Using the first match.\")\n",
    "    \n",
    "    # Return the first matched file\n",
    "    return matched_files[0]\n",
    "\n",
    "def read_and_process_file(file_path, criterion_type, university_name):\n",
    "    \"\"\"\n",
    "    Reads a CSV file and processes it for aggregation.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        criterion_type (str): Human-readable criterion type (e.g., 'Patel Criteria').\n",
    "        university_name (str): Full name of the university.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with standardized column names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"      Reading file: {file_path}\")\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if the file has exactly four columns\n",
    "        if df.shape[1] != 4:\n",
    "            print(f\"      Error: Expected 4 columns in {file_path}, but found {df.shape[1]}. Skipping this file.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Assign standard column names regardless of original names\n",
    "        df.columns = ['timeline', 'value', 'criteria', 'site']\n",
    "        \n",
    "        # Optional: Display the first few rows to verify\n",
    "        # display(df.head())\n",
    "        \n",
    "        # Add additional columns for aggregation\n",
    "        df['criterion_type'] = criterion_type\n",
    "        df['university'] = university_name\n",
    "        \n",
    "        # Select relevant columns for aggregation\n",
    "        df = df[['timeline', 'value', 'criterion_type', 'university']]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"      Error reading {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def aggregate_all_sites(main_dir):\n",
    "    \"\"\"\n",
    "    Aggregates data from all site directories.\n",
    "    \n",
    "    Parameters:\n",
    "        main_dir (str): Path to the main directory containing site subdirectories.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Aggregated DataFrame containing data from all sites and criteria.\n",
    "    \"\"\"\n",
    "    aggregated_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through each site directory\n",
    "    for site_dir, university_name in DIR_TO_UNIVERSITY.items():\n",
    "        site_name_in_file = DIR_TO_SITE_NAME.get(site_dir, \"\")\n",
    "        \n",
    "        if not site_name_in_file:\n",
    "            print(f\"\\n  Warning: No site name mapping found for directory '{site_dir}'. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing site: {university_name} ({site_dir})\")\n",
    "        \n",
    "        for criteria_key, criterion_type in CRITERIA_TYPES.items():\n",
    "            print(f\"  Processing criteria: {criterion_type}\")\n",
    "            file_path = construct_file_path(site_dir, criteria_key, site_name_in_file, main_dir)\n",
    "            \n",
    "            if file_path:\n",
    "                df = read_and_process_file(file_path, criterion_type, university_name)\n",
    "                \n",
    "                if not df.empty:\n",
    "                    aggregated_df = pd.concat([aggregated_df, df], ignore_index=True)\n",
    "                else:\n",
    "                    print(f\"  Warning: DataFrame is empty for file '{file_path}'.\")\n",
    "            else:\n",
    "                print(f\"  Warning: File for criteria '{criteria_key}' not found in site '{site_dir}'.\")\n",
    "    \n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data from all sites\n",
    "aggregated_df_all_hours = aggregate_all_sites(main_directory)\n",
    "\n",
    "# Check if data was aggregated\n",
    "if aggregated_df_all_hours.empty:\n",
    "    print(\"No data was aggregated. Please check the directory structure and file naming conventions.\")\n",
    "else:\n",
    "    print(f\"\\nTotal aggregated records: {len(aggregated_df_all_hours)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAggregated Data Preview:\")\n",
    "display(aggregated_df_all_hours.head())\n",
    "\n",
    "# Display information about the DataFrame\n",
    "print(\"\\nAggregated Data Info:\")\n",
    "aggregated_df_all_hours.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aggregated_data(aggregated_df):\n",
    "    \"\"\"\n",
    "    Plots the aggregated data in three panels, one for each criterion,\n",
    "    with y-axis fixed from 0 to 1 and a legend positioned below the plots.\n",
    "    \n",
    "    Parameters:\n",
    "        aggregated_df (pd.DataFrame): Aggregated DataFrame containing all sites and criteria.\n",
    "    \"\"\"\n",
    "    # Ensure 'timeline' is numeric\n",
    "    aggregated_df['timeline'] = pd.to_numeric(aggregated_df['timeline'], errors='coerce')\n",
    "    \n",
    "    # Drop rows with NaN 'timeline' or 'value'\n",
    "    aggregated_df = aggregated_df.dropna(subset=['timeline', 'value'])\n",
    "    \n",
    "    # Sort by 'timeline'\n",
    "    aggregated_df = aggregated_df.sort_values('timeline')\n",
    "    \n",
    "    # Initialize a FacetGrid with three columns for each criterion\n",
    "    g = sns.FacetGrid(\n",
    "        aggregated_df, \n",
    "        col='criterion_type', \n",
    "        sharey=True,  # Share y-axis to ensure consistent scaling\n",
    "        height=6, \n",
    "        aspect=1.2\n",
    "    )\n",
    "    \n",
    "    # Define a palette based on university colors\n",
    "    palette = UNIVERSITY_COLORS\n",
    "    \n",
    "    # Plot each facet without an internal legend\n",
    "    g.map_dataframe(\n",
    "        sns.lineplot, \n",
    "        x='timeline', \n",
    "        y='value', \n",
    "        hue='university', \n",
    "        palette=palette,\n",
    "        legend=False  # Disable internal legends\n",
    "    )\n",
    "    \n",
    "    # Create custom legend handles\n",
    "    custom_handles = [\n",
    "        Line2D([0], [0], color=color, lw=2) for color in palette.values()\n",
    "    ]\n",
    "    custom_labels = list(palette.keys())\n",
    "    \n",
    "    # Add a single legend below the plots\n",
    "    g.fig.legend(\n",
    "        handles=custom_handles, \n",
    "        labels=custom_labels, \n",
    "        # title='Hospitals', \n",
    "        loc='lower center', \n",
    "        ncol=5, \n",
    "        bbox_to_anchor=(0.5, -0.02)  # Position the legend below the plots\n",
    "        # ncol=len(custom_labels),  # Adjust ncol to cover the entire width\n",
    "        # bbox_to_anchor=(0.5, -0.0003)  # Adjust position to cover the entire width\n",
    "    )\n",
    "    \n",
    "    # Adjust the layout to make space for the legend\n",
    "    plt.subplots_adjust(top=0.85, bottom=0.25, right=0.9)  # Increase the bottom margin for the legend\n",
    "    \n",
    "    # Set titles and labels\n",
    "    g.set_titles(\"{col_name}\")\n",
    "    g.set_axis_labels(\"Timeline (Business Hours)\", \"Cumulative Incidence Probability\")\n",
    "    \n",
    "    # Set y-axis limits from 0 to 1 for all plots\n",
    "    g.set(ylim=(0, 1))\n",
    "    \n",
    "    # Set a super title for the entire figure\n",
    "    plt.suptitle('Cumulative Incidence Function for Time to First Eligibility Across CLIF Sites', fontsize=16)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure seaborn aesthetics\n",
    "# sns.set(style=\"whitegrid\")\n",
    "plot_aggregated_data(aggregated_df_all_hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OHSU seems off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_all_hours.value_counts('university')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_all_hours.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine median time to eligibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate median time\n",
    "def calculate_median_time(df):\n",
    "    median_times = {}\n",
    "    criteria = df['criterion_type'].unique()\n",
    "    universities = df['university'].unique()\n",
    "    \n",
    "    for criterion in criteria:\n",
    "        for uni in universities:\n",
    "            subset = df[(df['criterion_type'] == criterion) & (df['university'] == uni)]\n",
    "            eligible = subset[subset['value'] >= 0.5]\n",
    "            if not eligible.empty:\n",
    "                median_time = eligible['timeline'].min()\n",
    "                median_times[(criterion, uni)] = f\"{median_time} hours\"\n",
    "            else:\n",
    "                median_times[(criterion, uni)] = \"Not reached within 72 hours\"\n",
    "    return median_times\n",
    "\n",
    "median_times = calculate_median_time(aggregated_df_all_hours)\n",
    "\n",
    "# Convert 'Not reached within 72 hours' to infinite values\n",
    "processed_median_times = {}\n",
    "for (criterion, uni), time in median_times.items():\n",
    "    if isinstance(time, str) and 'Not reached' in time:\n",
    "        processed_median_times[(criterion, uni)] = np.inf\n",
    "    else:\n",
    "        processed_median_times[(criterion, uni)] = float(time.split()[0])\n",
    "\n",
    "median_df = pd.DataFrame(list(processed_median_times.items()), columns=['Criterion_University', 'Median_Time'])\n",
    "median_df[['Criterion', 'university']] = pd.DataFrame(median_df['Criterion_University'].tolist(), index=median_df.index)\n",
    "median_df = median_df.drop('Criterion_University', axis=1)\n",
    "\n",
    "# site_n_df = aggregated_df_all_hours[['university', 'n']].drop_duplicates()\n",
    "median_df = median_df.merge(site_n_df, on='university', how='left')\n",
    "\n",
    "median_df['Weighted_Median'] = median_df['Median_Time'] * median_df['n']\n",
    "# Calculate overall weighted mean\n",
    "overall_weighted_mean = median_df.groupby('Criterion')['Weighted_Median'].sum() / median_df.groupby('Criterion')['n'].sum()\n",
    "overall_weighted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportion of Eligible Hours and Encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_process_aggregate_file(file_path, site_name):\n",
    "    \"\"\"\n",
    "    Reads an aggregates CSV file and processes it for analysis.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the aggregates CSV file.\n",
    "        site_name (str): Name of the site (derived from directory name).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with computed proportions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"  Reading file: {file_path}\")\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if the file has the expected number of columns\n",
    "        print(\"Printing existing columns:\", df.columns)\n",
    "        expected_columns = ['Criteria', 'Total Patients', 'Eligible Patients', \n",
    "                            'Total Observed Hours', 'Eligible Hours', \n",
    "                            'Median Time', 'CI Lower Median Time', 'CI Upper Median Time']\n",
    "        if not all(col in df.columns for col in expected_columns):\n",
    "            # print(\"Printing existing columns:\", df.columns)\n",
    "            print(\"Expected columns:\", expected_columns)\n",
    "            print(f\"    Error: Missing required columns in {file_path}. Skipping this file.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Assign standard column names if necessary (optional)\n",
    "        df.columns = ['Criteria', 'Total Patients', 'Eligible Patients', \n",
    "                      'Total Observed Hours', 'Eligible Hours', \n",
    "                      'Median Time', 'CI Lower Median Time', 'CI Upper Median Time']\n",
    "        \n",
    "        # Add a column for site_name\n",
    "        df['site_name'] = site_name\n",
    "        \n",
    "        # Compute proportions\n",
    "        df['proportion_encounter'] = df['Eligible Patients'] / df['Total Patients']\n",
    "        df['proportion_hours'] = df['Eligible Hours'] / df['Total Observed Hours']\n",
    "        \n",
    "        # Select relevant columns for aggregation\n",
    "        # df = df[['site_name', 'Criteria', 'proportion_encounter', 'proportion_hours']]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"    Error reading {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def aggregate_all_aggregate_files(main_dir):\n",
    "    \"\"\"\n",
    "    Aggregates data from all 'aggregates_*.csv' files across site directories.\n",
    "    \n",
    "    Parameters:\n",
    "        main_dir (str): Path to the main directory containing site subdirectories.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with all aggregated data.\n",
    "    \"\"\"\n",
    "    aggregated_data = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through each site directory\n",
    "    for dir_name, university_name in DIR_TO_UNIVERSITY.items():\n",
    "        site_name = DIR_TO_SITE_NAME.get(dir_name, \"\")\n",
    "        \n",
    "        if not site_name:\n",
    "            print(f\"\\n  Warning: No site name mapping found for directory '{dir_name}'. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        site_dir_path = os.path.join(main_dir, dir_name)\n",
    "        \n",
    "        if not os.path.isdir(site_dir_path):\n",
    "            print(f\"\\n  Warning: Site directory '{site_dir_path}' does not exist. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing site: {university_name} ({dir_name})\")\n",
    "        \n",
    "        # Search for 'aggregates_*.csv' files in the site directory\n",
    "        pattern = os.path.join(site_dir_path, 'aggregates_*.csv')\n",
    "        aggregate_files = glob.glob(pattern)\n",
    "        \n",
    "        if not aggregate_files:\n",
    "            print(f\"  Warning: No 'aggregates_*.csv' files found in '{site_dir_path}'.\")\n",
    "            continue\n",
    "        \n",
    "        for file in aggregate_files:\n",
    "            processed_df = read_and_process_aggregate_file(file, site_name)\n",
    "            if not processed_df.empty:\n",
    "                aggregated_data = pd.concat([aggregated_data, processed_df], ignore_index=True)\n",
    "    \n",
    "    return aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data from all sites\n",
    "aggregated_df2 = aggregate_all_aggregate_files(main_directory)\n",
    "\n",
    "# Check if data was aggregated\n",
    "if aggregated_df2.empty:\n",
    "    print(\"No data was aggregated. Please check the directory structure and file naming conventions.\")\n",
    "else:\n",
    "    print(f\"\\nTotal aggregated records: {len(aggregated_df2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAggregated Data Preview:\")\n",
    "display(aggregated_df2.head())\n",
    "\n",
    "# Display information about the DataFrame\n",
    "print(\"\\nAggregated Data Info:\")\n",
    "aggregated_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITERIA_COLORS = {\n",
    "    'Patel Criteria': 'lightskyblue',                 # Blue\n",
    "    'Consensus Guidelines Criteria': 'thistle',  # Yellow\n",
    "    'TEAM Criteria': 'lightcoral'                \n",
    "}\n",
    "\n",
    "def create_multiple_bar_plot(aggregated_df):\n",
    "    \"\"\"\n",
    "    Creates a grouped (multiple) bar plot for the proportion of eligible business hours\n",
    "    across different universities and criteria with a frame and a legend inside the plot.\n",
    "    \n",
    "    Parameters:\n",
    "        aggregated_df (pd.DataFrame): Aggregated DataFrame containing 'site_name', 'Criteria',\n",
    "                                      and 'proportion_hours'.\n",
    "    \"\"\"\n",
    "    # Ensure that the necessary columns are present\n",
    "    required_columns = {'site_name', 'Criteria', 'proportion_hours'}\n",
    "    if not required_columns.issubset(aggregated_df.columns):\n",
    "        raise ValueError(f\"DataFrame must contain columns: {required_columns}\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    plot_df = aggregated_df.copy()\n",
    "    \n",
    "    # Map 'site_name' to 'university_name'\n",
    "    plot_df['university_name'] = plot_df['site_name'].map(site_to_university)\n",
    "    \n",
    "    # Handle missing mappings if any\n",
    "    missing_universities = plot_df['university_name'].isnull()\n",
    "    if missing_universities.any():\n",
    "        print(f\"Warning: {missing_universities.sum()} site_names could not be mapped to university names.\")\n",
    "        # Optionally, fill missing values with original 'site_name' or another placeholder\n",
    "        plot_df['university_name'] = plot_df['university_name'].fillna(plot_df['site_name'])\n",
    "    \n",
    "    # Rename 'Yellow' to 'Consensus Guidelines Criteria'\n",
    "    plot_df['Criteria'] = plot_df['Criteria'].replace('Yellow', 'Consensus Guidelines Criteria')\n",
    "    \n",
    "    # Rename 'Patel' and 'TEAM' to include 'Criteria' for consistency\n",
    "    plot_df['Criteria'] = plot_df['Criteria'].replace({\n",
    "        'Patel': 'Patel Criteria',\n",
    "        'TEAM': 'TEAM Criteria'\n",
    "    })\n",
    "    \n",
    "    # Define the order of criteria\n",
    "    criteria_order = ['Patel Criteria', 'Consensus Guidelines Criteria', 'TEAM Criteria']\n",
    "    \n",
    "    # Ensure 'Criteria' is a categorical type with the specified order\n",
    "    plot_df['Criteria'] = pd.Categorical(plot_df['Criteria'], categories=criteria_order, ordered=True)\n",
    "    \n",
    "    # Define the order of sites (university names)\n",
    "    site_order = plot_df['university_name'].unique()\n",
    "    \n",
    "    # Set Seaborn style to remove grid background\n",
    "    sns.set_style(\"white\")\n",
    "    \n",
    "    # Initialize the matplotlib figure\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Create the grouped bar plot\n",
    "    bar_plot = sns.barplot(\n",
    "        data=plot_df,\n",
    "        x='university_name',\n",
    "        y='proportion_hours',\n",
    "        hue='Criteria',\n",
    "        palette=CRITERIA_COLORS,\n",
    "        order=site_order\n",
    "    )\n",
    "    \n",
    "    # Set plot labels and title\n",
    "    bar_plot.set_title('Proportion of Eligible Business Hours Across Sites', fontsize=18, pad=20)\n",
    "    bar_plot.set_xlabel('Site Name', fontsize=14)\n",
    "    bar_plot.set_ylabel('Proportion of Eligible Business Hours', fontsize=14)\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    bar_plot.set_ylim(0, 1)\n",
    "    \n",
    "    # Rotate x-axis labels for better readability if there are many sites\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    # Add a box frame around the plot\n",
    "    plt.gca().set_frame_on(True)\n",
    "    \n",
    "    # Add the legend inside the plot at the top right without overlapping any bars\n",
    "    plt.legend(\n",
    "        title='Criteria',\n",
    "        title_fontsize=14,\n",
    "        fontsize=12,\n",
    "        loc='upper right',\n",
    "        bbox_to_anchor=(0.95, 0.95),\n",
    "        frameon=True\n",
    "    )\n",
    "    \n",
    "    # Adjust layout to ensure everything fits without overlapping\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Run the function with your DataFrame\n",
    "create_multiple_bar_plot(aggregated_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITERIA_COLORS = {\n",
    "    'Patel Criteria': 'darkblue',                 \n",
    "    'Consensus Guidelines Criteria': 'gold',  \n",
    "    'TEAM Criteria': 'brown'                \n",
    "}\n",
    "\n",
    "def create_horizontal_scatter_plot(aggregated_df):\n",
    "    \"\"\"\n",
    "    Creates a horizontal scatter plot for the proportion of eligible business hours\n",
    "    across different universities and criteria, with each proportion represented by a circle.\n",
    "    \n",
    "    Parameters:\n",
    "        aggregated_df (pd.DataFrame): Aggregated DataFrame containing 'site_name', 'Criteria',\n",
    "                                      and 'proportion_hours'.\n",
    "    \"\"\"\n",
    "    # Ensure that the necessary columns are present\n",
    "    required_columns = {'site_name', 'Criteria', 'proportion_hours'}\n",
    "    if not required_columns.issubset(aggregated_df.columns):\n",
    "        raise ValueError(f\"DataFrame must contain columns: {required_columns}\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    plot_df = aggregated_df.copy()\n",
    "    \n",
    "    # Map 'site_name' to 'university_name'\n",
    "    plot_df['university_name'] = plot_df['site_name'].map(site_to_university)\n",
    "    \n",
    "    # Handle missing mappings if any\n",
    "    missing_universities = plot_df['university_name'].isnull()\n",
    "    if missing_universities.any():\n",
    "        print(f\"Warning: {missing_universities.sum()} site_names could not be mapped to university names.\")\n",
    "        plot_df['university_name'] = plot_df['university_name'].fillna(plot_df['site_name'])\n",
    "    \n",
    "    # Rename criteria for consistency\n",
    "    plot_df['Criteria'] = plot_df['Criteria'].replace({\n",
    "        'Yellow': 'Consensus Guidelines Criteria',\n",
    "        'Patel': 'Patel Criteria',\n",
    "        'TEAM': 'TEAM Criteria'\n",
    "    })\n",
    "    \n",
    "    # Define the order of criteria\n",
    "    criteria_order = ['Patel Criteria', 'Consensus Guidelines Criteria', 'TEAM Criteria']\n",
    "    plot_df['Criteria'] = pd.Categorical(plot_df['Criteria'], categories=criteria_order, ordered=True)\n",
    "    \n",
    "    # Set Seaborn style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Initialize the figure\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Create the scatter plot\n",
    "    scatter_plot = sns.scatterplot(\n",
    "        data=plot_df,\n",
    "        y='university_name',\n",
    "        x='proportion_hours',\n",
    "        hue='Criteria',\n",
    "        style='Criteria',\n",
    "        palette=CRITERIA_COLORS,\n",
    "        markers=['o', 's', 'D'],\n",
    "        s=200,\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    # Set labels and title\n",
    "    scatter_plot.set_title('Proportion of Eligible Business Hours Across Sites', fontsize=18, pad=20)\n",
    "    scatter_plot.set_ylabel('Site Name', fontsize=14)\n",
    "    scatter_plot.set_xlabel('Proportion of Eligible Business Hours', fontsize=14)\n",
    "    \n",
    "    # Set x-axis limits\n",
    "    scatter_plot.set_xlim(0, 1)\n",
    "    \n",
    "    # Format ticks\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xticks([0, 0.5, 1], fontsize=12)\n",
    "    \n",
    "    # Add a frame around the entire plot\n",
    "    plt.gca().spines['top'].set_visible(True)\n",
    "    plt.gca().spines['right'].set_visible(True)\n",
    "    plt.gca().spines['left'].set_visible(True)\n",
    "    plt.gca().spines['bottom'].set_visible(True)\n",
    "    \n",
    "    # Add the legend at the bottom of the plot\n",
    "    plt.legend(\n",
    "        # title='Criteria',\n",
    "        title_fontsize=14,\n",
    "        fontsize=12,\n",
    "        loc='upper center',\n",
    "        bbox_to_anchor=(0.5, -0.1),\n",
    "        ncol=3,\n",
    "        frameon=True\n",
    "    )\n",
    "    \n",
    "    # Adjust layout to fit legend and plot elements\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Run the function with your DataFrame\n",
    "create_horizontal_scatter_plot(aggregated_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITERIA_COLORS = {\n",
    "    'Patel Criteria': 'darkblue',                 \n",
    "    'Consensus Guidelines Criteria': 'gold',  \n",
    "    'TEAM Criteria': 'brown'                \n",
    "}\n",
    "\n",
    "def create_horizontal_scatter_plot(aggregated_df):\n",
    "    \"\"\"\n",
    "    Creates a horizontal scatter plot for the proportion of eligible business hours\n",
    "    across different universities and criteria, with each proportion represented by a circle.\n",
    "    \n",
    "    Parameters:\n",
    "        aggregated_df (pd.DataFrame): Aggregated DataFrame containing 'site_name', 'Criteria',\n",
    "                                      and 'proportion_hours'.\n",
    "    \"\"\"\n",
    "    # Ensure that the necessary columns are present\n",
    "    required_columns = {'site_name', 'Criteria', 'proportion_hours'}\n",
    "    if not required_columns.issubset(aggregated_df.columns):\n",
    "        raise ValueError(f\"DataFrame must contain columns: {required_columns}\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    plot_df = aggregated_df.copy()\n",
    "    \n",
    "    # Map 'site_name' to 'university_name'\n",
    "    plot_df['university_name'] = plot_df['site_name'].map(site_to_university)\n",
    "    \n",
    "    # Handle missing mappings if any\n",
    "    missing_universities = plot_df['university_name'].isnull()\n",
    "    if missing_universities.any():\n",
    "        print(f\"Warning: {missing_universities.sum()} site_names could not be mapped to university names.\")\n",
    "        plot_df['university_name'] = plot_df['university_name'].fillna(plot_df['site_name'])\n",
    "    \n",
    "    # Rename criteria for consistency\n",
    "    plot_df['Criteria'] = plot_df['Criteria'].replace({\n",
    "        'Yellow': 'Consensus Guidelines Criteria',\n",
    "        'Patel': 'Patel Criteria',\n",
    "        'TEAM': 'TEAM Criteria'\n",
    "    })\n",
    "    \n",
    "    # Define the order of criteria\n",
    "    criteria_order = ['Patel Criteria', 'Consensus Guidelines Criteria', 'TEAM Criteria']\n",
    "    plot_df['Criteria'] = pd.Categorical(plot_df['Criteria'], categories=criteria_order, ordered=True)\n",
    "    \n",
    "    # Set Seaborn style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Initialize the figure\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Create the scatter plot\n",
    "    scatter_plot = sns.scatterplot(\n",
    "        data=plot_df,\n",
    "        y='university_name',\n",
    "        x='proportion_encounter',\n",
    "        hue='Criteria',\n",
    "        style='Criteria',\n",
    "        palette=CRITERIA_COLORS,\n",
    "        markers=['o', 's', 'D'],\n",
    "        s=200,\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    # Set labels and title\n",
    "    scatter_plot.set_title('Proportion of Eligible Encounters Across Sites', fontsize=18, pad=20)\n",
    "    scatter_plot.set_ylabel('Site Name', fontsize=14)\n",
    "    scatter_plot.set_xlabel('Proportion of Eligible Number of Encounters', fontsize=14)\n",
    "    \n",
    "    # Set x-axis limits\n",
    "    scatter_plot.set_xlim(0, 1)\n",
    "    \n",
    "    # Format ticks\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xticks([0, 0.5, 1], fontsize=12)\n",
    "    \n",
    "    # Add a frame around the entire plot\n",
    "    plt.gca().spines['top'].set_visible(True)\n",
    "    plt.gca().spines['right'].set_visible(True)\n",
    "    plt.gca().spines['left'].set_visible(True)\n",
    "    plt.gca().spines['bottom'].set_visible(True)\n",
    "    \n",
    "    # Add the legend at the bottom of the plot\n",
    "    plt.legend(\n",
    "        # title='Criteria',\n",
    "        title_fontsize=14,\n",
    "        fontsize=12,\n",
    "        loc='upper center',\n",
    "        bbox_to_anchor=(0.5, -0.1),\n",
    "        ncol=3,\n",
    "        frameon=True\n",
    "    )\n",
    "    \n",
    "    # Adjust layout to fit legend and plot elements\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Run the function with your DataFrame\n",
    "create_horizontal_scatter_plot(aggregated_df2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mobilization)",
   "language": "python",
   "name": ".mobilization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
