{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190fb89d",
   "metadata": {
    "papermill": {
     "duration": 0.021914,
     "end_time": "2025-05-05T20:32:20.562643",
     "exception": false,
     "start_time": "2025-05-05T20:32:20.540729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Eligibility for mobilization: Cohort ID and Discretizing script\n",
    "\n",
    "Author: Kaveri Chhikara\n",
    "\n",
    "This script identifies the cohort using CLIF 2.1 tables and discretizes the dataset at an hourly level\n",
    "\n",
    " \n",
    "                        ðŸš¨Code will break if the following requirements are not satisfiedðŸš¨  \n",
    "#### Requirements\n",
    "\n",
    "* Required table filenames should be `clif_patient`, `clif_hospitalization`, `clif_adt`, `clif_vitals`, `clif_labs`, `clif_medication_admin_continuous`, `clif_respiratory_support` \n",
    "* Within each table, the following variables and categories are required.\n",
    "\n",
    "| Table Name | Required Variables | Required Categories |\n",
    "| --- | --- | --- |\n",
    "| **patient** | `patient_id`, `race_category`, `ethnicity_category`, `sex_category`, `death_dttm` | - |\n",
    "| **hospitalization** | `patient_id`, `hospitalization_id`, `admission_dttm`, `discharge_dttm`, `age_at_admission` | - |\n",
    "| **adt** |  `hospitalization_id`, `hospital_id`,`in_dttm`, `out_dttm`, `location_category` | - |\n",
    "| **vitals** | `hospitalization_id`, `recorded_dttm`, `vital_category`, `vital_value` | heart_rate, resp_rate, sbp, dbp, map, spo2, weight_kg, height_cm |\n",
    "| **labs** | `hospitalization_id`, `lab_result_dttm`, `lab_category`, `lab_value` | lactate, creatinine, bilirubin_total, po2_arterial, platelet_count |\n",
    "| **medication_admin_continuous** | `hospitalization_id`, `admin_dttm`, `med_name`, `med_category`, `med_dose`, `med_dose_unit` | norepinephrine, epinephrine, phenylephrine, vasopressin, dopamine, angiotensin(optional), nicardipine, nitroprusside, clevidipine, cisatracurium, vecuronium, rocuronium |\n",
    "| **respiratory_support** | `hospitalization_id`, `recorded_dttm`, `device_category`, `mode_category`, `tracheostomy`, `fio2_set`, `lpm_set`, `resp_rate_set`, `peep_set`, `resp_rate_obs`, `tidal_volume_set`, `pressure_control_set`, `pressure_support_set`, `peak_inspiratory_pressure_set`, `tidal_volume_obs` | - |\n",
    "| **crrt_therapy** | `hospitalization_id`, `recorded_dttm` | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908fead7",
   "metadata": {
    "papermill": {
     "duration": 0.028543,
     "end_time": "2025-05-05T20:32:20.604336",
     "exception": false,
     "start_time": "2025-05-05T20:32:20.575793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd0f16a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:20.621690Z",
     "iopub.status.busy": "2025-05-05T20:32:20.621513Z",
     "iopub.status.idle": "2025-05-05T20:32:21.637159Z",
     "shell.execute_reply": "2025-05-05T20:32:21.636826Z"
    },
    "papermill": {
     "duration": 1.025214,
     "end_time": "2025-05-05T20:32:21.638110",
     "exception": false,
     "start_time": "2025-05-05T20:32:20.612896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pyCLIF\n",
    "from datetime import timedelta\n",
    "import pyarrow\n",
    "import sofa_score\n",
    "import waterfall\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## import outlier json\n",
    "with open('../config/outlier_config.json', 'r', encoding='utf-8') as f:\n",
    "    outlier_cfg = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## â”€â”€ Output Folder Management â”€â”€\n",
    "print(\"=== Output Folder Management ===\")\n",
    "\n",
    "output_folder = '../output'\n",
    "output_old_folder = '../output_old'\n",
    "\n",
    "# Check if output folder exists\n",
    "if os.path.exists(output_folder):\n",
    "    print(f\"Existing output folder found: {output_folder}\")\n",
    "    \n",
    "    # If output_old already exists, remove it first\n",
    "    if os.path.exists(output_old_folder):\n",
    "        print(f\"Removing existing output_old folder...\")\n",
    "        shutil.rmtree(output_old_folder)\n",
    "    \n",
    "    # Rename current output to output_old\n",
    "    print(f\"Renaming {output_folder} â†’ {output_old_folder}\")\n",
    "    os.rename(output_folder, output_old_folder)\n",
    "    \n",
    "    # Log what was backed up\n",
    "    if os.path.exists(output_old_folder):\n",
    "        backup_size = sum(\n",
    "            os.path.getsize(os.path.join(dirpath, filename))\n",
    "            for dirpath, dirnames, filenames in os.walk(output_old_folder)\n",
    "            for filename in filenames\n",
    "        ) / (1024 * 1024)  # Convert to MB\n",
    "        print(f\"Backup created: {backup_size:.1f} MB\")\n",
    "\n",
    "# Create fresh output directory structure\n",
    "print(f\"Creating fresh output directory structure...\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(f'{output_folder}/final', exist_ok=True)\n",
    "os.makedirs(f'{output_folder}/intermediate', exist_ok=True)\n",
    "# Create empty output files\n",
    "with open(f'{output_folder}/final/final_output.txt', 'w') as f:\n",
    "    pass\n",
    "with open(f'{output_folder}/intermediate/intermediate.txt', 'w') as f:\n",
    "    pass\n",
    "\n",
    "# Create graphs subfolder\n",
    "graphs_folder = f'{output_folder}/final/graphs'\n",
    "os.makedirs(graphs_folder, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory structure ready:\")\n",
    "print(f\"   {output_folder}/\")\n",
    "print(f\"   â”œâ”€â”€ final/\")\n",
    "print(f\"   â”‚   â””â”€â”€ graphs/\")\n",
    "print(f\"   â””â”€â”€ intermediate/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e50f3",
   "metadata": {
    "papermill": {
     "duration": 0.006739,
     "end_time": "2025-05-05T20:32:21.651612",
     "exception": false,
     "start_time": "2025-05-05T20:32:21.644873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Required columns and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c095d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:21.665676Z",
     "iopub.status.busy": "2025-05-05T20:32:21.665488Z",
     "iopub.status.idle": "2025-05-05T20:32:21.668547Z",
     "shell.execute_reply": "2025-05-05T20:32:21.668287Z"
    },
    "papermill": {
     "duration": 0.011229,
     "end_time": "2025-05-05T20:32:21.669302",
     "exception": false,
     "start_time": "2025-05-05T20:32:21.658073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rst_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'device_name',\n",
    "    'device_category',\n",
    "    'mode_name', \n",
    "    'mode_category',\n",
    "    'tracheostomy',\n",
    "    'fio2_set',\n",
    "    'lpm_set',\n",
    "    'resp_rate_set',\n",
    "    'peep_set',\n",
    "    'resp_rate_obs',\n",
    "    'tidal_volume_set', \n",
    "    'pressure_control_set',\n",
    "    'pressure_support_set',\n",
    "    'peak_inspiratory_pressure_set'\n",
    "\n",
    "]\n",
    "\n",
    "vitals_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'vital_category',\n",
    "    'vital_value'\n",
    "]\n",
    "vitals_of_interest = ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
    "\n",
    "labs_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'lab_result_dttm',\n",
    "    'lab_category',\n",
    "    'lab_value',\n",
    "    'lab_value_numeric'\n",
    "]\n",
    "labs_of_interest = ['lactate']\n",
    "\n",
    "meds_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'admin_dttm',\n",
    "    'med_name',\n",
    "    'med_category',\n",
    "    'med_dose',\n",
    "    'med_dose_unit'\n",
    "]\n",
    "meds_of_interest = [\n",
    "    'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin', 'nicardipine', 'nitroprusside',\n",
    "    'clevidipine', 'cisatracurium', 'vecuronium', 'rocuronium '\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8808bd",
   "metadata": {
    "papermill": {
     "duration": 0.006579,
     "end_time": "2025-05-05T20:32:21.682656",
     "exception": false,
     "start_time": "2025-05-05T20:32:21.676077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33337471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:21.695607Z",
     "iopub.status.busy": "2025-05-05T20:32:21.695484Z",
     "iopub.status.idle": "2025-05-05T20:32:22.597731Z",
     "shell.execute_reply": "2025-05-05T20:32:22.597315Z"
    },
    "papermill": {
     "duration": 0.909698,
     "end_time": "2025-05-05T20:32:22.598635",
     "exception": false,
     "start_time": "2025-05-05T20:32:21.688937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient = pyCLIF.load_data('clif_patient')\n",
    "hospitalization = pyCLIF.load_data('clif_hospitalization')\n",
    "adt = pyCLIF.load_data('clif_adt')\n",
    "\n",
    "# ensure id variable is of dtype character\n",
    "hospitalization['hospitalization_id']= hospitalization['hospitalization_id'].astype(str)\n",
    "patient['patient_id']= patient['patient_id'].astype(str)\n",
    "adt['hospitalization_id']= adt['hospitalization_id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c735fb",
   "metadata": {
    "papermill": {
     "duration": 0.00683,
     "end_time": "2025-05-05T20:32:22.612492",
     "exception": false,
     "start_time": "2025-05-05T20:32:22.605662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Duplicate check\n",
    "\n",
    "If duplicates exist, only the first row is preserved after arranging the data by time. Please check your CLIF tables if there are duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5cffa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:22.626434Z",
     "iopub.status.busy": "2025-05-05T20:32:22.626278Z",
     "iopub.status.idle": "2025-05-05T20:32:22.829988Z",
     "shell.execute_reply": "2025-05-05T20:32:22.829665Z"
    },
    "papermill": {
     "duration": 0.211934,
     "end_time": "2025-05-05T20:32:22.831024",
     "exception": false,
     "start_time": "2025-05-05T20:32:22.619090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "# patient table should be unique by patient id\n",
    "patient = pyCLIF.remove_duplicates(patient, ['patient_id'], 'patient')\n",
    "# hospitalization table should be unique by hospitalization id\n",
    "hospitalization = pyCLIF.remove_duplicates(hospitalization, ['hospitalization_id'], 'hospitalization')\n",
    "# adt table should be unique by hospitalization id and in dttm\n",
    "adt = pyCLIF.remove_duplicates(adt, ['hospitalization_id', 'hospital_id', 'in_dttm'], 'adt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65966d02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:22.845535Z",
     "iopub.status.busy": "2025-05-05T20:32:22.845383Z",
     "iopub.status.idle": "2025-05-05T20:32:22.890922Z",
     "shell.execute_reply": "2025-05-05T20:32:22.890510Z"
    },
    "papermill": {
     "duration": 0.054058,
     "end_time": "2025-05-05T20:32:22.891925",
     "exception": false,
     "start_time": "2025-05-05T20:32:22.837867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Total Number of unique encounters in the hospitalization table: {pyCLIF.count_unique_encounters(hospitalization, 'hospitalization_id')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27408e1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:22.906430Z",
     "iopub.status.busy": "2025-05-05T20:32:22.906269Z",
     "iopub.status.idle": "2025-05-05T20:32:22.952838Z",
     "shell.execute_reply": "2025-05-05T20:32:22.952508Z"
    },
    "papermill": {
     "duration": 0.054882,
     "end_time": "2025-05-05T20:32:22.953660",
     "exception": false,
     "start_time": "2025-05-05T20:32:22.898778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standardize all _dttm variables to the same format\n",
    "patient = pyCLIF.convert_datetime_columns_to_site_tz(patient,  pyCLIF.helper['timezone'])\n",
    "hospitalization = pyCLIF.convert_datetime_columns_to_site_tz(hospitalization, pyCLIF.helper['timezone'])\n",
    "adt = pyCLIF.convert_datetime_columns_to_site_tz(adt,  pyCLIF.helper['timezone'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b519142",
   "metadata": {
    "papermill": {
     "duration": 0.007029,
     "end_time": "2025-05-05T20:32:22.967660",
     "exception": false,
     "start_time": "2025-05-05T20:32:22.960631",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cohort Identification\n",
    "\n",
    "**Inclusion Criteria:**\n",
    "\n",
    "* Adult admissions between 2018-01-01 and 2024-12-31\n",
    "* Encounters receiving invasive mechanical ventilation during this period\n",
    "\n",
    "**Exclusion criteria:**\n",
    "\n",
    "1. Encounters that were on vent for less than 4 hours in the first 72 hours of first intubation\n",
    "2. Encounters that were on trach at the time of intubation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c92e011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:22.982408Z",
     "iopub.status.busy": "2025-05-05T20:32:22.982243Z",
     "iopub.status.idle": "2025-05-05T20:32:22.984291Z",
     "shell.execute_reply": "2025-05-05T20:32:22.983816Z"
    },
    "papermill": {
     "duration": 0.01057,
     "end_time": "2025-05-05T20:32:22.985156",
     "exception": false,
     "start_time": "2025-05-05T20:32:22.974586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setting up a dictionary to keep track of STROBE counts\n",
    "strobe_counts = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bdfb0d",
   "metadata": {
    "papermill": {
     "duration": 0.006998,
     "end_time": "2025-05-05T20:32:22.999195",
     "exception": false,
     "start_time": "2025-05-05T20:32:22.992197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### (A) Date and Age Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa39048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:23.014135Z",
     "iopub.status.busy": "2025-05-05T20:32:23.013927Z",
     "iopub.status.idle": "2025-05-05T20:32:23.102423Z",
     "shell.execute_reply": "2025-05-05T20:32:23.102008Z"
    },
    "papermill": {
     "duration": 0.09729,
     "end_time": "2025-05-05T20:32:23.103423",
     "exception": false,
     "start_time": "2025-05-05T20:32:23.006133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP A: Basic Data Cleaning + Date/Age Filter\n",
    "#   - Filter hospitalization for date range & adult patients\n",
    "#   - Then reduce ADT to those hospitalization_ids\n",
    "print(\"\\n=== STEP A: Filter by date range & age ===\\n\")\n",
    "date_mask = (hospitalization['admission_dttm'] >= '2018-01-01') & \\\n",
    "            (hospitalization['admission_dttm'] <= '2024-12-31')\n",
    "age_mask = (hospitalization['age_at_admission'] >= 18)\n",
    "\n",
    "if pyCLIF.helper['site_name'].lower() == 'mimic':\n",
    "    hospitalization_cohort = hospitalization[age_mask].copy()\n",
    "else:\n",
    "    hospitalization_cohort = hospitalization[date_mask & age_mask].copy()\n",
    "\n",
    "strobe_counts['A_after_date_age_filter'] = hospitalization_cohort['hospitalization_id'].nunique()\n",
    "print(f\"Number of unique hospitalizations after date & age filter: {strobe_counts['A_after_date_age_filter']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f1c05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:23.118609Z",
     "iopub.status.busy": "2025-05-05T20:32:23.118439Z",
     "iopub.status.idle": "2025-05-05T20:32:23.205258Z",
     "shell.execute_reply": "2025-05-05T20:32:23.204756Z"
    },
    "papermill": {
     "duration": 0.095341,
     "end_time": "2025-05-05T20:32:23.206251",
     "exception": false,
     "start_time": "2025-05-05T20:32:23.110910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get total unique hospitalizations without time filter, only age filter\n",
    "age_mask = (hospitalization['age_at_admission'] >= 18)\n",
    "total_adult_hospitalizations = hospitalization[age_mask]['hospitalization_id'].nunique()\n",
    "strobe_counts['A_after_age_filter'] = total_adult_hospitalizations\n",
    "print(f\"\\nTotal number of unique adult hospitalizations (no date filter): {total_adult_hospitalizations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecdf8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001a4bf1",
   "metadata": {
    "papermill": {
     "duration": 0.007497,
     "end_time": "2025-05-05T20:32:23.310689",
     "exception": false,
     "start_time": "2025-05-05T20:32:23.303192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### (B) Stitch hospitalizations\n",
    "\n",
    "Combine multiple `hospitalization_ids` into a single `encounter_block` for patients who transfer between hospital campuses or return soon after discharge. Hospitalizations that have a gap of **6 hours or less** between the discharge dttm and admission dttm are put in one encounter block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d6a257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:23.325587Z",
     "iopub.status.busy": "2025-05-05T20:32:23.325437Z",
     "iopub.status.idle": "2025-05-05T20:32:23.505179Z",
     "shell.execute_reply": "2025-05-05T20:32:23.504728Z"
    },
    "papermill": {
     "duration": 0.188645,
     "end_time": "2025-05-05T20:32:23.506372",
     "exception": false,
     "start_time": "2025-05-05T20:32:23.317727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter ADT to only those in the cohort set\n",
    "cohort_ids = hospitalization_cohort['hospitalization_id'].unique().tolist()\n",
    "adt_cohort = adt[adt['hospitalization_id'].isin(cohort_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86deb7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:23.521897Z",
     "iopub.status.busy": "2025-05-05T20:32:23.521756Z",
     "iopub.status.idle": "2025-05-05T20:32:23.524741Z",
     "shell.execute_reply": "2025-05-05T20:32:23.524452Z"
    },
    "papermill": {
     "duration": 0.012002,
     "end_time": "2025-05-05T20:32:23.525740",
     "exception": false,
     "start_time": "2025-05-05T20:32:23.513738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for missing values in admission and discharge dates\n",
    "print(\"\\nMissing values in admission_dttm:\", hospitalization_cohort['admission_dttm'].isna().sum())\n",
    "print(\"Missing values in discharge_dttm:\", hospitalization_cohort['discharge_dttm'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655760d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:23.540952Z",
     "iopub.status.busy": "2025-05-05T20:32:23.540787Z",
     "iopub.status.idle": "2025-05-05T20:32:27.345811Z",
     "shell.execute_reply": "2025-05-05T20:32:27.345442Z"
    },
    "papermill": {
     "duration": 3.813947,
     "end_time": "2025-05-05T20:32:27.346864",
     "exception": false,
     "start_time": "2025-05-05T20:32:23.532917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP B: Stitch Encounters => 'encounter_block'\n",
    "# Use stitch_encounters from pyCLIF with time_interval=6\n",
    "print(\"\\n=== STEP B: Stitch encounters ===\\n\")\n",
    "stitched_cohort = pyCLIF.stitch_encounters(hospitalization_cohort, adt_cohort, time_interval=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf287a2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:27.362221Z",
     "iopub.status.busy": "2025-05-05T20:32:27.362077Z",
     "iopub.status.idle": "2025-05-05T20:32:27.437006Z",
     "shell.execute_reply": "2025-05-05T20:32:27.436663Z"
    },
    "papermill": {
     "duration": 0.083553,
     "end_time": "2025-05-05T20:32:27.437928",
     "exception": false,
     "start_time": "2025-05-05T20:32:27.354375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stitched_cohort now has: 'patient_id','hospitalization_id','encounter_block', discharge category and other ADT variables. This will have duplicate rows because of location category\n",
    "# We only want 1 row per unique encounter_block for the next steps.\n",
    "stitched_unique = stitched_cohort[['patient_id', 'encounter_block']].drop_duplicates()\n",
    "\n",
    "strobe_counts['B_before_stitching'] = stitched_cohort['hospitalization_id'].nunique()\n",
    "strobe_counts['B_after_stitching'] = stitched_unique['encounter_block'].nunique()\n",
    "strobe_counts['B_stitched_hosp_ids'] = strobe_counts['B_before_stitching']-strobe_counts['B_after_stitching']\n",
    "print(f\"Number of unique hospitalizations before stitching: {stitched_cohort['hospitalization_id'].nunique()}\")\n",
    "print(f\"Number of unique encounter blocks after stitching: {strobe_counts['B_after_stitching']}\")\n",
    "print(f\"Number of linked hospitalization ids: {strobe_counts['B_before_stitching']-strobe_counts['B_after_stitching']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a78648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:27.453713Z",
     "iopub.status.busy": "2025-05-05T20:32:27.453388Z",
     "iopub.status.idle": "2025-05-05T20:32:27.575145Z",
     "shell.execute_reply": "2025-05-05T20:32:27.574748Z"
    },
    "papermill": {
     "duration": 0.131149,
     "end_time": "2025-05-05T20:32:27.576259",
     "exception": false,
     "start_time": "2025-05-05T20:32:27.445110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mapping of patient id, hospitalization id and encounter blocks\n",
    "all_ids = stitched_cohort[['patient_id', 'hospitalization_id', 'encounter_block', 'discharge_category', 'discharge_dttm']].drop_duplicates()\n",
    "print(\"\\nUnique values in each column:\")\n",
    "for col in all_ids.columns[:3]:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(all_ids[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb24d29",
   "metadata": {
    "papermill": {
     "duration": 0.007682,
     "end_time": "2025-05-05T20:32:27.591724",
     "exception": false,
     "start_time": "2025-05-05T20:32:27.584042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### (C) Identify ventilator usage\n",
    "\n",
    "Filter down to encounters that received invasive mechanical ventilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84222980",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:27.608033Z",
     "iopub.status.busy": "2025-05-05T20:32:27.607861Z",
     "iopub.status.idle": "2025-05-05T20:32:30.358385Z",
     "shell.execute_reply": "2025-05-05T20:32:30.358041Z"
    },
    "papermill": {
     "duration": 2.76037,
     "end_time": "2025-05-05T20:32:30.359523",
     "exception": false,
     "start_time": "2025-05-05T20:32:27.599153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP C: Identify Ventilator Usage\n",
    "# Load respiratory support only for the relevant â€œhospitalization_idâ€ set\n",
    "# These hospitalizations map to an encounter_block for final grouping.\n",
    "\n",
    "print(\"\\n=== STEP C: Load & process respiratory support => Apply Waterfall & Identify IMV usage ===\\n\")\n",
    "\n",
    "# 1) Load respiratory support\n",
    "resp_support_raw = pyCLIF.load_data(\n",
    "    'clif_respiratory_support',\n",
    "    columns=rst_required_columns,\n",
    "    filters={'hospitalization_id': all_ids['hospitalization_id'].unique().tolist()}\n",
    ")\n",
    "\n",
    "resp_support = resp_support_raw.copy()\n",
    "resp_support['device_category'] = resp_support['device_category'].str.lower()\n",
    "resp_support['mode_category'] = resp_support['mode_category'].str.lower()\n",
    "resp_support['lpm_set'] = pd.to_numeric(resp_support['lpm_set'], errors='coerce')\n",
    "resp_support['resp_rate_set'] = pd.to_numeric(resp_support['resp_rate_set'], errors='coerce')\n",
    "resp_support['peep_set'] = pd.to_numeric(resp_support['peep_set'], errors='coerce')\n",
    "resp_support['resp_rate_obs'] = pd.to_numeric(resp_support['resp_rate_obs'], errors='coerce')\n",
    "resp_support = resp_support.sort_values(['hospitalization_id', 'recorded_dttm'])\n",
    "# del resp_support_raw\n",
    "\n",
    "print(\"\\n=== Apply outlier thresholds ===\\n\")\n",
    "resp_support['fio2_set'] = pd.to_numeric(resp_support['fio2_set'], errors='coerce')\n",
    "# (Optional) If FiO2 is >1 on average => scale by /100\n",
    "fio2_mean = resp_support['fio2_set'].mean(skipna=True)\n",
    "# If the mean is greater than 1, divide 'fio2_set' by 100\n",
    "if fio2_mean and fio2_mean > 1.0:\n",
    "    # Only divide values greater than 1 to avoid re-dividing already correct values\n",
    "    resp_support.loc[resp_support['fio2_set'] > 1, 'fio2_set'] = \\\n",
    "        resp_support.loc[resp_support['fio2_set'] > 1, 'fio2_set'] / 100\n",
    "    print(\"Updated fio2_set to be between 0.21 and 1\")\n",
    "else:\n",
    "    print(\"FIO2_SET mean=\", fio2_mean, \"is within the required range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b31ad5",
   "metadata": {
    "papermill": {
     "duration": 0.007496,
     "end_time": "2025-05-05T20:32:30.374464",
     "exception": false,
     "start_time": "2025-05-05T20:32:30.366968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Respiratory Support Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48293d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:30.389581Z",
     "iopub.status.busy": "2025-05-05T20:32:30.389449Z",
     "iopub.status.idle": "2025-05-05T20:32:31.169625Z",
     "shell.execute_reply": "2025-05-05T20:32:31.169252Z"
    },
    "papermill": {
     "duration": 0.789198,
     "end_time": "2025-05-05T20:32:31.170923",
     "exception": false,
     "start_time": "2025-05-05T20:32:30.381725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_list = []\n",
    "group_cols = 'device_category'  # or a list like ['device_category','mode_category']\n",
    "numeric_cols = ['fio2_set','peep_set','lpm_set', 'resp_rate_set', 'resp_rate_obs']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    tmp = pyCLIF.create_summary_table(\n",
    "        df=resp_support,\n",
    "        numeric_col=col,\n",
    "        group_by_cols=group_cols\n",
    "    )\n",
    "    #   ['device_category','N','missing','min','q25','median','q75','mean','max']\n",
    "    # Insert a \"variable\" column next to the group-by columns:\n",
    "    tmp['variable'] = col\n",
    "    # We want \"device_category\" (the group col), then \"variable\", then the rest\n",
    "    if isinstance(group_cols, str):\n",
    "        group_cols_list = [group_cols]  # unify into list\n",
    "    else:\n",
    "        group_cols_list = group_cols  # already a list\n",
    "    # Reorder so that group-by columns come first, then 'variable', then the rest\n",
    "    front_cols = group_cols_list + ['variable']\n",
    "    # Build the list of remaining columns\n",
    "    rest_cols = [c for c in tmp.columns if c not in front_cols]\n",
    "    new_cols = front_cols + rest_cols\n",
    "    tmp = tmp[new_cols]\n",
    "    results_list.append(tmp)\n",
    "\n",
    "# concatenate all results\n",
    "final_summary_resp_support = pd.concat(results_list, ignore_index=True)\n",
    "final_summary_resp_support.to_csv('../output/final/summary_respiratory_support_by_device.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af9c109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:31.186923Z",
     "iopub.status.busy": "2025-05-05T20:32:31.186785Z",
     "iopub.status.idle": "2025-05-05T20:32:32.097567Z",
     "shell.execute_reply": "2025-05-05T20:32:32.097174Z"
    },
    "papermill": {
     "duration": 0.92006,
     "end_time": "2025-05-05T20:32:32.098663",
     "exception": false,
     "start_time": "2025-05-05T20:32:31.178603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_list = []\n",
    "group_cols = ['device_category','mode_category']\n",
    "numeric_cols = ['fio2_set','peep_set','lpm_set', 'resp_rate_set', 'resp_rate_obs']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    tmp = pyCLIF.create_summary_table(\n",
    "        df=resp_support,\n",
    "        numeric_col=col,\n",
    "        group_by_cols=group_cols\n",
    "    )\n",
    "    # tmp might have columns:\n",
    "    #   ['device_category','N','missing','min','q25','median','q75','mean','max']\n",
    "    # Insert a \"variable\" column next to the group-by columns:\n",
    "    tmp['variable'] = col\n",
    "    # We want \"device_category\" (the group col), then \"variable\", then the rest\n",
    "    if isinstance(group_cols, str):\n",
    "        group_cols_list = [group_cols]  # unify into list\n",
    "    else:\n",
    "        group_cols_list = group_cols  # already a list\n",
    "    # Reorder so that group-by columns come first, then 'variable', then the rest\n",
    "    front_cols = group_cols_list + ['variable']\n",
    "    # Build the list of remaining columns\n",
    "    rest_cols = [c for c in tmp.columns if c not in front_cols]\n",
    "    new_cols = front_cols + rest_cols\n",
    "    tmp = tmp[new_cols]\n",
    "    results_list.append(tmp)\n",
    "\n",
    "# Finally, concatenate all results\n",
    "final_summary_resp_support = pd.concat(results_list, ignore_index=True)\n",
    "final_summary_resp_support.to_csv('../output/final/summary_respiratory_support_by_device_mode.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2766f16",
   "metadata": {
    "papermill": {
     "duration": 0.007225,
     "end_time": "2025-05-05T20:32:32.113442",
     "exception": false,
     "start_time": "2025-05-05T20:32:32.106217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### (C.1) Waterfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e2e3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:32.128933Z",
     "iopub.status.busy": "2025-05-05T20:32:32.128782Z",
     "iopub.status.idle": "2025-05-05T20:32:32.876324Z",
     "shell.execute_reply": "2025-05-05T20:32:32.875948Z"
    },
    "papermill": {
     "duration": 0.756549,
     "end_time": "2025-05-05T20:32:32.877315",
     "exception": false,
     "start_time": "2025-05-05T20:32:32.120766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Identify encounters on IMV\n",
    "# Create mask to identify IMV entries\n",
    "imv_mask = resp_support['device_category'].str.contains(\"imv\", case=False, na=False)\n",
    "\n",
    "# Get unique hospitalization_ids with at least one IMV entry\n",
    "resp_stitched_imv_ids = resp_support[imv_mask][['hospitalization_id']].drop_duplicates()\n",
    "\n",
    "# Filter the full table to just these hospitalization_ids\n",
    "resp_support_filtered = resp_support[\n",
    "    resp_support[\"hospitalization_id\"].isin(resp_stitched_imv_ids[\"hospitalization_id\"])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# filter down to only those hospitalization_ids that are in the cohort\n",
    "all_ids = all_ids[all_ids['hospitalization_id'].isin(resp_support_filtered['hospitalization_id'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a3d3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:32:32.893114Z",
     "iopub.status.busy": "2025-05-05T20:32:32.892978Z",
     "iopub.status.idle": "2025-05-05T20:34:46.369831Z",
     "shell.execute_reply": "2025-05-05T20:34:46.369485Z"
    },
    "papermill": {
     "duration": 133.48615,
     "end_time": "2025-05-05T20:34:46.370836",
     "exception": false,
     "start_time": "2025-05-05T20:32:32.884686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandas\")\n",
    "\n",
    "processed_resp_support = waterfall.process_resp_support_waterfall(resp_support_filtered, \n",
    "                                                        id_col = \"hospitalization_id\",\n",
    "                                                        verbose = True)\n",
    "\n",
    "processed_resp_support = pyCLIF.convert_datetime_columns_to_site_tz(processed_resp_support, pyCLIF.helper['timezone'])\n",
    "processed_resp_support.to_parquet('../output/intermediate/processed_resp_support.parquet', index=False)\n",
    "# processed_resp_support = pd.read_parquet('../output_old/intermediate/processed_resp_support.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2908139a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:34:46.387546Z",
     "iopub.status.busy": "2025-05-05T20:34:46.387414Z",
     "iopub.status.idle": "2025-05-05T20:34:46.745154Z",
     "shell.execute_reply": "2025-05-05T20:34:46.744821Z"
    },
    "papermill": {
     "duration": 0.367264,
     "end_time": "2025-05-05T20:34:46.745990",
     "exception": false,
     "start_time": "2025-05-05T20:34:46.378726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge to get encounter_block for the cohort identified so far\n",
    "resp_stitched = processed_resp_support.merge(\n",
    "    all_ids[['hospitalization_id','encounter_block']],\n",
    "    on='hospitalization_id', how='right'\n",
    ")\n",
    "\n",
    "print(\"Missing values in recorded_dttm:\", resp_stitched['recorded_dttm'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a5ccd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:34:46.763130Z",
     "iopub.status.busy": "2025-05-05T20:34:46.762995Z",
     "iopub.status.idle": "2025-05-05T20:34:46.784303Z",
     "shell.execute_reply": "2025-05-05T20:34:46.783849Z"
    },
    "papermill": {
     "duration": 0.031113,
     "end_time": "2025-05-05T20:34:46.785263",
     "exception": false,
     "start_time": "2025-05-05T20:34:46.754150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'fio2_set', *outlier_cfg['fio2_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'peep_set', *outlier_cfg['peep_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'lpm_set',  *outlier_cfg['lpm_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'resp_rate_set', *outlier_cfg['resp_rate_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'resp_rate_obs', *outlier_cfg['resp_rate_obs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f689914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill values of fio2_set if the device is nasal cannula, and lpm_set is available\n",
    "# https://www.respiratorytherapyzone.com/oxygen-flow-rate-fio2/\n",
    "resp_stitched = pyCLIF.impute_fio2_from_nasal_cannula_flow(resp_stitched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10313e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:34:46.802163Z",
     "iopub.status.busy": "2025-05-05T20:34:46.802027Z",
     "iopub.status.idle": "2025-05-05T20:34:47.395403Z",
     "shell.execute_reply": "2025-05-05T20:34:47.395088Z"
    },
    "papermill": {
     "duration": 0.603277,
     "end_time": "2025-05-05T20:34:47.396318",
     "exception": false,
     "start_time": "2025-05-05T20:34:46.793041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4) Identify IMV\n",
    "imv_mask = resp_stitched['device_category'].str.contains(\"imv\", case=False, na=False)\n",
    "resp_stitched_imv = resp_stitched[imv_mask].copy()\n",
    "# this creates a on vent field for everytime the patient is on a vent\n",
    "# Create on_vent column for IMV records\n",
    "resp_stitched_imv['on_vent'] = 1\n",
    "\n",
    "# Left join back to full resp_stitched to include non-vent records\n",
    "resp_stitched_final = resp_stitched.merge(\n",
    "    resp_stitched_imv[['hospitalization_id', 'recorded_dttm', 'on_vent']], \n",
    "    on=['hospitalization_id', 'recorded_dttm'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill NaN values with 0 for times when not on vent\n",
    "resp_stitched_final['on_vent'] = resp_stitched_final['on_vent'].fillna(0)\n",
    "strobe_counts['C_imv_hospitalizations'] = resp_stitched_final['hospitalization_id'].nunique()\n",
    "strobe_counts['C_imv_encounter_blocks'] = resp_stitched_final['encounter_block'].nunique()\n",
    "\n",
    "print(f\"Total IMV respiratory support hospitalizations: {strobe_counts['C_imv_hospitalizations']}\")\n",
    "print(f\"Total IMV respiratory support encounter blocks: {strobe_counts['C_imv_encounter_blocks']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd287c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:34:47.413556Z",
     "iopub.status.busy": "2025-05-05T20:34:47.413421Z",
     "iopub.status.idle": "2025-05-05T20:34:47.452586Z",
     "shell.execute_reply": "2025-05-05T20:34:47.452204Z"
    },
    "papermill": {
     "duration": 0.049194,
     "end_time": "2025-05-05T20:34:47.453635",
     "exception": false,
     "start_time": "2025-05-05T20:34:47.404441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_ids =  all_ids[all_ids['encounter_block'].isin(resp_stitched_final['encounter_block'].unique())]\n",
    "all_ids = all_ids[all_ids['hospitalization_id'].isin(resp_stitched_final['hospitalization_id'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a269100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:34:47.470903Z",
     "iopub.status.busy": "2025-05-05T20:34:47.470764Z",
     "iopub.status.idle": "2025-05-05T20:34:47.474203Z",
     "shell.execute_reply": "2025-05-05T20:34:47.473946Z"
    },
    "papermill": {
     "duration": 0.013172,
     "end_time": "2025-05-05T20:34:47.474906",
     "exception": false,
     "start_time": "2025-05-05T20:34:47.461734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in all_ids.columns[:3]:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(all_ids[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70512b3",
   "metadata": {
    "papermill": {
     "duration": 0.007618,
     "end_time": "2025-05-05T20:34:47.490435",
     "exception": false,
     "start_time": "2025-05-05T20:34:47.482817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### (D) Vent start and end times \n",
    "\n",
    "Calculate vent start times for the first episode of invasive mechanical intubation.   \n",
    "Limitation: the vent end time might not be associated with the same intubation episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa491b8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:34:47.506880Z",
     "iopub.status.busy": "2025-05-05T20:34:47.506675Z",
     "iopub.status.idle": "2025-05-05T20:34:47.593893Z",
     "shell.execute_reply": "2025-05-05T20:34:47.593527Z"
    },
    "papermill": {
     "duration": 0.096451,
     "end_time": "2025-05-05T20:34:47.594715",
     "exception": false,
     "start_time": "2025-05-05T20:34:47.498264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP E: Determine Vent Start/End for Each Hospitalization and Encounter block\n",
    "\n",
    "print(\"\\n=== STEP D: Determine ventilation times (start/end) at d encounter block level ===\\n\")\n",
    "\n",
    "# at the hospitalization id level\n",
    "vent_start_end = resp_stitched_imv.groupby('hospitalization_id').agg(\n",
    "    vent_start_time=('recorded_dttm','min'),\n",
    "    vent_end_time=('recorded_dttm','max')\n",
    ").reset_index()\n",
    "\n",
    "# Exclude edge case: if start_time == end_time \n",
    "# these would otherwise have been excluded when we remove encounters on vent for less than 4 hours\n",
    "check_same_vent_start_end = vent_start_end[vent_start_end['vent_start_time'] == vent_start_end['vent_end_time']].copy()\n",
    "vent_start_end= vent_start_end[vent_start_end['vent_start_time'] != vent_start_end['vent_end_time']].copy()\n",
    "\n",
    "strobe_counts['D_hospitalizations_with_valid_vent'] = vent_start_end['hospitalization_id'].nunique()\n",
    "strobe_counts['D_hospitalizations_with_same_vent_start_end'] = check_same_vent_start_end['hospitalization_id'].nunique()\n",
    "print(f\"Unique hospitalizations with valid IMV start/end: {strobe_counts['D_hospitalizations_with_valid_vent']}\")\n",
    "\n",
    "# at the block level\n",
    "block_vent_times = resp_stitched_imv.groupby('encounter_block', dropna=True).agg(\n",
    "    block_vent_start_dttm=('recorded_dttm','min'),\n",
    "    block_vent_end_dttm=('recorded_dttm','max')\n",
    ").reset_index()\n",
    "\n",
    "# If start==end, no real vent- there was just ONE vent entry\n",
    "block_same_vent = block_vent_times[block_vent_times['block_vent_start_dttm']==block_vent_times['block_vent_end_dttm']].copy()\n",
    "block_vent_times = block_vent_times[block_vent_times['block_vent_start_dttm']!=block_vent_times['block_vent_end_dttm']].copy()\n",
    "\n",
    "strobe_counts['D_blocks_with_valid_vent'] = block_vent_times['encounter_block'].nunique()\n",
    "strobe_counts['D_blocks_with_same_vent_start_end'] = block_same_vent['encounter_block'].nunique()\n",
    "print(f\"Unique encounter blocks with valid IMV start/end: {strobe_counts['D_blocks_with_valid_vent']}\")\n",
    "\n",
    "valid_blocks_vent = block_vent_times['encounter_block'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ef2e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:34:47.611844Z",
     "iopub.status.busy": "2025-05-05T20:34:47.611676Z",
     "iopub.status.idle": "2025-05-05T20:34:47.615631Z",
     "shell.execute_reply": "2025-05-05T20:34:47.615284Z"
    },
    "papermill": {
     "duration": 0.013608,
     "end_time": "2025-05-05T20:34:47.616467",
     "exception": false,
     "start_time": "2025-05-05T20:34:47.602859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd3122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:34:47.633160Z",
     "iopub.status.busy": "2025-05-05T20:34:47.633015Z",
     "iopub.status.idle": "2025-05-05T20:34:47.636005Z",
     "shell.execute_reply": "2025-05-05T20:34:47.635752Z"
    },
    "papermill": {
     "duration": 0.012157,
     "end_time": "2025-05-05T20:34:47.636731",
     "exception": false,
     "start_time": "2025-05-05T20:34:47.624574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter all_ids to only keep rows where encounter_block is in valid_blocks_vent\n",
    "all_ids = all_ids[all_ids['encounter_block'].isin(valid_blocks_vent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c28059",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:34:47.654241Z",
     "iopub.status.busy": "2025-05-05T20:34:47.653985Z",
     "iopub.status.idle": "2025-05-05T20:34:47.658852Z",
     "shell.execute_reply": "2025-05-05T20:34:47.658382Z"
    },
    "papermill": {
     "duration": 0.015107,
     "end_time": "2025-05-05T20:34:47.659840",
     "exception": false,
     "start_time": "2025-05-05T20:34:47.644733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in all_ids.columns[:3]:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(all_ids[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7f258",
   "metadata": {
    "papermill": {
     "duration": 0.008818,
     "end_time": "2025-05-05T20:34:47.677539",
     "exception": false,
     "start_time": "2025-05-05T20:34:47.668721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### (E) Hourly Sequence \n",
    "\n",
    "This section achieves the following steps:  \n",
    "* Identifies the first and last recorded times for vitals for each encounter block\n",
    "* These times are used to generate an hourly sequence of patients hospitalization journey\n",
    "* Combines with hourly vent usage data from the respiratory support table\n",
    "* Excludes encounters on vent for less than 4 hours in the first 72 hours\n",
    "* Creates a final dataframe with the identified cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0942c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:34:47.694315Z",
     "iopub.status.busy": "2025-05-05T20:34:47.694084Z",
     "iopub.status.idle": "2025-05-05T20:34:52.498996Z",
     "shell.execute_reply": "2025-05-05T20:34:52.498641Z"
    },
    "papermill": {
     "duration": 4.814319,
     "end_time": "2025-05-05T20:34:52.499956",
     "exception": false,
     "start_time": "2025-05-05T20:34:47.685637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate Hourly Sequence & Exclude encounter blocks with <4 Vent Hours\n",
    "#  Create an hourly timeline from vent_start to last vital or outcome time for each encounter block\n",
    "# We stop operating at hospitalization id level \n",
    "\n",
    "print(\"\\n=== STEP E: Hourly sequence generation & < 4 hour vent exclusion BLOCK level===\\n\")\n",
    "\n",
    "# 1) define the 'end_time' for the sequence from vitals or outcome.\n",
    "vitals_cohort = pyCLIF.load_data('clif_vitals',\n",
    "    columns=vitals_required_columns,\n",
    "    filters={'hospitalization_id': all_ids['hospitalization_id'].unique().tolist(), \n",
    "             'vital_category': vitals_of_interest}\n",
    ")\n",
    "vitals_cohort = pyCLIF.convert_datetime_columns_to_site_tz(vitals_cohort, pyCLIF.helper['timezone'])\n",
    "vitals_cohort['vital_value'] = pd.to_numeric(vitals_cohort['vital_value'], errors='coerce')\n",
    "# sort vitals cohort by hospitalization_id and recorded_dttm\n",
    "vitals_cohort = vitals_cohort.sort_values(['hospitalization_id', 'recorded_dttm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51516fbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:34:52.517405Z",
     "iopub.status.busy": "2025-05-05T20:34:52.517264Z",
     "iopub.status.idle": "2025-05-05T20:34:55.699640Z",
     "shell.execute_reply": "2025-05-05T20:34:55.699299Z"
    },
    "papermill": {
     "duration": 3.192027,
     "end_time": "2025-05-05T20:34:55.700751",
     "exception": false,
     "start_time": "2025-05-05T20:34:52.508724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace outliers with NAs in the vitals table \n",
    "# Extract min/max values from config for each vital\n",
    "min_hr, max_hr = outlier_cfg['heart_rate']\n",
    "min_rr, max_rr = outlier_cfg['respiratory_rate'] \n",
    "min_sbp, max_sbp = outlier_cfg['sbp']\n",
    "min_dbp, max_dbp = outlier_cfg['dbp']\n",
    "min_map, max_map = outlier_cfg['map']\n",
    "min_spo2, max_spo2 = outlier_cfg['spo2']\n",
    "min_weight, max_weight = outlier_cfg['weight_kg']\n",
    "min_height, max_height = outlier_cfg['height_cm']\n",
    "\n",
    "# For each vital category, set out-of-range values to NaN\n",
    "is_hr = vitals_cohort['vital_category'] == 'heart_rate'\n",
    "vitals_cohort.loc[is_hr & (vitals_cohort['vital_value'] < min_hr), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_hr & (vitals_cohort['vital_value'] > max_hr), 'vital_value'] = np.nan\n",
    "\n",
    "is_rr = vitals_cohort['vital_category'] == 'respiratory_rate'\n",
    "vitals_cohort.loc[is_rr & (vitals_cohort['vital_value'] < min_rr), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_rr & (vitals_cohort['vital_value'] > max_rr), 'vital_value'] = np.nan\n",
    "\n",
    "is_sbp = vitals_cohort['vital_category'] == 'sbp'\n",
    "vitals_cohort.loc[is_sbp & (vitals_cohort['vital_value'] < min_sbp), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_sbp & (vitals_cohort['vital_value'] > max_sbp), 'vital_value'] = np.nan\n",
    "\n",
    "is_dbp = vitals_cohort['vital_category'] == 'dbp'\n",
    "vitals_cohort.loc[is_dbp & (vitals_cohort['vital_value'] < min_dbp), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_dbp & (vitals_cohort['vital_value'] > max_dbp), 'vital_value'] = np.nan\n",
    "\n",
    "is_map = vitals_cohort['vital_category'] == 'map'\n",
    "vitals_cohort.loc[is_map & (vitals_cohort['vital_value'] < min_map), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_map & (vitals_cohort['vital_value'] > max_map), 'vital_value'] = np.nan\n",
    "\n",
    "is_spo2 = vitals_cohort['vital_category'] == 'spo2'\n",
    "vitals_cohort.loc[is_spo2 & (vitals_cohort['vital_value'] < min_spo2), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_spo2 & (vitals_cohort['vital_value'] > max_spo2), 'vital_value'] = np.nan\n",
    "\n",
    "is_weight = vitals_cohort['vital_category'] == 'weight_kg'\n",
    "vitals_cohort.loc[is_weight & (vitals_cohort['vital_value'] < min_weight), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_weight & (vitals_cohort['vital_value'] > max_weight), 'vital_value'] = np.nan\n",
    "\n",
    "is_height = vitals_cohort['vital_category'] == 'height_cm'\n",
    "vitals_cohort.loc[is_height & (vitals_cohort['vital_value'] < min_height), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_height & (vitals_cohort['vital_value'] > max_height), 'vital_value'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d7131f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:34:55.719131Z",
     "iopub.status.busy": "2025-05-05T20:34:55.718858Z",
     "iopub.status.idle": "2025-05-05T20:34:56.744635Z",
     "shell.execute_reply": "2025-05-05T20:34:56.744251Z"
    },
    "papermill": {
     "duration": 1.036579,
     "end_time": "2025-05-05T20:34:56.745725",
     "exception": false,
     "start_time": "2025-05-05T20:34:55.709146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_vitals = pyCLIF.create_summary_table(\n",
    "        df=vitals_cohort,\n",
    "        numeric_col='vital_value',\n",
    "        group_by_cols='vital_category'\n",
    "    )\n",
    "summary_vitals.to_csv('../output/final/summary_vitals_by_category.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84825ff6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:34:56.763895Z",
     "iopub.status.busy": "2025-05-05T20:34:56.763740Z",
     "iopub.status.idle": "2025-05-05T20:34:59.541163Z",
     "shell.execute_reply": "2025-05-05T20:34:59.540813Z"
    },
    "papermill": {
     "duration": 2.787985,
     "end_time": "2025-05-05T20:34:59.542123",
     "exception": false,
     "start_time": "2025-05-05T20:34:56.754138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge to get encounter_block on each vital\n",
    "vitals_stitched = vitals_cohort.merge(all_ids, on='hospitalization_id', how='left')\n",
    "# Group by block => find earliest & latest vital for that block\n",
    "vital_bounds_block = vitals_stitched.groupby('encounter_block', dropna=True)['recorded_dttm'].agg(['min','max']).reset_index()\n",
    "vital_bounds_block.columns = ['encounter_block','block_first_vital_dttm','block_last_vital_dttm']\n",
    "\n",
    "# 2) Merge block_vent_times with vital_bounds_block\n",
    "final_blocks = block_vent_times.merge(vital_bounds_block, on='encounter_block', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023a684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:34:59.560955Z",
     "iopub.status.busy": "2025-05-05T20:34:59.560810Z",
     "iopub.status.idle": "2025-05-05T20:34:59.564039Z",
     "shell.execute_reply": "2025-05-05T20:34:59.563762Z"
    },
    "papermill": {
     "duration": 0.013633,
     "end_time": "2025-05-05T20:34:59.564793",
     "exception": false,
     "start_time": "2025-05-05T20:34:59.551160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3) If block_last_vital_dttm < vent_start_time => weird edge case. Ideally shouldn't happen. \n",
    "# If such bad blocks exist, check your CLIF tables bro\n",
    "bad_block = final_blocks[final_blocks['block_last_vital_dttm'] < final_blocks['block_vent_start_dttm']]\n",
    "strobe_counts['E_blocks_with_vent_end_before_vital_start'] = bad_block['encounter_block'].nunique()\n",
    "if len(bad_block) > 0:\n",
    "    print(\"Warning: Some blocks have last vital < vent start:\\n\", len(bad_block))\n",
    "else:\n",
    "    print(\"There are no bad blocks! Good job CLIF-ing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff50a3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:34:59.582070Z",
     "iopub.status.busy": "2025-05-05T20:34:59.581946Z",
     "iopub.status.idle": "2025-05-05T20:35:01.118874Z",
     "shell.execute_reply": "2025-05-05T20:35:01.118451Z"
    },
    "papermill": {
     "duration": 1.546714,
     "end_time": "2025-05-05T20:35:01.119930",
     "exception": false,
     "start_time": "2025-05-05T20:34:59.573216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4) Generate the hourly sequence at block level\n",
    "def generate_hourly_sequence_block(group):\n",
    "    blk = group.name  # use group name from groupby\n",
    "    start_time = group['block_vent_start_dttm'].iloc[0]\n",
    "    end_time   = group['block_last_vital_dttm'].iloc[0]\n",
    "    hourly_timestamps = pd.date_range(start=start_time, end=end_time, freq='h')\n",
    "    return pd.DataFrame({\n",
    "        'encounter_block': blk,\n",
    "        'recorded_dttm': hourly_timestamps\n",
    "    })\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "    hourly_seq_block = (\n",
    "    final_blocks\n",
    "    .groupby('encounter_block')\n",
    "    .apply(generate_hourly_sequence_block)\n",
    "    .reset_index(drop=True)\n",
    "    )\n",
    "hourly_seq_block = hourly_seq_block.reset_index(drop=True)\n",
    "\n",
    "hourly_seq_block['recorded_date'] = hourly_seq_block['recorded_dttm'].dt.date\n",
    "hourly_seq_block['recorded_hour'] = hourly_seq_block['recorded_dttm'].dt.hour\n",
    "hourly_seq_block = hourly_seq_block.drop(columns=['recorded_dttm'])\n",
    "hourly_seq_block = hourly_seq_block.drop_duplicates(subset=['encounter_block', 'recorded_date', 'recorded_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fbebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Combine with actual vent usage by hour\n",
    "resp_stitched_final = resp_stitched_final[resp_stitched_final['encounter_block'].isin(all_ids['encounter_block'])].copy()\n",
    "resp_stitched_final['recorded_date'] = resp_stitched_final['recorded_dttm'].dt.date\n",
    "resp_stitched_final['recorded_hour'] = resp_stitched_final['recorded_dttm'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6abccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill tracheostomy within each encounter_block BEFORE hourly aggregation\n",
    "print(\"Forward filling tracheostomy within encounter blocks...\")\n",
    "\n",
    "# Sort data properly\n",
    "resp_stitched_final = resp_stitched_final.sort_values(['encounter_block', 'recorded_dttm'])\n",
    "\n",
    "# Forward fill tracheostomy within each encounter_block\n",
    "resp_stitched_final['tracheostomy_filled'] = (\n",
    "    resp_stitched_final.groupby('encounter_block')['tracheostomy']\n",
    "    .transform(lambda x: x.ffill())\n",
    ")\n",
    "\n",
    "# Fill any remaining NaN values with 0 (no trach)\n",
    "resp_stitched_final['tracheostomy_filled'] = resp_stitched_final['tracheostomy_filled'].fillna(0)\n",
    "\n",
    "# Show the impact\n",
    "before_blocks = resp_stitched_final[resp_stitched_final['tracheostomy'] == 1]['encounter_block'].nunique()\n",
    "after_blocks = resp_stitched_final[resp_stitched_final['tracheostomy_filled'] == 1]['encounter_block'].nunique()\n",
    "\n",
    "print(f\"Blocks with trach (before forward fill): {before_blocks}\")\n",
    "print(f\"Blocks with trach (after forward fill): {after_blocks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621f4c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:01.325996Z",
     "iopub.status.busy": "2025-05-05T20:35:01.325734Z",
     "iopub.status.idle": "2025-05-05T20:35:14.484567Z",
     "shell.execute_reply": "2025-05-05T20:35:14.484183Z"
    },
    "papermill": {
     "duration": 13.168895,
     "end_time": "2025-05-05T20:35:14.485852",
     "exception": false,
     "start_time": "2025-05-05T20:35:01.316957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hourly_vent_block = resp_stitched_final.groupby(['encounter_block','recorded_date','recorded_hour']).agg(\n",
    "    min_fio2_set=('fio2_set','min'),\n",
    "    max_fio2_set=('fio2_set','max'),\n",
    "    min_peep_set=('peep_set','min'),\n",
    "    max_peep_set=('peep_set','max'),\n",
    "    min_lpm_set=('lpm_set', 'min'),\n",
    "    max_lpm_set=('lpm_set', 'max'),\n",
    "    min_resp_rate_obs=('resp_rate_obs', 'min'),\n",
    "    max_resp_rate_obs=('resp_rate_obs', 'max'),\n",
    "    hourly_trach=('tracheostomy_filled', 'max'), # 1 if any value within that hour is 1\n",
    "    hourly_on_vent=('on_vent','max'),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa8dea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:14.504056Z",
     "iopub.status.busy": "2025-05-05T20:35:14.503889Z",
     "iopub.status.idle": "2025-05-05T20:35:14.517464Z",
     "shell.execute_reply": "2025-05-05T20:35:14.517115Z"
    },
    "papermill": {
     "duration": 0.024222,
     "end_time": "2025-05-05T20:35:14.518650",
     "exception": false,
     "start_time": "2025-05-05T20:35:14.494428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity check- Find encounter_blocks that are in hourly_seq_block but not in hourly_vent_block and vice versa\n",
    "# This is possible when the patient is put on IMV in the ED, and dies shortly after. \n",
    "# Still might be worth exploring the trajectory for these patients \n",
    "seq_blocks = set(hourly_seq_block['encounter_block'].unique())\n",
    "vent_blocks = set(hourly_vent_block['encounter_block'].unique())\n",
    "\n",
    "blocks_in_seq_not_vent = seq_blocks - vent_blocks\n",
    "blocks_in_vent_not_seq = vent_blocks - seq_blocks\n",
    "\n",
    "print(\"Blocks in hourly_seq_block but not in hourly_vent_block:\", len(blocks_in_seq_not_vent))\n",
    "if len(blocks_in_seq_not_vent) > 0:\n",
    "    print(sorted(list(blocks_in_seq_not_vent)))\n",
    "print(\"\\nBlocks in hourly_vent_block but not in hourly_seq_block:\", len(blocks_in_vent_not_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e05ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We want all hours from hourly_seq block, and \n",
    "# any extra hours from hourly_vent_block that occur after the last hour in hourly_seq_block\n",
    "# This is to ensure that we capture all the hours of ventilation, even if they are not in the hourly_seq_block\n",
    "\n",
    "# Step 1: Reconstruct timestamps\n",
    "hourly_seq_block['recorded_dttm'] = pd.to_datetime(hourly_seq_block['recorded_date']) + pd.to_timedelta(hourly_seq_block['recorded_hour'], unit='h')\n",
    "hourly_vent_block['recorded_dttm'] = pd.to_datetime(hourly_vent_block['recorded_date']) + pd.to_timedelta(hourly_vent_block['recorded_hour'], unit='h')\n",
    "\n",
    "# Step 2: Get max scaffold time per encounter\n",
    "max_times = (\n",
    "    hourly_seq_block.groupby('encounter_block')['recorded_dttm']\n",
    "    .max().reset_index()\n",
    "    .rename(columns={'recorded_dttm': 'max_seq_dttm'})\n",
    ")\n",
    "\n",
    "# Step 3: Identify extra vent rows beyond scaffold\n",
    "vent_plus_max = pd.merge(hourly_vent_block, max_times, on='encounter_block', how='left')\n",
    "\n",
    "extra_rows = vent_plus_max[\n",
    "    vent_plus_max['recorded_dttm'] > vent_plus_max['max_seq_dttm']\n",
    "].copy()\n",
    "\n",
    "# Step 4: Create gap-filler rows for each encounter with extra data\n",
    "gap_rows = []\n",
    "for enc_id, group in extra_rows.groupby('encounter_block'):\n",
    "    max_time = pd.to_datetime(\n",
    "        max_times.loc[max_times['encounter_block'] == enc_id, 'max_seq_dttm'].values[0]\n",
    "    )\n",
    "    first_extra_time = group['recorded_dttm'].min()\n",
    "    \n",
    "    # Skip if there's no gap\n",
    "    if first_extra_time <= max_time + timedelta(hours=1):\n",
    "        continue\n",
    "\n",
    "    # Fill hourly timestamps between scaffold end and first extra\n",
    "    gap_times = pd.date_range(\n",
    "        start=max_time + timedelta(hours=1),\n",
    "        end=first_extra_time - timedelta(hours=1),\n",
    "        freq='H'\n",
    "    )\n",
    "\n",
    "    for dt in gap_times:\n",
    "        gap_rows.append({\n",
    "            'encounter_block': enc_id,\n",
    "            'recorded_date': dt.date(),\n",
    "            'recorded_hour': dt.hour,\n",
    "            'recorded_dttm': dt\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "gap_df = pd.DataFrame(gap_rows)\n",
    "\n",
    "# Step 5: Add all required columns to gap_df, using NA defaults\n",
    "missing_cols = set(hourly_vent_block.columns) - set(gap_df.columns)\n",
    "for col in missing_cols:\n",
    "    gap_df[col] = np.nan\n",
    "\n",
    "# Ensure column order matches\n",
    "gap_df = gap_df[hourly_vent_block.columns]\n",
    "\n",
    "# Step 6: Get scaffold rows with vent info via left join\n",
    "scaffold_df = pd.merge(\n",
    "    hourly_seq_block.drop(columns='recorded_dttm'),\n",
    "    hourly_vent_block.drop(columns='recorded_dttm'),\n",
    "    on=['encounter_block', 'recorded_date', 'recorded_hour'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "gap_df = gap_df.drop(columns='recorded_dttm', errors='ignore')\n",
    "extra_rows = extra_rows.drop(columns='recorded_dttm', errors='ignore')\n",
    "extra_rows = extra_rows.drop(columns='max_seq_dttm', errors='ignore')\n",
    "# Step 7: Combine all three\n",
    "final_df_block = pd.concat([scaffold_df, gap_df, extra_rows], ignore_index=True)\n",
    "\n",
    "# Step 8: Sort\n",
    "final_df_block = final_df_block.sort_values(\n",
    "    by=['encounter_block', 'recorded_date', 'recorded_hour']\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Step 9: Add time_from_vent\n",
    "final_df_block['time_from_vent'] = final_df_block.groupby('encounter_block').cumcount()\n",
    "final_df_block['time_from_vent_adjusted'] = np.where(\n",
    "    final_df_block['time_from_vent'] < 4, -1, final_df_block['time_from_vent'] - 4\n",
    ")\n",
    "\n",
    "# arrange columns as 'encounter_block', 'recorded_date', 'recorded_hour' 'time_from_vent' 'time_from_vent_adjusted' and then the rest\n",
    "cols = ['encounter_block', 'recorded_date', 'recorded_hour', 'time_from_vent', 'time_from_vent_adjusted']\n",
    "cols += [col for col in final_df_block.columns if col not in cols]\n",
    "final_df_block = final_df_block[cols]\n",
    "\n",
    "print(\"Final shape:\", final_df_block.shape)\n",
    "print(\"Unique encounter_blocks:\", final_df_block['encounter_block'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d537a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:15.210464Z",
     "iopub.status.busy": "2025-05-05T20:35:15.210316Z",
     "iopub.status.idle": "2025-05-05T20:35:15.237754Z",
     "shell.execute_reply": "2025-05-05T20:35:15.237398Z"
    },
    "papermill": {
     "duration": 0.038382,
     "end_time": "2025-05-05T20:35:15.238854",
     "exception": false,
     "start_time": "2025-05-05T20:35:15.200472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 7) Count how many vent hours per block in the first 72 hours after first intubation,\n",
    "#  Exclude <4 hours on vent in first 72 hours at block level- They cannot meaningfully be studied for early mobilization if theyâ€™re barely intubated.. including them could bias results\n",
    "first_72_hours = final_df_block[(final_df_block['time_from_vent'] >= 0) & (final_df_block['time_from_vent'] < 72)]\n",
    "\n",
    "# forward fill the hourly_on_vent column in first_72_hours\n",
    "first_72_hours['hourly_on_vent'] = first_72_hours['hourly_on_vent'].ffill()\n",
    "first_72_hours['hourly_trach'] = first_72_hours['hourly_trach'].ffill()\n",
    "vent_hours_per_block = first_72_hours.groupby('encounter_block')['hourly_on_vent'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514c269e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:15.256666Z",
     "iopub.status.busy": "2025-05-05T20:35:15.256528Z",
     "iopub.status.idle": "2025-05-05T20:35:15.445485Z",
     "shell.execute_reply": "2025-05-05T20:35:15.445175Z"
    },
    "papermill": {
     "duration": 0.198943,
     "end_time": "2025-05-05T20:35:15.446373",
     "exception": false,
     "start_time": "2025-05-05T20:35:15.247430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exclude blocks with imv for 4 hours or less\n",
    "blocks_under_4 = vent_hours_per_block[vent_hours_per_block < 4].index\n",
    "blocks_under_4_df = final_df_block[final_df_block['encounter_block'].isin(blocks_under_4)]\n",
    "final_df_block = final_df_block[~final_df_block['encounter_block'].isin(blocks_under_4)]\n",
    "\n",
    "strobe_counts['G_blocks_with_vent_4_or_more'] = final_df_block['encounter_block'].nunique()\n",
    "strobe_counts['G_blocks_with_vent_less_than_4'] = len(blocks_under_4)\n",
    "print(f\"Unique encounter blocks with valid IMV start/end: {strobe_counts['G_blocks_with_vent_4_or_more']}\")\n",
    "print(f\"Excluded {len(blocks_under_4)} encounter blocks with <4 vent hours in first 72 hours of intubation.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Exclude blocks with trach at the time of intubation\n",
    "# Check for trach at time of intubation (time_from_vent = 0)\n",
    "blocks_with_trach_at_intubation = final_df_block[\n",
    "    (final_df_block['time_from_vent'] == 0) & \n",
    "    (final_df_block['hourly_trach'] == 1)\n",
    "]['encounter_block'].unique()\n",
    "\n",
    "print(f\"Blocks with trach at intubation: {len(blocks_with_trach_at_intubation)}\")\n",
    "\n",
    "# Exclude these blocks\n",
    "final_df_block = final_df_block[\n",
    "    ~final_df_block['encounter_block'].isin(blocks_with_trach_at_intubation)\n",
    "]\n",
    "\n",
    "# Update STROBE counts\n",
    "strobe_counts['G_final_blocks_with_trach_at_intubation'] = len(blocks_with_trach_at_intubation)\n",
    "strobe_counts['G_final_blocks_without_trach_at_intubation'] = final_df_block['encounter_block'].nunique()\n",
    "\n",
    "print(f\"Excluded {len(blocks_with_trach_at_intubation)} blocks with trach at intubation\")\n",
    "print(f\"Final cohort size: {strobe_counts['G_final_blocks_without_trach_at_intubation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb99097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:15.464874Z",
     "iopub.status.busy": "2025-05-05T20:35:15.464729Z",
     "iopub.status.idle": "2025-05-05T20:35:15.467182Z",
     "shell.execute_reply": "2025-05-05T20:35:15.466907Z"
    },
    "papermill": {
     "duration": 0.012956,
     "end_time": "2025-05-05T20:35:15.468089",
     "exception": false,
     "start_time": "2025-05-05T20:35:15.455133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace0c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = all_ids[all_ids['encounter_block'].isin(final_df_block['encounter_block'])]\n",
    "all_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144311f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:15.486715Z",
     "iopub.status.busy": "2025-05-05T20:35:15.486484Z",
     "iopub.status.idle": "2025-05-05T20:35:15.752488Z",
     "shell.execute_reply": "2025-05-05T20:35:15.751966Z"
    },
    "papermill": {
     "duration": 0.276967,
     "end_time": "2025-05-05T20:35:15.753623",
     "exception": false,
     "start_time": "2025-05-05T20:35:15.476656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df = pd.merge(\n",
    "    final_df_block,\n",
    "    all_ids,\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ").reindex(columns=[\n",
    "    'encounter_block', 'recorded_date', 'recorded_hour',\n",
    "    'time_from_vent', 'time_from_vent_adjusted', \n",
    "    'min_fio2_set', 'max_fio2_set', 'min_peep_set', 'max_peep_set',\n",
    "    'min_lpm_set', 'max_lpm_set', 'min_resp_rate_obs', 'max_resp_rate_obs',\n",
    "    'hourly_trach', 'hourly_on_vent'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465aec97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:15.772256Z",
     "iopub.status.busy": "2025-05-05T20:35:15.772059Z",
     "iopub.status.idle": "2025-05-05T20:35:15.853793Z",
     "shell.execute_reply": "2025-05-05T20:35:15.853376Z"
    },
    "papermill": {
     "duration": 0.092116,
     "end_time": "2025-05-05T20:35:15.854655",
     "exception": false,
     "start_time": "2025-05-05T20:35:15.762539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "key_cols = ['encounter_block', 'recorded_date', 'recorded_hour']\n",
    "duplicates = final_df.duplicated(subset=key_cols).sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e0bd51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:15.872821Z",
     "iopub.status.busy": "2025-05-05T20:35:15.872650Z",
     "iopub.status.idle": "2025-05-05T20:35:15.883802Z",
     "shell.execute_reply": "2025-05-05T20:35:15.883285Z"
    },
    "papermill": {
     "duration": 0.021718,
     "end_time": "2025-05-05T20:35:15.885106",
     "exception": false,
     "start_time": "2025-05-05T20:35:15.863388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_ids = all_ids[all_ids['encounter_block'].isin(final_df['encounter_block'])]\n",
    "all_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6abfe6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:15.909570Z",
     "iopub.status.busy": "2025-05-05T20:35:15.908804Z",
     "iopub.status.idle": "2025-05-05T20:35:15.928795Z",
     "shell.execute_reply": "2025-05-05T20:35:15.928230Z"
    },
    "papermill": {
     "duration": 0.035414,
     "end_time": "2025-05-05T20:35:15.929914",
     "exception": false,
     "start_time": "2025-05-05T20:35:15.894500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in all_ids.columns[:3]:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(all_ids[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a3f50",
   "metadata": {
    "papermill": {
     "duration": 0.013692,
     "end_time": "2025-05-05T20:35:15.972379",
     "exception": false,
     "start_time": "2025-05-05T20:35:15.958687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### (F) Add final outcome dttm\n",
    "\n",
    "Calculate final outcome dttm for each encounter block using last vital recorded dttm and discharge disposition.   \n",
    "\n",
    "To get the `final_outcome_dttm`, we use the `block_last_vital_dttm`. Added a `is_dead` flag when `discharge_category` == `Expired` or `Hospice`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abaf2d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:15.992216Z",
     "iopub.status.busy": "2025-05-05T20:35:15.992018Z",
     "iopub.status.idle": "2025-05-05T20:35:16.062277Z",
     "shell.execute_reply": "2025-05-05T20:35:16.061886Z"
    },
    "papermill": {
     "duration": 0.081894,
     "end_time": "2025-05-05T20:35:16.063786",
     "exception": false,
     "start_time": "2025-05-05T20:35:15.981892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) Merge `all_ids` (patient_id, hospitalization_id, encounter_block)\n",
    "#    with final blocks DataFrame (which has block-level columns -  \tblock_vent_start_dttm,\tblock_vent_end_dttm block_first_vital_dttm, block_last_vital_dttm, ).\n",
    "all_ids_w_outcome = pd.merge(\n",
    "    all_ids,\n",
    "    final_blocks,           \n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 2) Merge with patient table to get death_dttm\n",
    "all_ids_w_outcome = pd.merge(\n",
    "    all_ids_w_outcome,\n",
    "    patient[['patient_id','death_dttm']],\n",
    "    on='patient_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Use block_last_vital_dttm as the final_outcome_dttm\n",
    "all_ids_w_outcome['final_outcome_dttm'] = all_ids_w_outcome['block_last_vital_dttm']\n",
    "\n",
    "# Add is_dead flag based on discharge_category\n",
    "all_ids_w_outcome['is_dead'] = (all_ids_w_outcome['discharge_category'].str.lower().isin(['expired', 'hospice'])).astype(int)\n",
    "\n",
    "# Handle case where death_dttm is less than discharge_dttm, final outcome should be death_dttm\n",
    "# and is_dead should be 1\n",
    "mask_death_before_discharge = all_ids_w_outcome['death_dttm'] < all_ids_w_outcome['discharge_dttm']\n",
    "all_ids_w_outcome.loc[mask_death_before_discharge, 'final_outcome_dttm'] = all_ids_w_outcome['death_dttm']\n",
    "all_ids_w_outcome.loc[mask_death_before_discharge, 'is_dead'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ed559",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:16.090738Z",
     "iopub.status.busy": "2025-05-05T20:35:16.090552Z",
     "iopub.status.idle": "2025-05-05T20:35:16.099827Z",
     "shell.execute_reply": "2025-05-05T20:35:16.099446Z"
    },
    "papermill": {
     "duration": 0.021478,
     "end_time": "2025-05-05T20:35:16.100697",
     "exception": false,
     "start_time": "2025-05-05T20:35:16.079219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SANITY CHECK- check blocks where death_dttm is before block_last_vital_dttm\n",
    "## For this project, we used bloack_last_vital_dttm as the final_outcome_dttm to circumvent possible issues \n",
    "mask_death_before_vitals = (all_ids_w_outcome['death_dttm'].notna()) & (all_ids_w_outcome['death_dttm'] < all_ids_w_outcome['block_last_vital_dttm'])\n",
    "print(\"Number of blocks where death_dttm is before block_last_vital_dttm:\", mask_death_before_vitals.sum())\n",
    "print(\"\\nExample cases:\")\n",
    "death_before_vitals_df = all_ids_w_outcome[mask_death_before_vitals][['patient_id', 'hospitalization_id', 'encounter_block', 'death_dttm', 'block_last_vital_dttm', 'final_outcome_dttm']]\n",
    "\n",
    "# Calculate the difference in hours between death_dttm and block_last_vital_dttm\n",
    "death_before_vitals_df['diff_hour'] = (death_before_vitals_df['death_dttm'] - death_before_vitals_df['block_last_vital_dttm']).dt.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc097c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:16.157708Z",
     "iopub.status.busy": "2025-05-05T20:35:16.157400Z",
     "iopub.status.idle": "2025-05-05T20:35:16.163051Z",
     "shell.execute_reply": "2025-05-05T20:35:16.162704Z"
    },
    "papermill": {
     "duration": 0.028221,
     "end_time": "2025-05-05T20:35:16.164011",
     "exception": false,
     "start_time": "2025-05-05T20:35:16.135790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in all_ids_w_outcome.columns[:3]:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(all_ids_w_outcome[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c21a29f",
   "metadata": {
    "papermill": {
     "duration": 0.009935,
     "end_time": "2025-05-05T20:35:16.188142",
     "exception": false,
     "start_time": "2025-05-05T20:35:16.178207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hourly Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014911a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:16.283417Z",
     "iopub.status.busy": "2025-05-05T20:35:16.283110Z",
     "iopub.status.idle": "2025-05-05T20:35:16.734738Z",
     "shell.execute_reply": "2025-05-05T20:35:16.734181Z"
    },
    "papermill": {
     "duration": 0.471038,
     "end_time": "2025-05-05T20:35:16.735636",
     "exception": false,
     "start_time": "2025-05-05T20:35:16.264598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## get height , weight to calculate bmi\n",
    "# Filter vitals to include only height and weight\n",
    "vitals_bmi = vitals_stitched[\n",
    "    (vitals_stitched['vital_category'].isin(['weight_kg', 'height_cm'])) &\n",
    "    (vitals_stitched['encounter_block'].isin(all_ids_w_outcome['encounter_block']))\n",
    "].copy()\n",
    "\n",
    "# Remove outliers\n",
    "# Extract the min/max from the config\n",
    "min_height, max_height = outlier_cfg['height_cm']\n",
    "min_weight, max_weight = outlier_cfg['weight_kg']\n",
    "\n",
    "# For height rows: set out-of-range to NaN\n",
    "is_height = vitals_bmi['vital_category'] == 'height_cm'\n",
    "height_mask_low  = is_height & (vitals_bmi['vital_value'] < min_height)\n",
    "height_mask_high = is_height & (vitals_bmi['vital_value'] > max_height)\n",
    "vitals_bmi.loc[height_mask_low | height_mask_high, 'vital_value'] = np.nan\n",
    "\n",
    "# For weight rows: set out-of-range to NaN\n",
    "is_weight = vitals_bmi['vital_category'] == 'weight_kg'\n",
    "weight_mask_low  = is_weight & (vitals_bmi['vital_value'] < min_weight)\n",
    "weight_mask_high = is_weight & (vitals_bmi['vital_value'] > max_weight)\n",
    "vitals_bmi.loc[weight_mask_low | weight_mask_high, 'vital_value'] = np.nan\n",
    "\n",
    "# Merge with vent_start_end to get ventilation start time\n",
    "vitals_bmi = vitals_bmi.merge(\n",
    "    block_vent_times[['encounter_block','block_vent_start_dttm']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate time difference between recorded_dttm and vent_start_time\n",
    "vitals_bmi['time_diff'] = (vitals_bmi['recorded_dttm'] - vitals_bmi['block_vent_start_dttm']).dt.total_seconds() / 3600  # in hours\n",
    "\n",
    "# Define whether measurement is before or after vent_start_time\n",
    "vitals_bmi['before_vent_start'] = (vitals_bmi['time_diff'] <= 0).astype(int)\n",
    "\n",
    "# Calculate absolute time difference\n",
    "vitals_bmi['abs_time_diff'] = vitals_bmi['time_diff'].abs()\n",
    "\n",
    "# Sort data to prioritize measurements before vent start and closest in time\n",
    "vitals_bmi = vitals_bmi.sort_values(['encounter_block', 'vital_category', 'before_vent_start', 'abs_time_diff'], \n",
    "                                    ascending=[True, True, False, True])\n",
    "\n",
    "# Drop duplicates to keep the closest measurement for each vital_category per encounter block\n",
    "vitals_bmi = vitals_bmi.drop_duplicates(subset=['encounter_block', 'vital_category'], keep='first')\n",
    "\n",
    "# Pivot to get height and weight per encounter block\n",
    "vitals_bmi_pivot = vitals_bmi.pivot(index='encounter_block', \n",
    "                                    columns='vital_category', \n",
    "                                    values='vital_value'\n",
    "                                    ).reset_index()\n",
    "\n",
    "# Calculate BMI\n",
    "vitals_bmi_pivot['bmi'] = vitals_bmi_pivot['weight_kg'] / ((vitals_bmi_pivot['height_cm'] / 100) ** 2)\n",
    "\n",
    "print(f\"Number of unique encounter blocks with BMI data: {vitals_bmi_pivot['encounter_block'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc63ce3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:16.755624Z",
     "iopub.status.busy": "2025-05-05T20:35:16.755492Z",
     "iopub.status.idle": "2025-05-05T20:35:19.368044Z",
     "shell.execute_reply": "2025-05-05T20:35:19.367561Z"
    },
    "papermill": {
     "duration": 2.623673,
     "end_time": "2025-05-05T20:35:19.368909",
     "exception": false,
     "start_time": "2025-05-05T20:35:16.745236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract 'recorded_date' and 'recorded_hour' from recorded_dttm\n",
    "vitals_stitched['recorded_date'] = vitals_stitched['recorded_dttm'].dt.date\n",
    "vitals_stitched['recorded_hour'] = vitals_stitched['recorded_dttm'].dt.hour\n",
    "print(f\"Number of unique encounter blocks BEFORE stitching vitals: {vitals_stitched['encounter_block'].nunique()}\")\n",
    "vitals_stitched = vitals_stitched[vitals_stitched['encounter_block'].isin(all_ids_w_outcome['encounter_block'])]\n",
    "print(f\"Number of unique encounter blocks AFTER stitching vitals: {vitals_stitched['encounter_block'].nunique()}\")\n",
    "strobe_counts['final_blocks_with_vitals'] = vitals_stitched['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1269b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:19.388904Z",
     "iopub.status.busy": "2025-05-05T20:35:19.388773Z",
     "iopub.status.idle": "2025-05-05T20:35:19.665275Z",
     "shell.execute_reply": "2025-05-05T20:35:19.664954Z"
    },
    "papermill": {
     "duration": 0.287848,
     "end_time": "2025-05-05T20:35:19.666160",
     "exception": false,
     "start_time": "2025-05-05T20:35:19.378312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate MAP if it doesn't exist\n",
    "# Calculate MAP, even if it exists in the data\n",
    "vitals_stitched = vitals_stitched[vitals_stitched['vital_category'] != 'map']\n",
    "if 'map' not in vitals_stitched['vital_category'].unique():\n",
    "    # 1) Filter for sbp & dbp\n",
    "    sbp_dbp = vitals_stitched[vitals_stitched['vital_category'].isin(['sbp','dbp'])].copy()\n",
    "    \n",
    "    # 2) Pivot at the encounter_block + recorded_dttm level\n",
    "    sbp_dbp_pivot = sbp_dbp.pivot_table(\n",
    "        index=['encounter_block','recorded_dttm'],\n",
    "        columns='vital_category',\n",
    "        values='vital_value'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # 3) Drop any row missing sbp or dbp\n",
    "    sbp_dbp_pivot = sbp_dbp_pivot.dropna(subset=['sbp','dbp'])\n",
    "    \n",
    "    # 4) Calculate MAP\n",
    "    sbp_dbp_pivot['map'] = (sbp_dbp_pivot['sbp'] + 2*sbp_dbp_pivot['dbp']) / 3\n",
    "    \n",
    "    # 5) Build a DataFrame for map\n",
    "    map_vitals = sbp_dbp_pivot[['encounter_block','recorded_dttm','map']].copy()\n",
    "    map_vitals['vital_category'] = 'map'\n",
    "    map_vitals['vital_value'] = map_vitals['map']\n",
    "    \n",
    "    # Also add recorded_date/hour\n",
    "    map_vitals['recorded_date'] = map_vitals['recorded_dttm'].dt.date\n",
    "    map_vitals['recorded_hour'] = map_vitals['recorded_dttm'].dt.hour\n",
    "    \n",
    "    # Keep only the needed columns\n",
    "    map_vitals = map_vitals[[\n",
    "        'encounter_block','recorded_dttm','recorded_date','recorded_hour','vital_category','vital_value'\n",
    "    ]]\n",
    "    \n",
    "    # 6) Append 'map' to the main vitals_stitched DataFrame\n",
    "    vitals_stitched = pd.concat([vitals_stitched, map_vitals], ignore_index=True)\n",
    "    print(\"...map was calculated and appended to vitals_stitched.\")\n",
    "else:\n",
    "    print(\"Map exists in your CLIF database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854b4ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:19.686079Z",
     "iopub.status.busy": "2025-05-05T20:35:19.685854Z",
     "iopub.status.idle": "2025-05-05T20:35:23.490195Z",
     "shell.execute_reply": "2025-05-05T20:35:23.489875Z"
    },
    "papermill": {
     "duration": 3.814912,
     "end_time": "2025-05-05T20:35:23.491002",
     "exception": false,
     "start_time": "2025-05-05T20:35:19.676090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Compute min/max vitals  at the BLOCK level\n",
    "# group by encounter_block + recorded_date + recorded_hour + vital_category\n",
    "vitals_min_max = vitals_stitched.groupby(\n",
    "    ['encounter_block', 'recorded_date', 'recorded_hour', 'vital_category']\n",
    ").agg(\n",
    "    min_val=('vital_value', 'min'),\n",
    "    max_val=('vital_value', 'max'),\n",
    "    avg_val=('vital_value', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# 2) Pivot to get one row per (encounter_block, date, hour), columns for min/max/avg of each vital\n",
    "vitals_pivot = vitals_min_max.pivot_table(\n",
    "    index=['encounter_block', 'recorded_date', 'recorded_hour'],\n",
    "    columns='vital_category',\n",
    "    values=['min_val', 'max_val', 'avg_val']\n",
    ").reset_index()\n",
    "\n",
    "# 3) Flatten multi-level columns like ('min_val', 'sbp') -> 'min_sbp'\n",
    "vitals_pivot.columns = [\n",
    "    '_'.join(col).rstrip('_') if isinstance(col, tuple) else col\n",
    "    for col in vitals_pivot.columns\n",
    "]\n",
    "\n",
    "# 4) Clean up prefixes to be 'min_', 'max_', and 'avg_'\n",
    "rename_dict = {}\n",
    "for c in vitals_pivot.columns:\n",
    "    if c.startswith('min_val_'):\n",
    "        rename_dict[c] = c.replace('min_val_', 'min_')\n",
    "    elif c.startswith('max_val_'):\n",
    "        rename_dict[c] = c.replace('max_val_', 'max_')\n",
    "    elif c.startswith('avg_val_'):\n",
    "        rename_dict[c] = c.replace('avg_val_', 'avg_')\n",
    "\n",
    "vitals_pivot = vitals_pivot.rename(columns=rename_dict)\n",
    "\n",
    "print(\"Finished creating block-level min/max/avg vitals pivot:\")\n",
    "vitals_pivot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a173dbc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:23.512015Z",
     "iopub.status.busy": "2025-05-05T20:35:23.511882Z",
     "iopub.status.idle": "2025-05-05T20:35:23.602506Z",
     "shell.execute_reply": "2025-05-05T20:35:23.602045Z"
    },
    "papermill": {
     "duration": 0.102487,
     "end_time": "2025-05-05T20:35:23.603567",
     "exception": false,
     "start_time": "2025-05-05T20:35:23.501080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## confirm duplicates don't exist\n",
    "checkpoint_vitals = pyCLIF.remove_duplicates(vitals_pivot, [\n",
    "    'encounter_block','recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba846ef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:23.624185Z",
     "iopub.status.busy": "2025-05-05T20:35:23.624002Z",
     "iopub.status.idle": "2025-05-05T20:35:24.070149Z",
     "shell.execute_reply": "2025-05-05T20:35:24.069757Z"
    },
    "papermill": {
     "duration": 0.457758,
     "end_time": "2025-05-05T20:35:24.071226",
     "exception": false,
     "start_time": "2025-05-05T20:35:23.613468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge vitals with final_df\n",
    "final_df = pd.merge(final_df, vitals_pivot, on=['encounter_block', 'recorded_date', 'recorded_hour'], \n",
    "                   how='left')\n",
    "print(\"\\n Columns in final_df after merging with vitals:\")\n",
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c7343",
   "metadata": {
    "papermill": {
     "duration": 0.01004,
     "end_time": "2025-05-05T20:35:24.091038",
     "exception": false,
     "start_time": "2025-05-05T20:35:24.080998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hourly Meds\n",
    "\n",
    "* Handle med dose unit conversion for all vasoactives\n",
    "* Calculate NE equivalent levels using \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \"vasopressin\", \"dopamine\",  \"angiotensin\"\n",
    "* Create flags for \"nicardipine\", \"nitroprusside\", \"clevidipine\" for the red criteria under consensus criteria\n",
    "* Identify encounters on paralytics - cisatracurium, vecuronium, rocuronium- and create flags for each of these paralytic meds. These patients will not be considered eligible for mobilization during the hour they were receiving paralytic medication. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ab9191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:24.111152Z",
     "iopub.status.busy": "2025-05-05T20:35:24.110895Z",
     "iopub.status.idle": "2025-05-05T20:35:24.453642Z",
     "shell.execute_reply": "2025-05-05T20:35:24.453292Z"
    },
    "papermill": {
     "duration": 0.35391,
     "end_time": "2025-05-05T20:35:24.454519",
     "exception": false,
     "start_time": "2025-05-05T20:35:24.100609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import clif continuous meds for the cohort on vent during the required time period\n",
    "meds_filters = {\n",
    "    'hospitalization_id': all_ids['hospitalization_id'].unique().tolist(),\n",
    "    'med_category': meds_of_interest\n",
    "}\n",
    "meds = pyCLIF.load_data('clif_medication_admin_continuous', columns=meds_required_columns, filters=meds_filters)\n",
    "meds = meds.merge(all_ids, on='hospitalization_id', how='left')\n",
    "print(\"Unique encounters in meds\", pyCLIF.count_unique_encounters(meds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49dfe9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:24.475368Z",
     "iopub.status.busy": "2025-05-05T20:35:24.475230Z",
     "iopub.status.idle": "2025-05-05T20:35:24.570614Z",
     "shell.execute_reply": "2025-05-05T20:35:24.570298Z"
    },
    "papermill": {
     "duration": 0.107071,
     "end_time": "2025-05-05T20:35:24.571643",
     "exception": false,
     "start_time": "2025-05-05T20:35:24.464572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ensure correct format\n",
    "meds['hospitalization_id']= meds['hospitalization_id'].astype(str)\n",
    "meds['med_dose_unit'] = meds['med_dose_unit'].str.lower()\n",
    "meds = pyCLIF.convert_datetime_columns_to_site_tz(meds,  pyCLIF.helper['timezone'])\n",
    "meds['med_dose'] = pd.to_numeric(meds['med_dose'], errors='coerce')\n",
    "# Create 'date' and 'hour_of_day' columns\n",
    "meds['recorded_date'] = meds['admin_dttm'].dt.date\n",
    "meds['recorded_hour'] = meds['admin_dttm'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3019437b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:24.593103Z",
     "iopub.status.busy": "2025-05-05T20:35:24.592962Z",
     "iopub.status.idle": "2025-05-05T20:35:24.637048Z",
     "shell.execute_reply": "2025-05-05T20:35:24.636747Z"
    },
    "papermill": {
     "duration": 0.055939,
     "end_time": "2025-05-05T20:35:24.637954",
     "exception": false,
     "start_time": "2025-05-05T20:35:24.582015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a summary table for each med_category\n",
    "summary_meds= meds.groupby('med_category').agg(\n",
    "    total_N=('med_category', 'size'),\n",
    "    min=('med_dose', 'min'),\n",
    "    max=('med_dose', 'max'),\n",
    "    first_quantile=('med_dose', lambda x: x.quantile(0.25)),\n",
    "    second_quantile=('med_dose', lambda x: x.quantile(0.5)),\n",
    "    third_quantile=('med_dose', lambda x: x.quantile(0.75)),\n",
    "    missing_values=('med_dose', lambda x: x.isna().sum())\n",
    ").reset_index()\n",
    "\n",
    "summary_meds.to_csv('../output/final/summary_meds_by_category.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1891a818",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:24.658613Z",
     "iopub.status.busy": "2025-05-05T20:35:24.658449Z",
     "iopub.status.idle": "2025-05-05T20:35:24.712251Z",
     "shell.execute_reply": "2025-05-05T20:35:24.711905Z"
    },
    "papermill": {
     "duration": 0.065267,
     "end_time": "2025-05-05T20:35:24.713261",
     "exception": false,
     "start_time": "2025-05-05T20:35:24.647994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a summary table for each med_category and med_dose_unit combination\n",
    "summary_meds_cat_dose= meds.groupby(['med_category', 'med_dose_unit']).agg(\n",
    "    total_N=('med_category', 'size'),\n",
    "    min=('med_dose', 'min'),\n",
    "    max=('med_dose', 'max'),\n",
    "    first_quantile=('med_dose', lambda x: x.quantile(0.25)),\n",
    "    second_quantile=('med_dose', lambda x: x.quantile(0.5)),\n",
    "    third_quantile=('med_dose', lambda x: x.quantile(0.75)),\n",
    "    missing_values=('med_dose', lambda x: x.isna().sum())\n",
    ").reset_index()\n",
    "summary_meds_cat_dose.to_csv('../output/final/summary_meds_by_category_dose_units.csv', index=False)\n",
    "## check the distrbituon of required continuous meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d43e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Check which groups have all NaN values\n",
    "print(\"Groups with all NaN med_dose values:\")\n",
    "for (med_category, med_dose_unit), group in meds.groupby(['med_category', 'med_dose_unit']):\n",
    "    if group['med_dose'].isna().all():\n",
    "        print(f\"  {med_category} - {med_dose_unit}: {len(group)} rows, all NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a07464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:24.734527Z",
     "iopub.status.busy": "2025-05-05T20:35:24.734275Z",
     "iopub.status.idle": "2025-05-05T20:35:26.663047Z",
     "shell.execute_reply": "2025-05-05T20:35:26.662705Z"
    },
    "papermill": {
     "duration": 1.940646,
     "end_time": "2025-05-05T20:35:26.664333",
     "exception": false,
     "start_time": "2025-05-05T20:35:24.723687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by med_category and med_dose_unit\n",
    "grouped_data = meds.groupby(['med_category', 'med_dose_unit'])\n",
    "\n",
    "# Dynamically determine the number of required subplots\n",
    "n_plots = len(grouped_data.groups.keys())\n",
    "n_cols = 4\n",
    "n_rows = (n_plots + n_cols - 1) // n_cols  # Round up to determine rows\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5))\n",
    "\n",
    "# Flatten the axs array for easier indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Loop through each group and plot the histogram\n",
    "for i, ((med_category, med_dose_unit), group) in enumerate(grouped_data):\n",
    "    ax = axs[i]\n",
    "    \n",
    "    # Filter out NaN values\n",
    "    valid_doses = group['med_dose'].dropna()\n",
    "    \n",
    "    if len(valid_doses) > 0:\n",
    "        ax.hist(valid_doses, bins=20, alpha=0.7, label=f\"N = {len(valid_doses)}\")\n",
    "        ax.set_title(f\"{med_category} - {med_dose_unit}\")\n",
    "        ax.set_xlabel('Med Dose')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    else:\n",
    "        # Handle case where all values are NaN\n",
    "        ax.text(0.5, 0.5, 'No valid data', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(f\"{med_category} - {med_dose_unit} (No Data)\")\n",
    "\n",
    "# Hide any unused axes\n",
    "for j in range(i + 1, len(axs)):\n",
    "    axs[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/final/graphs/meds_histograms.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca871a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:26.687010Z",
     "iopub.status.busy": "2025-05-05T20:35:26.686859Z",
     "iopub.status.idle": "2025-05-05T20:35:26.715400Z",
     "shell.execute_reply": "2025-05-05T20:35:26.715104Z"
    },
    "papermill": {
     "duration": 0.040714,
     "end_time": "2025-05-05T20:35:26.716170",
     "exception": false,
     "start_time": "2025-05-05T20:35:26.675456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SANITY CHECKS- Check the med_dose_unit for each med_category in the meds table\n",
    "med_dose_unit_check = meds.groupby(['med_category', 'med_dose_unit']).size().reset_index(name='count')\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "med_dose_unit_check['unit_validity'] = med_dose_unit_check.apply(pyCLIF.check_dose_unit, axis=1)\n",
    "\n",
    "# # Optional: Filter for invalid units\n",
    "invalid_units = med_dose_unit_check[med_dose_unit_check['unit_validity'] == 'Not an acceptable unit']\n",
    "print(\"Invalid units. These will be dropped:\\n\")\n",
    "print(invalid_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe63b74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:26.739450Z",
     "iopub.status.busy": "2025-05-05T20:35:26.739314Z",
     "iopub.status.idle": "2025-05-05T20:35:26.956407Z",
     "shell.execute_reply": "2025-05-05T20:35:26.956020Z"
    },
    "papermill": {
     "duration": 0.229692,
     "end_time": "2025-05-05T20:35:26.957445",
     "exception": false,
     "start_time": "2025-05-05T20:35:26.727753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Norepinephrine equivalent calculation\n",
    "# Goradia S, Sardaneh AA, Narayan SW, Penm J, Patanwala AE. Vasopressor dose equivalence: \n",
    "# A scoping review and suggested formula. J Crit Care. 2021 Feb;61:233-240. doi: 10.1016/j.jcrc.2020.11.002. Epub 2020 Nov 14. PMID: 33220576.\n",
    "\n",
    "# Filter meds to include only rows with '/hr' or '/min' in 'med_dose_unit'\n",
    "meds_filtered = meds[~meds['med_dose'].isnull()].copy()\n",
    "meds_filtered = meds_filtered[meds_filtered['med_dose_unit'].apply(pyCLIF.has_per_hour_or_min)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abbe629",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:26.980064Z",
     "iopub.status.busy": "2025-05-05T20:35:26.979917Z",
     "iopub.status.idle": "2025-05-05T20:35:31.736745Z",
     "shell.execute_reply": "2025-05-05T20:35:31.736359Z"
    },
    "papermill": {
     "duration": 4.769477,
     "end_time": "2025-05-05T20:35:31.737792",
     "exception": false,
     "start_time": "2025-05-05T20:35:26.968315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "meds_list = [\n",
    "    \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \n",
    "    \"vasopressin\", \"dopamine\",  \n",
    "    \"angiotensin\"\n",
    "]\n",
    "\n",
    "# **2. Convert Medication Doses to Required Units**\n",
    "ne_df = meds_filtered[meds_filtered['med_category'].isin(meds_list)].copy()\n",
    "# Merge weight_kg into meds_filtered (assuming 'vitals_bmi_pivot' is available)\n",
    "ne_df = ne_df.merge(vitals_bmi_pivot[['encounter_block', 'weight_kg']], on='encounter_block', how='left')\n",
    "ne_df[\"med_dose_converted\"] = ne_df.apply(pyCLIF.convert_dose, axis=1)\n",
    "\n",
    "# Filter doses within acceptable ranges\n",
    "ne_df = ne_df[ne_df.apply(pyCLIF.is_dose_within_range, axis=1, args=(outlier_cfg,))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f3308",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:31.762365Z",
     "iopub.status.busy": "2025-05-05T20:35:31.762208Z",
     "iopub.status.idle": "2025-05-05T20:35:31.818062Z",
     "shell.execute_reply": "2025-05-05T20:35:31.817489Z"
    },
    "papermill": {
     "duration": 0.069368,
     "end_time": "2025-05-05T20:35:31.819009",
     "exception": false,
     "start_time": "2025-05-05T20:35:31.749641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# **4. Flag Medications Not in the Dataset**\n",
    "for med in meds_list:\n",
    "    if med not in ne_df['med_category'].unique():\n",
    "        print(f\"{med} is not in the dataset.\")\n",
    "    else:\n",
    "        print(f\"{med} is in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42487ed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:31.842888Z",
     "iopub.status.busy": "2025-05-05T20:35:31.842513Z",
     "iopub.status.idle": "2025-05-05T20:35:32.245152Z",
     "shell.execute_reply": "2025-05-05T20:35:32.244804Z"
    },
    "papermill": {
     "duration": 0.415532,
     "end_time": "2025-05-05T20:35:32.246192",
     "exception": false,
     "start_time": "2025-05-05T20:35:31.830660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pivot and Aggregate the Data**\n",
    "# Group and aggregate doses\n",
    "group_cols = ['encounter_block', 'recorded_date', 'recorded_hour', 'med_category']\n",
    "dose_agg = ne_df.groupby(group_cols)['med_dose_converted'].agg(['min', 'max', 'first', 'last']).reset_index()\n",
    "\n",
    "# Pivot to have medications as columns\n",
    "dose_pivot_min   = dose_agg.pivot_table(index=['encounter_block', 'recorded_date', 'recorded_hour'], columns='med_category', values='min').reset_index()\n",
    "dose_pivot_max   = dose_agg.pivot_table(index=['encounter_block', 'recorded_date', 'recorded_hour'], columns='med_category', values='max').reset_index()\n",
    "dose_pivot_first = dose_agg.pivot_table(index=['encounter_block', 'recorded_date', 'recorded_hour'], columns='med_category', values='first').reset_index()\n",
    "dose_pivot_last  = dose_agg.pivot_table(index=['encounter_block', 'recorded_date', 'recorded_hour'], columns='med_category', values='last').reset_index()\n",
    "\n",
    "# Rename columns to indicate min and max\n",
    "dose_pivot_min.columns   = ['encounter_block', 'recorded_date', 'recorded_hour'] + ['min_'   + col for col in dose_pivot_min.columns if col not in ['encounter_block', 'recorded_date', 'recorded_hour']]\n",
    "dose_pivot_max.columns   = ['encounter_block', 'recorded_date', 'recorded_hour'] + ['max_'   + col for col in dose_pivot_max.columns if col not in ['encounter_block', 'recorded_date', 'recorded_hour']]\n",
    "dose_pivot_first.columns = ['encounter_block', 'recorded_date', 'recorded_hour'] + ['first_' + col for col in dose_pivot_first.columns if col not in ['encounter_block', 'recorded_date', 'recorded_hour']]\n",
    "dose_pivot_last.columns  = ['encounter_block', 'recorded_date', 'recorded_hour'] + ['last_'  + col for col in dose_pivot_last.columns if col not in ['encounter_block', 'recorded_date', 'recorded_hour']]\n",
    "\n",
    "# Merge min and max DataFrames\n",
    "dose_pivot = pyCLIF.merge_multiple_dfs(dose_pivot_min, dose_pivot_max, dose_pivot_first, dose_pivot_last,\n",
    "                              on=['encounter_block', 'recorded_date', 'recorded_hour'],\n",
    "                              how='outer')\n",
    "\n",
    "# **6. Calculate Norepinephrine Equivalents**\n",
    "\n",
    "# Replace NaN with 0 for calculations\n",
    "dose_pivot.fillna(0, inplace=True)\n",
    "\n",
    "# Calculate NE min\n",
    "dose_pivot['ne_calc_min'] = (\n",
    "    dose_pivot.get('min_norepinephrine', 0) +\n",
    "    dose_pivot.get('min_epinephrine', 0) +\n",
    "    dose_pivot.get('min_phenylephrine', 0) / 10 +\n",
    "    dose_pivot.get('min_dopamine', 0) / 100 +\n",
    "    dose_pivot.get('min_metaraminol', 0) / 8 +\n",
    "    dose_pivot.get('min_vasopressin', 0) * 2.5 +\n",
    "    dose_pivot.get('min_angiotensin', 0) * 10\n",
    ")\n",
    "\n",
    "# Calculate NE max\n",
    "dose_pivot['ne_calc_max'] = (\n",
    "    dose_pivot.get('max_norepinephrine', 0) +\n",
    "    dose_pivot.get('max_epinephrine', 0) +\n",
    "    dose_pivot.get('max_phenylephrine', 0) / 10 +\n",
    "    dose_pivot.get('max_dopamine', 0) / 100 +\n",
    "    dose_pivot.get('max_metaraminol', 0) / 8 +\n",
    "    dose_pivot.get('max_vasopressin', 0) * 2.5 +\n",
    "    dose_pivot.get('max_angiotensin', 0) * 10\n",
    ")\n",
    "\n",
    "# Calculate NE first\n",
    "dose_pivot['ne_calc_first'] = (\n",
    "    dose_pivot.get('first_norepinephrine', 0) +\n",
    "    dose_pivot.get('first_epinephrine', 0) +\n",
    "    dose_pivot.get('first_phenylephrine', 0) / 10 +\n",
    "    dose_pivot.get('first_dopamine', 0) / 100 +\n",
    "    dose_pivot.get('first_metaraminol', 0) / 8 +\n",
    "    dose_pivot.get('first_vasopressin', 0) * 2.5 +\n",
    "    dose_pivot.get('first_angiotensin', 0) * 10\n",
    ")\n",
    "\n",
    "# Calculate NE last\n",
    "dose_pivot['ne_calc_last'] = (\n",
    "    dose_pivot.get('last_norepinephrine', 0) +\n",
    "    dose_pivot.get('last_epinephrine', 0) +\n",
    "    dose_pivot.get('last_phenylephrine', 0) / 10 +\n",
    "    dose_pivot.get('last_dopamine', 0) / 100 +\n",
    "    dose_pivot.get('last_metaraminol', 0) / 8 +\n",
    "    dose_pivot.get('last_vasopressin', 0) * 2.5 +\n",
    "    dose_pivot.get('last_angiotensin', 0) * 10\n",
    ")\n",
    "\n",
    "# **7. Prepare the Final Dataset**\n",
    "# Keep only the required columns\n",
    "ne_calc_df = dose_pivot[['encounter_block', 'recorded_date', \n",
    "                         'recorded_hour', \n",
    "                         'ne_calc_min', 'ne_calc_max', \n",
    "                         'ne_calc_first', 'ne_calc_last']].drop_duplicates(subset=['encounter_block', 'recorded_date', 'recorded_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f47792",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:32.270561Z",
     "iopub.status.busy": "2025-05-05T20:35:32.270309Z",
     "iopub.status.idle": "2025-05-05T20:35:32.273561Z",
     "shell.execute_reply": "2025-05-05T20:35:32.273180Z"
    },
    "papermill": {
     "duration": 0.016871,
     "end_time": "2025-05-05T20:35:32.274430",
     "exception": false,
     "start_time": "2025-05-05T20:35:32.257559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "strobe_counts['final_blocks_with_norepi_eq'] = ne_calc_df['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec9b8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:32.297936Z",
     "iopub.status.busy": "2025-05-05T20:35:32.297652Z",
     "iopub.status.idle": "2025-05-05T20:35:33.394684Z",
     "shell.execute_reply": "2025-05-05T20:35:33.394298Z"
    },
    "papermill": {
     "duration": 1.110139,
     "end_time": "2025-05-05T20:35:33.395725",
     "exception": false,
     "start_time": "2025-05-05T20:35:32.285586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encounter_blocks_list = ne_df['encounter_block'].unique().tolist()\n",
    "hourly_ne = pyCLIF.build_meds_hourly_scaffold(\n",
    "    ne_df,\n",
    "    id_col=\"encounter_block\",      # column to group by\n",
    "    ids=encounter_blocks_list,     # Iterable of id_col to keep\n",
    "    timestamp_col=\"admin_dttm\",    # change if your column is named differently\n",
    "    site_tz=pyCLIF.helper['timezone']          # change to the zone you need\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aea057",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:33.419771Z",
     "iopub.status.busy": "2025-05-05T20:35:33.419609Z",
     "iopub.status.idle": "2025-05-05T20:35:33.497586Z",
     "shell.execute_reply": "2025-05-05T20:35:33.497249Z"
    },
    "papermill": {
     "duration": 0.091583,
     "end_time": "2025-05-05T20:35:33.498673",
     "exception": false,
     "start_time": "2025-05-05T20:35:33.407090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure the DataFrame is sorted by ID and 'time_from_vent'\n",
    "ne_calc_df = ne_calc_df.sort_values(by=['encounter_block', 'recorded_date', 'recorded_hour'])\n",
    "# Merge the norepinephrine equivalent DataFrame with the hourly norepinephrine DataFrame\n",
    "hourly_ne_merged = pd.merge(\n",
    "    hourly_ne,\n",
    "    ne_calc_df,\n",
    "    on=['encounter_block', 'recorded_date', 'recorded_hour'],\n",
    "    how='left'\n",
    ")\n",
    "# Fill forward the specified columns\n",
    "cols_to_fill = ['ne_calc_min', 'ne_calc_max', 'ne_calc_first', 'ne_calc_last']\n",
    "hourly_ne_merged[cols_to_fill] = hourly_ne_merged[cols_to_fill].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf17577c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:33.522685Z",
     "iopub.status.busy": "2025-05-05T20:35:33.522392Z",
     "iopub.status.idle": "2025-05-05T20:35:34.272301Z",
     "shell.execute_reply": "2025-05-05T20:35:34.271931Z"
    },
    "papermill": {
     "duration": 0.763668,
     "end_time": "2025-05-05T20:35:34.273318",
     "exception": false,
     "start_time": "2025-05-05T20:35:33.509650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_last_ne_6h(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For one encounter_block add/overwrite the column\n",
    "    `last_ne_dose_last_6_hours` with the `ne_calc_last` value that\n",
    "    occurred **exactly six hours earlier**.  If that row does not\n",
    "    exist (e.g. the first <6 hours of the stay) the value is 0.\n",
    "    \"\"\"\n",
    "    group['last_ne_dose_last_6_hours'] = (\n",
    "        group['ne_calc_last']\n",
    "        .shift(6)           # value 6 rows (hours) ago\n",
    "        .fillna(0)          # treat â€œno recordâ€ as 0\n",
    "    )\n",
    "    return group\n",
    "\n",
    "hourly_ne_merged = (\n",
    "    hourly_ne_merged\n",
    "      .groupby('encounter_block', group_keys=False)\n",
    "      .apply(add_last_ne_6h)\n",
    "      .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d53d95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:34.297152Z",
     "iopub.status.busy": "2025-05-05T20:35:34.296973Z",
     "iopub.status.idle": "2025-05-05T20:35:34.326858Z",
     "shell.execute_reply": "2025-05-05T20:35:34.326359Z"
    },
    "papermill": {
     "duration": 0.042618,
     "end_time": "2025-05-05T20:35:34.327764",
     "exception": false,
     "start_time": "2025-05-05T20:35:34.285146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## confirm duplicates don't exist\n",
    "checkpoint_meds = pyCLIF.remove_duplicates(hourly_ne_merged, [\n",
    "    'encounter_block','recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59510f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"final_df shape before merging\", final_df.shape)\n",
    "final_df = pyCLIF.extend_hourly_dataset(\n",
    "    base_df=final_df,\n",
    "    addon_df=hourly_ne_merged,\n",
    "    merge_cols=['encounter_block', 'recorded_date', 'recorded_hour']\n",
    ")\n",
    "print(\"final_df shape after merging\", final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9ebe9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:35.709279Z",
     "iopub.status.busy": "2025-05-05T20:35:35.709136Z",
     "iopub.status.idle": "2025-05-05T20:35:35.737421Z",
     "shell.execute_reply": "2025-05-05T20:35:35.737093Z"
    },
    "papermill": {
     "duration": 0.041191,
     "end_time": "2025-05-05T20:35:35.738400",
     "exception": false,
     "start_time": "2025-05-05T20:35:35.697209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "red_meds_list = [\n",
    "    \"nicardipine\", \"nitroprusside\", \"clevidipine\"\n",
    "]\n",
    "\n",
    "# Filter meds_filtered for the medications in red_meds_list\n",
    "red_meds_df = meds[meds['med_category'].isin(red_meds_list)].copy()\n",
    "\n",
    "# Create a flag for each medication in red_meds_list\n",
    "for med in red_meds_list:\n",
    "    # Create a flag that is 1 if the medication was administered in that hour, 0 otherwise\n",
    "    red_meds_df[med + '_flag'] = np.where((red_meds_df['med_category'] == med) & \n",
    "                                         (red_meds_df['med_dose'] > 0.0) & \n",
    "                                         (red_meds_df['med_dose'].notna()), 1, 0).astype(int)\n",
    "\n",
    "# Aggregate to get the maximum value for each flag (per hospitalization_id, recorded_date, recorded_hour)\n",
    "# This ensures that if the medication was administered even once in the hour, the flag is 1\n",
    "red_meds_flags = red_meds_df.groupby(['encounter_block', 'recorded_date', 'recorded_hour']).agg(\n",
    "    {med + '_flag': 'max' for med in red_meds_list}\n",
    ").reset_index()\n",
    "\n",
    "#  combine all flags into a single 'red_meds_flag', you can do so like this:\n",
    "red_meds_flags['red_meds_flag'] = red_meds_flags[[med + '_flag' for med in red_meds_list]].max(axis=1)\n",
    "\n",
    "# Select the relevant columns\n",
    "red_meds_flags_final = red_meds_flags[[\n",
    "    'encounter_block', 'recorded_date', 'recorded_hour',\n",
    "    'nicardipine_flag', 'nitroprusside_flag',\n",
    "    'clevidipine_flag', 'red_meds_flag'\n",
    "]].drop_duplicates(subset=['encounter_block', 'recorded_date', 'recorded_hour'])\n",
    "\n",
    "red_meds_flags_final['nicardipine_flag'] = red_meds_flags_final['nicardipine_flag'].astype(int)\n",
    "red_meds_flags_final['nitroprusside_flag'] = red_meds_flags_final['nitroprusside_flag'].astype(int)\n",
    "red_meds_flags_final['clevidipine_flag'] = red_meds_flags_final['clevidipine_flag'].astype(int)\n",
    "red_meds_flags_final['red_meds_flag'] = red_meds_flags_final['red_meds_flag'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3480efd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:35.761537Z",
     "iopub.status.busy": "2025-05-05T20:35:35.761417Z",
     "iopub.status.idle": "2025-05-05T20:35:35.763498Z",
     "shell.execute_reply": "2025-05-05T20:35:35.763205Z"
    },
    "papermill": {
     "duration": 0.014821,
     "end_time": "2025-05-05T20:35:35.764314",
     "exception": false,
     "start_time": "2025-05-05T20:35:35.749493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "strobe_counts['final_blocks_with_red_meds'] = red_meds_flags_final['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b7caf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:35.787890Z",
     "iopub.status.busy": "2025-05-05T20:35:35.787746Z",
     "iopub.status.idle": "2025-05-05T20:35:35.790956Z",
     "shell.execute_reply": "2025-05-05T20:35:35.790628Z"
    },
    "papermill": {
     "duration": 0.015966,
     "end_time": "2025-05-05T20:35:35.791667",
     "exception": false,
     "start_time": "2025-05-05T20:35:35.775701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## confirm duplicates don't exist\n",
    "checkpoint_red_meds = pyCLIF.remove_duplicates(red_meds_flags_final, [\n",
    "    'encounter_block','recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_red_meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bc059",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:35.850268Z",
     "iopub.status.busy": "2025-05-05T20:35:35.850015Z",
     "iopub.status.idle": "2025-05-05T20:35:37.206635Z",
     "shell.execute_reply": "2025-05-05T20:35:37.206288Z"
    },
    "papermill": {
     "duration": 1.38075,
     "end_time": "2025-05-05T20:35:37.207544",
     "exception": false,
     "start_time": "2025-05-05T20:35:35.826794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"final_df shape before merging\", final_df.shape)\n",
    "final_df = pyCLIF.extend_hourly_dataset(\n",
    "    base_df=final_df,\n",
    "    addon_df=red_meds_flags_final,\n",
    "    merge_cols=['encounter_block', 'recorded_date', 'recorded_hour']\n",
    ")\n",
    "print(\"final_df shape after merging\", final_df.shape)\n",
    "print(\"\\n Columns in final_df after merging with red_meds_flags_final columns:\")\n",
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe90db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:37.231378Z",
     "iopub.status.busy": "2025-05-05T20:35:37.231234Z",
     "iopub.status.idle": "2025-05-05T20:35:37.251324Z",
     "shell.execute_reply": "2025-05-05T20:35:37.250786Z"
    },
    "papermill": {
     "duration": 0.032801,
     "end_time": "2025-05-05T20:35:37.252245",
     "exception": false,
     "start_time": "2025-05-05T20:35:37.219444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paralytics_list = [\n",
    "    \"cisatracurium\", \"vecuronium\", \"rocuronium\" \n",
    "]\n",
    "\n",
    "# Filter meds_filtered for the medications in paralytics_list\n",
    "paralytics_df = meds[meds['med_category'].isin(paralytics_list)].copy()\n",
    "\n",
    "# Create a flag for each medication in paralytics_list\n",
    "for med in paralytics_list:\n",
    "    # Create a flag that is 1 if the medication was administered in that hour, 0 otherwise\n",
    "    paralytics_df[med + '_flag'] = np.where((paralytics_df['med_category'] == med) & \n",
    "                                           (paralytics_df['med_dose'] > 0.0) &\n",
    "                                           (paralytics_df['med_dose'].notna()), 1, 0).astype(int)\n",
    "\n",
    "# Aggregate to get the maximum value for each flag (per hospitalization_id, recorded_date, recorded_hour)\n",
    "# This ensures that if the medication was administered even once in the hour, the flag is 1\n",
    "paralytics_flags = paralytics_df.groupby(['encounter_block', 'recorded_date', 'recorded_hour']).agg(\n",
    "    {med + '_flag': 'max' for med in paralytics_list}\n",
    ").reset_index()\n",
    "\n",
    "#  combine all flags into a single 'paralytics_flag', you can do so like this:\n",
    "paralytics_flags['paralytics_flag'] = paralytics_flags[[med + '_flag' for med in paralytics_list]].max(axis=1)\n",
    "\n",
    "# Select the relevant columns\n",
    "paralytics_flags_final = paralytics_flags[[\n",
    "    'encounter_block', 'recorded_date', 'recorded_hour',\n",
    "    'cisatracurium_flag', 'vecuronium_flag',\n",
    "    'rocuronium_flag', 'paralytics_flag'\n",
    "]].drop_duplicates(subset=['encounter_block', 'recorded_date', 'recorded_hour'])\n",
    "\n",
    "paralytics_flags_final['cisatracurium_flag'] = paralytics_flags_final['cisatracurium_flag'].astype(int)\n",
    "paralytics_flags_final['vecuronium_flag'] = paralytics_flags_final['vecuronium_flag'].astype(int)\n",
    "paralytics_flags_final['rocuronium_flag'] = paralytics_flags_final['rocuronium_flag'].astype(int)\n",
    "paralytics_flags_final['paralytics_flag'] = paralytics_flags_final['paralytics_flag'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec20a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:37.277018Z",
     "iopub.status.busy": "2025-05-05T20:35:37.276881Z",
     "iopub.status.idle": "2025-05-05T20:35:37.278926Z",
     "shell.execute_reply": "2025-05-05T20:35:37.278644Z"
    },
    "papermill": {
     "duration": 0.015192,
     "end_time": "2025-05-05T20:35:37.279676",
     "exception": false,
     "start_time": "2025-05-05T20:35:37.264484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "strobe_counts['final_blocks_with_paralytics'] = paralytics_flags_final['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9300eb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:37.303765Z",
     "iopub.status.busy": "2025-05-05T20:35:37.303464Z",
     "iopub.status.idle": "2025-05-05T20:35:37.306779Z",
     "shell.execute_reply": "2025-05-05T20:35:37.306440Z"
    },
    "papermill": {
     "duration": 0.016365,
     "end_time": "2025-05-05T20:35:37.307547",
     "exception": false,
     "start_time": "2025-05-05T20:35:37.291182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## confirm duplicates don't exist\n",
    "checkpoint_paralytics_meds = pyCLIF.remove_duplicates(paralytics_flags_final, [\n",
    "    'encounter_block','recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_paralytics_meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6937bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:37.332717Z",
     "iopub.status.busy": "2025-05-05T20:35:37.332504Z",
     "iopub.status.idle": "2025-05-05T20:35:38.727274Z",
     "shell.execute_reply": "2025-05-05T20:35:38.726863Z"
    },
    "papermill": {
     "duration": 1.408561,
     "end_time": "2025-05-05T20:35:38.728232",
     "exception": false,
     "start_time": "2025-05-05T20:35:37.319671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# final_df = pd.merge(final_df, \n",
    "#                     paralytics_flags_final, \n",
    "#                     on=['encounter_block', 'recorded_date', 'recorded_hour'], \n",
    "#                     how='left')\n",
    "\n",
    "print(\"final_df shape before merging\", final_df.shape)\n",
    "final_df = pyCLIF.extend_hourly_dataset(\n",
    "    base_df=final_df,\n",
    "    addon_df=paralytics_flags_final,\n",
    "    merge_cols=['encounter_block', 'recorded_date', 'recorded_hour']\n",
    ")\n",
    "print(\"final_df shape after merging\", final_df.shape)\n",
    "\n",
    "print(\"\\n Columns in final_df after merging with paralytics columns:\")\n",
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd43d39",
   "metadata": {
    "papermill": {
     "duration": 0.011973,
     "end_time": "2025-05-05T20:35:38.751936",
     "exception": false,
     "start_time": "2025-05-05T20:35:38.739963",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hourly Labs\n",
    "\n",
    "Get most recent lactate defined as closest lab result time to the start of first intubation event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b044950a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:38.775747Z",
     "iopub.status.busy": "2025-05-05T20:35:38.775600Z",
     "iopub.status.idle": "2025-05-05T20:35:38.954278Z",
     "shell.execute_reply": "2025-05-05T20:35:38.953914Z"
    },
    "papermill": {
     "duration": 0.191821,
     "end_time": "2025-05-05T20:35:38.955183",
     "exception": false,
     "start_time": "2025-05-05T20:35:38.763362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import labs\n",
    "labs_filters = {\n",
    "    'hospitalization_id': all_ids['hospitalization_id'].unique().tolist(),\n",
    "    'lab_category': labs_of_interest\n",
    "}\n",
    "labs = pyCLIF.load_data('clif_labs', columns=labs_required_columns, filters=labs_filters)\n",
    "print(\"unique encounters in labs\", pyCLIF.count_unique_encounters(labs))\n",
    "labs['hospitalization_id']= labs['hospitalization_id'].astype(str)\n",
    "labs = labs.merge(all_ids, on='hospitalization_id', how='left')\n",
    "labs = labs.sort_values(by=['encounter_block', 'lab_result_dttm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13104c15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:38.980296Z",
     "iopub.status.busy": "2025-05-05T20:35:38.980134Z",
     "iopub.status.idle": "2025-05-05T20:35:38.982409Z",
     "shell.execute_reply": "2025-05-05T20:35:38.982131Z"
    },
    "papermill": {
     "duration": 0.015892,
     "end_time": "2025-05-05T20:35:38.983132",
     "exception": false,
     "start_time": "2025-05-05T20:35:38.967240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "strobe_counts['final_blocks_with_lactate_lab'] = labs['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5339f01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:39.006493Z",
     "iopub.status.busy": "2025-05-05T20:35:39.006370Z",
     "iopub.status.idle": "2025-05-05T20:35:39.059747Z",
     "shell.execute_reply": "2025-05-05T20:35:39.059264Z"
    },
    "papermill": {
     "duration": 0.065967,
     "end_time": "2025-05-05T20:35:39.060662",
     "exception": false,
     "start_time": "2025-05-05T20:35:38.994695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labs = pyCLIF.convert_datetime_columns_to_site_tz(labs, pyCLIF.helper['timezone'])\n",
    "labs['lab_value_numeric'] = pd.to_numeric(labs['lab_value_numeric'], errors='coerce')\n",
    "labs['recorded_hour'] = labs['lab_result_dttm'].dt.hour\n",
    "labs['recorded_date'] = labs['lab_result_dttm'].dt.date\n",
    "\n",
    "lactate_df = pd.merge(labs, block_vent_times, on='encounter_block', how='left')\n",
    "lactate_df['time_since_vent_start_hours'] = (\n",
    "    (lactate_df['lab_result_dttm'] - lactate_df['block_vent_start_dttm']).dt.total_seconds() / 3600\n",
    ")\n",
    "\n",
    "# Calculate the absolute time difference between lab_result_dttm and vent_start_time in hours\n",
    "lactate_df['time_diff_hours'] = abs((lactate_df['lab_result_dttm'] - lactate_df['block_vent_start_dttm']).dt.total_seconds() / 3600)\n",
    "\n",
    "# Sort by encounter_block, recorded_hour, and time_diff_hours to find the closest measurement to vent_start_time\n",
    "lactate_df = lactate_df.sort_values(by=['encounter_block', 'recorded_date', 'recorded_hour', 'time_diff_hours'])\n",
    "\n",
    "# Group by encounter_block and recorded_hour, and get the first row in each group (which is the closest measurement)\n",
    "# closest lactate measurement is defined as closest to the vent_start_time in that hour.\n",
    "# we keep the first recorded value in that hour \n",
    "closest_lactate_df = lactate_df.groupby(['encounter_block', 'recorded_date','recorded_hour']).first().reset_index()\n",
    "\n",
    "labs_final = closest_lactate_df[['encounter_block', 'recorded_date', 'recorded_hour', 'lab_value_numeric']].copy()\n",
    "\n",
    "# Rename the 'lab_value_numeric' column to 'lactate'\n",
    "labs_final = labs_final.rename(columns={'lab_value_numeric': 'lactate'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be19f1d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:39.085159Z",
     "iopub.status.busy": "2025-05-05T20:35:39.084974Z",
     "iopub.status.idle": "2025-05-05T20:35:39.219457Z",
     "shell.execute_reply": "2025-05-05T20:35:39.219092Z"
    },
    "papermill": {
     "duration": 0.147962,
     "end_time": "2025-05-05T20:35:39.220360",
     "exception": false,
     "start_time": "2025-05-05T20:35:39.072398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_labs= pyCLIF.remove_duplicates(labs_final, [\n",
    "    'encounter_block', 'recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d02989",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:39.245749Z",
     "iopub.status.busy": "2025-05-05T20:35:39.245588Z",
     "iopub.status.idle": "2025-05-05T20:35:41.009453Z",
     "shell.execute_reply": "2025-05-05T20:35:41.009024Z"
    },
    "papermill": {
     "duration": 1.77801,
     "end_time": "2025-05-05T20:35:41.010554",
     "exception": false,
     "start_time": "2025-05-05T20:35:39.232544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# final_df = pd.merge(final_df, \n",
    "#                     labs_final, \n",
    "#                     on=['encounter_block', 'recorded_date', 'recorded_hour'], \n",
    "#                    how='left')\n",
    "\n",
    "print(\"final_df shape before merging\", final_df.shape)\n",
    "final_df = pyCLIF.extend_hourly_dataset(\n",
    "    base_df=final_df,\n",
    "    addon_df=labs_final,\n",
    "    merge_cols=['encounter_block', 'recorded_date', 'recorded_hour']\n",
    ")\n",
    "print(\"final_df shape after merging\", final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0495f56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:41.036321Z",
     "iopub.status.busy": "2025-05-05T20:35:41.036067Z",
     "iopub.status.idle": "2025-05-05T20:35:41.038875Z",
     "shell.execute_reply": "2025-05-05T20:35:41.038619Z"
    },
    "papermill": {
     "duration": 0.016617,
     "end_time": "2025-05-05T20:35:41.039572",
     "exception": false,
     "start_time": "2025-05-05T20:35:41.022955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7175f4d8",
   "metadata": {
    "papermill": {
     "duration": 0.011819,
     "end_time": "2025-05-05T20:35:41.063550",
     "exception": false,
     "start_time": "2025-05-05T20:35:41.051731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SOFA\n",
    "\n",
    "Calculate SOFA score for the first 24 hours since the start of first intubation episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed89b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:41.089106Z",
     "iopub.status.busy": "2025-05-05T20:35:41.088841Z",
     "iopub.status.idle": "2025-05-05T20:35:50.575196Z",
     "shell.execute_reply": "2025-05-05T20:35:50.574831Z"
    },
    "papermill": {
     "duration": 9.500519,
     "end_time": "2025-05-05T20:35:50.576456",
     "exception": false,
     "start_time": "2025-05-05T20:35:41.075937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper = pyCLIF.load_config()\n",
    "tables_path= helper['tables_path']\n",
    "\n",
    "sofa_input_df = all_ids_w_outcome[['encounter_block', 'block_vent_start_dttm']].copy()\n",
    "sofa_input_df = sofa_input_df.rename(columns={'block_vent_start_dttm': 'start_dttm'})\n",
    "sofa_input_df['stop_dttm'] = sofa_input_df['start_dttm'] + pd.Timedelta(hours=24)\n",
    "id_mappings = all_ids_w_outcome[['encounter_block', 'hospitalization_id' ]].drop_duplicates()\n",
    "\n",
    "sofa_df = sofa_score.compute_sofa(\n",
    "            ids_w_dttm = sofa_input_df,          # id, start_dttm, end_dttm  (local time)\n",
    "            tables_path = tables_path,\n",
    "            use_hospitalization_id = False,         # or False + id_mapping (new id , hospitalization_id)\n",
    "            id_mapping = id_mappings,              # first column should be your new id_variable, second column is hospitalization id\n",
    "            helper_module = pyCLIF,                # â† your existing loader\n",
    "            output_filepath = \"../output/intermediate/sofa.parquet\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d8182",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:50.602498Z",
     "iopub.status.busy": "2025-05-05T20:35:50.602294Z",
     "iopub.status.idle": "2025-05-05T20:35:50.851649Z",
     "shell.execute_reply": "2025-05-05T20:35:50.851327Z"
    },
    "papermill": {
     "duration": 0.263635,
     "end_time": "2025-05-05T20:35:50.852597",
     "exception": false,
     "start_time": "2025-05-05T20:35:50.588962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df_blocks = sofa_df.merge(all_ids_w_outcome, on='encounter_block', how='left')\n",
    "final_df_blocks = final_df_blocks.merge(hospitalization[['hospitalization_id', 'admission_dttm', 'age_at_admission']], \n",
    "                                      on='hospitalization_id', how='left')\n",
    "final_df_blocks = final_df_blocks.merge(patient[['patient_id', 'race_category','ethnicity_category', 'sex_category','language_name']], \n",
    "                                      on='patient_id', how='left')\n",
    "\n",
    "# First join ADT with all_ids to get closest ADT row to vent start\n",
    "adt_with_blocks = pd.merge(\n",
    "    all_ids_w_outcome[['encounter_block', 'block_vent_start_dttm', 'hospitalization_id']],\n",
    "    adt,\n",
    "    on='hospitalization_id'\n",
    ")\n",
    "\n",
    "# Calculate time difference between vent start and ADT in_dttm\n",
    "adt_with_blocks['time_diff'] = abs(adt_with_blocks['block_vent_start_dttm'] - adt_with_blocks['in_dttm'])\n",
    "\n",
    "# Get the closest ADT row for each encounter block\n",
    "closest_adt = (adt_with_blocks\n",
    "    .sort_values('time_diff')\n",
    "    .groupby('encounter_block')\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Join with final_df_blocks\n",
    "final_df_blocks = final_df_blocks.merge(\n",
    "    closest_adt[['encounter_block', 'location_name', 'location_category', 'in_dttm', 'out_dttm']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "final_df_blocks.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4510317c",
   "metadata": {
    "papermill": {
     "duration": 0.012843,
     "end_time": "2025-05-05T20:35:50.878238",
     "exception": false,
     "start_time": "2025-05-05T20:35:50.865395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Write analysis dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b167bf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:50.904698Z",
     "iopub.status.busy": "2025-05-05T20:35:50.904524Z",
     "iopub.status.idle": "2025-05-05T20:35:51.724532Z",
     "shell.execute_reply": "2025-05-05T20:35:51.724219Z"
    },
    "papermill": {
     "duration": 0.834873,
     "end_time": "2025-05-05T20:35:51.725547",
     "exception": false,
     "start_time": "2025-05-05T20:35:50.890674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df.to_parquet('../output/intermediate/final_df_hourly.parquet')\n",
    "final_df_blocks.to_parquet('../output/intermediate/final_df_blocks.parquet')\n",
    "all_ids_w_outcome.to_parquet('../output/intermediate/cohort_all_ids_w_outcome.parquet')\n",
    "# Convert the dictionary to a DataFrame and save it as a CSV file\n",
    "pd.DataFrame(list(strobe_counts.items()), columns=['Metric', 'Value']).to_csv('../output/final/strobe_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d9fab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T20:35:51.752671Z",
     "iopub.status.busy": "2025-05-05T20:35:51.752376Z",
     "iopub.status.idle": "2025-05-05T20:35:51.755228Z",
     "shell.execute_reply": "2025-05-05T20:35:51.754935Z"
    },
    "papermill": {
     "duration": 0.017933,
     "end_time": "2025-05-05T20:35:51.756024",
     "exception": false,
     "start_time": "2025-05-05T20:35:51.738091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c80e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STROBE diagram\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, FancyArrowPatch\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.axis('off')\n",
    "\n",
    "# Box positions and texts\n",
    "boxes = [\n",
    "    {\"text\": f\"All adult encounters after date filter\\n(n = {strobe_counts['A_after_date_age_filter']})\", \"xy\": (0.5, 0.9)},\n",
    "    {\"text\": f\"Linked Encounter Blocks\\n(n = {strobe_counts['B_after_stitching']})\", \"xy\": (0.5, 0.75)},\n",
    "    {\"text\": f\"Encounter blocks receiving IMV\\n(n = {strobe_counts['C_imv_encounter_blocks']})\", \"xy\": (0.5, 0.6)},\n",
    "    {\"text\": f\"Encounter blocks receiving IMV â‰¥ 4 hrs\\n(n = {strobe_counts['G_blocks_with_vent_4_or_more']})\", \"xy\": (0.5, 0.45)},\n",
    "    {\"text\": f\"Encounter blocks not on trach\\n(n = {strobe_counts['G_final_blocks_without_trach_at_intubation']})\", \"xy\": (0.5, 0.3)},\n",
    "]\n",
    "\n",
    "exclusions = [\n",
    "    {\"text\": f\"Linked hospitalizations\\n(n = {strobe_counts['B_stitched_hosp_ids']})\", \"xy\": (0.8, 0.825)},\n",
    "    {\"text\": f\"Excluded: Encounters on vent for <4 hrs\\n(n = {strobe_counts['D_blocks_with_same_vent_start_end'] + strobe_counts['E_blocks_with_vent_end_before_vital_start'] + strobe_counts['G_blocks_with_vent_less_than_4']})\", \"xy\": (0.8, 0.525)},\n",
    "    {\"text\": f\"Excluded: Encounters with Tracheostomy\\n(n = {strobe_counts['G_final_blocks_with_trach_at_intubation']})\", \"xy\": (0.8, 0.375)},\n",
    "]\n",
    "\n",
    "# Draw main boxes and arrows\n",
    "for i, box in enumerate(boxes):\n",
    "    x, y = box[\"xy\"]\n",
    "    ax.add_patch(Rectangle((x - 0.25, y - 0.05), 0.5, 0.1, edgecolor='black', facecolor='white'))\n",
    "    ax.text(x, y, box[\"text\"], ha='center', va='center', fontsize=10)\n",
    "    if i < len(boxes) - 1:\n",
    "        ax.add_patch(FancyArrowPatch((x, y - 0.05), (x, y - 0.1), arrowstyle='->', mutation_scale=15))\n",
    "\n",
    "# Draw exclusion boxes and connectors\n",
    "for excl in exclusions:\n",
    "    x, y = excl[\"xy\"]\n",
    "    ax.add_patch(Rectangle((x - 0.20, y - 0.04), 0.38, 0.08, edgecolor='black', facecolor='#f8d7da'))\n",
    "    ax.text(x, y, excl[\"text\"], ha='center', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../output/final/graphs/strobe_diagram_{pyCLIF.helper[\"site_name\"]}.png')\n",
    "plt.close(fig)\n",
    "print(\"Created STROBE diagram\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mobilization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 213.599718,
   "end_time": "2025-05-05T20:35:53.300144",
   "environment_variables": {},
   "exception": null,
   "input_path": "01_cohort_identification.ipynb",
   "output_path": "01_cohort_identification.ipynb",
   "parameters": {},
   "start_time": "2025-05-05T20:32:19.700426",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
